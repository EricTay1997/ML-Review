# Diffusion

- Much like VAEs, Diffusion models are another type of generative model. 
- ![diffusion.png](diffusion.png)[Source](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)
- There are a few ways that diffusion is different from VAEs
  - $\mathbf{z}$ has same dimension as $\mathbf{x}_0$
  - The forward process (encoding) is done not by a neural network, but by simply adding Gaussian noise. 
  - The backward process (decoding) is still achieved by a neural network, but instead of predicting the image $\mathbf{x}_0$, we predict the noise $\mathbf{x}_t$ - $\mathbf{x}_0$. 
- Theoretical Details:
  - ![diffusion_algo.png](diffusion_algo.png)[Source](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)
  - Data points are assumed to come from a real data distribution $\mathbf{x}_0 \sim q(\mathbf{x})$
  - We define a forward diffusion process in which we add small amount of Gaussian noise to the sample in $T$ steps, producing a sequence of noisy samples $\mathbf{x}_1, \ldots, \mathbf{x}_T$. The step sizes are controlled by a variance schedule $\{\beta_t \in (0,1)\}_{t=1}^T$. 
    - $q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right) \sim \mathcal{N}\left(\mathbf{x}_t ; \sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}\right)$ 
    - $q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right)=\prod_{t=1}^T q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right)$
    - Note that this implies $q\left(\mathbf{x}_t \mid \mathbf{x}_0\right) \sim\mathcal{N}\left(\mathbf{x}_t ; \sqrt{\bar{\alpha}_t} \mathbf{x}_0,\left(1-\bar{\alpha}_t\right) \mathbf{I}\right)$, where $\bar{\alpha}_t = \prod_{i=1}^t (1 - \beta_t)$
      - This permits us to sample from $\mathbf{x}_t$ directly given $\mathbf{x}_0$
      - In addition, we can reparameterize $\mathbf{x}_t \mid \mathbf{x}_0$ = $\sqrt{\bar{\alpha}_t} \mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} \pmb{\epsilon}$ for $\pmb{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$
  - If we can sample from $q(\mathbf{x}_{t-1} \mid \mathbf{x}_t)$, **we can recreate the true sample from a Gaussian noise input**, $\mathbf{x}_t \sim \mathcal{N}(\mathbf{0, I})$.
    - But we can't! So we use a neural network to model $p_\theta,$ where we have
      - $p_\theta\left(\mathbf{x}_{0: T}\right)=p\left(\mathbf{x}_T\right) \prod_{t=1}^T p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right)$
      - $p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right) \sim \mathcal{N}\left(\mathbf{x}_{t-1} ; \pmb{\mu}_\theta\left(\mathbf{x}_t, t\right), \mathbf{\Sigma}_\theta\left(\mathbf{x}_t, t\right)\right)$
    - What is a good $p_\theta$? One that maximizes $p_\theta(\mathbf{x}_0)$.
      - As before, we instead maximize the variational lower bound
        - $\log p_\theta(\mathbf{x}_0) \geq \log p_\theta(\mathbf{x}_0) - D_{\mathrm{KL}}\left(q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right) \| p_\theta\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right)\right)$, which expands to 
        - -$\mathbb{E}_q[\underbrace{D_{\mathrm{KL}}\left(q\left(\mathbf{x}_T \mid \mathbf{x}_0\right) \| p\left(\mathbf{x}_T\right)\right)}_{L_T}+\sum_{t>1} \underbrace{D_{\mathrm{KL}}\left(q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0\right) \| p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right)\right)}_{L_{t-1}} \underbrace{-\log p_\theta\left(\mathbf{x}_0 \mid \mathbf{x}_1\right)}_{L_0}]$ [Source, Appendix A](https://arxiv.org/pdf/2006.11239)
        - This form is nice because:
          - $L_1, \ldots, L_T$ compares two Gaussian distributions and therefore they can be computed in closed form.
          - $L_T$ is constant and can be ignored during training because $q$ has no learnable parameters and $\mathbf{x}_T$ is a Gaussian noise
          - [Ho et al. 2020](https://arxiv.org/pdf/2006.11239) models $L_0$ using a separate discrete decoder (Section 3.3).
          - Hence, moving forward, we focus on $L_1, \ldots, L_{T-1}$. 
        - Minimizing $L_{t-1}$ 
          - Returning to $p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right) \sim \mathcal{N}\left(\mathbf{x}_{t-1} ; \pmb{\mu}_\theta\left(\mathbf{x}_t, t\right), \mathbf{\Sigma}_\theta\left(\mathbf{x}_t, t\right)\right)$,
            - We wish to minimize the KL divergence with $q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0\right)=\mathcal{N}\left(\mathbf{x}_{t-1} ; \tilde{\pmb{\mu}}_t\left(\mathbf{x}_t, \mathbf{x}_0\right), \tilde{\beta}_t \mathbf{I}\right)$, where we can show
              - $\quad \tilde{\pmb{\mu}}_t\left(\mathbf{x}_t, \mathbf{x}_0\right):=\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} \mathbf{x}_0+\frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t} \mathbf{x}_t \quad$ and $\quad \tilde{\beta}_t:=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t$
            - Unlike VAEs, where we optimize for both mean and covariance, we instead _set_ $\mathbf{\Sigma}_\theta$ and focus on optimizing for $\pmb{\mu}_\theta$ ([Details, Section 3.2](https://arxiv.org/pdf/2006.11239))
          - Using the reparameterization of $\mathbf{x}_t \mid \mathbf{x}_0$, we find that $L_{t-1}$ is minimized when 
          - $\pmb\mu_\theta(\mathbf{x}_t, t) = \frac{1}{\sqrt{1 - \beta_t}}\left(\mathbf{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \pmb{\epsilon}\right)$
            - In other words, having $\pmb\epsilon_\theta$, a function approximator intended to predict $\pmb\epsilon$ from $\mathbf{x}_t$, is equivalent to having $\pmb\mu_\theta$. 
          - Training
            - Reparameterizing $\pmb\mu_\theta$ with $\pmb\epsilon_\theta$, we find that we can rewrite our loss as: 
            - $\mathbb{E}_{\mathbf{x}_0, \pmb{\epsilon}}\left[\frac{\beta_t^2}{2 \sigma_t^2 \alpha_t\left(1-\bar{\alpha}_t\right)}\left\|\pmb{\epsilon}-\pmb{\epsilon}_\theta\left(\sqrt{\bar{\alpha}_t} \mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} \pmb{\epsilon}, t\right)\right\|^2\right]$
            - **That's it!** All this really is saying is that for each noised sample $\mathbf{x}_0 + \pmb\epsilon$, we want to predict the noise that was added to $\mathbf{x}_0$, and weight the squared loss as a function of $t$. 
            - As it turns out, the authors found it beneficial to instead train on the following simpler variant:
              - $L_{\text {simple }}(\theta):=\mathbb{E}_{t, \mathbf{x}_0, \pmb{\epsilon}}\left[\left\|\pmb{\epsilon}-\pmb{\epsilon}_\theta\left(\sqrt{\bar{\alpha}_t} \mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} \pmb{\epsilon}, t\right)\right\|^2\right]$
          - Sampling
            - Given our trained $\pmb\epsilon_\theta$, we can start with noise $\mathbf{x}_T \sim \mathcal{N}(\mathbf{0, I})$ and iteratively sample:
            - $\mathbf{x}_{t-1}=\frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \pmb{\epsilon}_\theta\left(\mathbf{x}_t, t\right)\right)+\sigma_t \mathbf{z}$
- Implementation
  - Timesteps $t$ are sampled from a uniform distribution from 1 to $T$. 
  - $\beta_t$ is scaled linearly from $\beta_1 = 10^{-4}$ to $\beta_T = 0.02$
    - Other variance schedules have also been suggested since:
      - ![cosine.png](cosine.png)[Nichol and Dhariwal](https://arxiv.org/pdf/2102.09672) argue that the training steps with large $t$ is irrelevant for linear. 
      - "The choice of the scheduling function can be arbitrary, as long as it provides a near-linear drop in the middle of the training process and subtle changes around $t=0$ and $t=T$" [Source](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) (ToDo: To understand)
  - $\mathbf{\Sigma}_\theta(\mathbf{x}_t, t) = \sigma^2_t\mathbf{I},$ where both $\sigma^2_t = \beta_t$ and $\sigma^2_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t$ were tried
  - We model $L_0$ using a separate discrete decoder ([Section 3.3](https://arxiv.org/pdf/2006.11239))
- Conditional Generation
  - We can pass in conditional information both during pre-training and [post-training](../22_post_training/notes.md)
  - Additional conditional information can be passed in with:
    - Embedding and concatenation, e.g. passing in an additional channel 
    - Embedding and adding, similar to how timestep conditioning is handled 
    - Adding cross-attention layers that can ‘attend’ to a (text) sequence passed in as conditioning
      - ![cross_attention.png](cross_attention.png)[Source](https://wandb.ai/johnowhitaker/midu-guidance/reports/Mid-U-Guidance-Fast-Classifier-Guidance-for-Latent-Diffusion-Models--VmlldzozMjg0NzA1)
  - Classifier-free Guidance
    - Here, we create two predictions at inference time: One with conditioning and one without. 
    - We then use the two to push even further in the direction indicated by the text-conditioned prediction.
  - Textual conditioning
    - Suppose we have a text/image embedding model (e.g. [CLIP](../15_contrastive_learning/notes.md))
      - Guidance
        - Generate images
        - Embed query text and generated images
        - Compute loss in embeddings space
        - Update noise that we start generation process with
  - Image + Text conditioning
    - ![image_conditioning.png](image_conditioning.png)[Source](https://arxiv.org/pdf/2208.01618)
    - Textual inversion
      - Learn a textual representation of input image.
      - ![textual_inversion.png](textual_inversion.png)[Source](https://arxiv.org/pdf/2208.01618)
    - [DreamBooth](https://arxiv.org/pdf/2208.12242)
      - Leverages semantic prior
      - Authors label all input images of the subject "a [identifer][class]"
      - The model is then finetuned with both "a [identifer][class]" and "a [class]" (the latter corresponding to prior preservation loss)
      - The finetuned model is then applied to new contexts
    - img2img
      - Add noise to original image and denoise with new prompt
    - DDIM Inversion
      - We invert the sampling process to get a noisy latent which, if used as the starting point for our usual sampling procedure, results in the original image being generated.
      - If we do this inversion with a prompt, the hope is that changing the prompt would give us the ability to condition our image generation.
  - Control
    - Masking
    - Cross-attention control 
- Latent Diffusion
  - Since attention grows quadratically with input size, large images are expensive to generate.
  - Latent Diffusion uses a VAE to compress images, and we run diffusion in the latent space.
  - ![latent.png](latent.png)[Source](https://arxiv.org/pdf/2112.10752)
- Faster Sampling
  - We could use a [strided sampling schedule](https://arxiv.org/pdf/2102.09672) to reduce the number of steps we take.  
  - [DDIM](https://arxiv.org/pdf/2010.02502) combines the idea of an accelerated trajectory with $\sigma_t = 0$ (deterministic)
    - The above is an oversimplification. Technically,
    - We define a family of distributions indexed by $\sigma$
      - $q_\sigma\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right)=q_\sigma\left(\mathbf{x}_T \mid \mathbf{x}_0\right) \prod_{t=2}^T q_\sigma\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0\right)$
        - This is non-Markovian because $\mathbf{x}_{t-1}$ now depends on $\mathbf{x}_t, \mathbf{x}_0$
        - We choose parameters such that $q_\sigma(\mathbf{x}_t \mid \mathbf{x}_0) \sim\mathcal{N}\left(\sqrt{\bar{\alpha}_t} \mathbf{x}_0,\left(1-\bar{\alpha}_t\right) \mathbf{I}\right)$ as before. 
      - As before the model tries to predict the noise at each time step. 
      - With such a prediction, we define a generative process where we first predict $\hat{\mathbf{x}}_0$, and then sample $\mathbf{x}_{t-1} \sim q_\sigma(\mathbf{x}_{t-1} \mid \hat{\mathbf{x}}_0, \mathbf{x}_{t})$
        - We sample $\mathbf{x}_{t-1}=\sqrt{\bar{\alpha}_{t-1}} \underbrace{\left(\frac{\mathbf{x}_t-\sqrt{1-\bar{\alpha}_t} \epsilon_\theta^{(t)}\left(\mathbf{x}_t\right)}{\sqrt{\bar{\alpha}_t}}\right)}_{\text {" predicted } \mathbf{x}_0 "}+\underbrace{\sqrt{1-\bar{\alpha}_{t-1}-\sigma_t^2} \cdot \epsilon_\theta^{(t)}\left(\mathbf{x}_t\right)}_{\text {"direction pointing to } \mathbf{x}_t "}+\underbrace{\sigma_t \epsilon_t}_{\text {random noise }}$
          - We can reparameterize $\sigma_t^2 = \eta\tilde{\beta}_t$. Setting $\eta = 1$ gives us a DDPM reverse process. 
          - $\sigma_t = 0$ gives a result that is deterministic, and is known as DDIM. 
      - In fact, because we can use any forward process such that our marginals $q_\sigma(\mathbf{x}_t \mid \mathbf{x}_0)$ match, we can use the above equation to sample from an accelerated schedule. 
        - Technically, I believe that it is only sound to do so if the parameters of the noise model are not shared across timesteps, but this works in practice.
  - Distillation
    - The ‘student’ model is initialized from the weights of the ‘teacher’ model. 
    - During training, the teacher model performs two sampling steps and the student model tries to match the resulting prediction in a single step. 
    - This process can be repeated multiple times.
- Other modes
  - [Video](../20_video/notes.md)
  - [Audio](../19_audio/notes.md)

