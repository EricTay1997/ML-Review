{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4efbd4a3-973f-4ed5-b5bd-9533d5779e33",
   "metadata": {},
   "source": [
    "# Code adapted from\n",
    "- https://huggingface.co/learn/nlp-course/en/chapter6/5\n",
    "- http://d2l.ai/chapter_natural-language-processing-pretraining/bert.html\n",
    "- https://github.com/rasbt/LLMs-from-scratch\n",
    "- https://github.com/karpathy/nanoGPT/blob/master/model.py\n",
    "\n",
    "In this notebook, we look into\n",
    "- Input processing for BERT + GPT-2, including BPE\n",
    "- From scratch implementations of BERT, GPT-2, Llama 2, 3, 3.1, 3.2 models (with some training and sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03b3929-c630-4e1d-a208-f4bdab0341dd",
   "metadata": {},
   "source": [
    "## Input Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07f72a3-5f94-43a3-9a11-4e90d8460a16",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2ed5f5c-b8c1-4175-89f6-96c06153574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a14c4cb4-5cf4-4085-a14d-beef91d778b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ea1c3bc-603a-47c5-a7dd-7579f99f0d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hopefully', (0, 9)),\n",
       " (',', (9, 10)),\n",
       " ('Ġyou', (10, 14)),\n",
       " ('Ġwill', (14, 19)),\n",
       " ('Ġbe', (19, 22)),\n",
       " ('Ġable', (22, 27)),\n",
       " ('Ġto', (27, 30)),\n",
       " ('Ġunderstand', (30, 41)),\n",
       " ('Ġhow', (41, 45)),\n",
       " ('Ġthey', (45, 50)),\n",
       " ('Ġare', (50, 54)),\n",
       " ('Ġtrained', (54, 62)),\n",
       " ('Ġand', (62, 66)),\n",
       " ('Ġgenerate', (66, 75)),\n",
       " ('Ġtokens', (75, 82)),\n",
       " ('.', (82, 83))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(corpus[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10c1b8f5-3368-45c5-a452-909d6bd517bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'.': 4, 'This': 3, 'Ġis': 2, 'Ġthe': 1, 'ĠHugging': 1, 'ĠFace': 1, 'ĠCourse': 1, 'Ġchapter': 1, 'Ġabout': 1, 'Ġtokenization': 1, 'Ġsection': 1, 'Ġshows': 1, 'Ġseveral': 1, 'Ġtokenizer': 1, 'Ġalgorithms': 1, 'Hopefully': 1, ',': 1, 'Ġyou': 1, 'Ġwill': 1, 'Ġbe': 1, 'Ġable': 1, 'Ġto': 1, 'Ġunderstand': 1, 'Ġhow': 1, 'Ġthey': 1, 'Ġare': 1, 'Ġtrained': 1, 'Ġand': 1, 'Ġgenerate': 1, 'Ġtokens': 1})\n"
     ]
    }
   ],
   "source": [
    "word_freqs = Counter()\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    word_freqs += Counter([x[0] for x in words_with_offsets])\n",
    "print(word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f83cf009-f9ef-4c24-af4c-099859d0933e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ']\n"
     ]
    }
   ],
   "source": [
    "alphabet = sorted(reduce(lambda a, b : a.union(b), [set(x) for x in word_freqs.keys()]))\n",
    "print(alphabet)\n",
    "vocab = [\"<|endoftext|>\"] + alphabet.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "878976d4-0ab4-4810-967f-205bdd45e144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', ['T', 'h', 'i', 's']),\n",
       " ('Ġis', ['Ġ', 'i', 's']),\n",
       " ('Ġthe', ['Ġ', 't', 'h', 'e'])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
    "list(splits.items())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "312666b4-9327-461b-9b3d-786849ab61d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', 'h'): 3\n",
      "('h', 'i'): 3\n",
      "('i', 's'): 5\n",
      "('Ġ', 'i'): 2\n",
      "('Ġ', 't'): 7\n",
      "('t', 'h'): 3\n"
     ]
    }
   ],
   "source": [
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = Counter()\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs\n",
    "\n",
    "pair_freqs = compute_pair_freqs(splits)\n",
    "\n",
    "for i, key in enumerate(pair_freqs.keys()):\n",
    "    print(f\"{key}: {pair_freqs[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f742d1ac-58a8-4754-8ddb-65ef48a61578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ġ', 't') 7\n"
     ]
    }
   ],
   "source": [
    "best_pair = sorted(pair_freqs, key = lambda x : pair_freqs[x], reverse = True)[0]\n",
    "print(best_pair, pair_freqs[best_pair])\n",
    "merges = {(\"Ġ\", \"t\"): \"Ġt\"}\n",
    "vocab.append(\"Ġt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f132a4a3-b8dc-4c5b-a722-4c5e4f3b6881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġt', 'r', 'a', 'i', 'n', 'e', 'd']\n"
     ]
    }
   ],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n",
    "\n",
    "splits = merge_pair(\"Ġ\", \"t\", splits)\n",
    "print(splits[\"Ġtrained\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "141e05b8-92bd-4c45-86eb-c609bde32782",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair = sorted(pair_freqs, key = lambda x : pair_freqs[x], reverse = True)[0]\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])\n",
    "\n",
    "token_to_id = dict(zip(vocab, range(len(vocab))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3efc2490-d582-492f-9ade-deb08a1fe37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "    splits = reduce(lambda a, b : a+b, splits)\n",
    "    print(splits)\n",
    "    return [token_to_id[x] for x in splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8297e33-3927-42ee-b904-7ba76fa1e78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(token_ids):\n",
    "    id_to_token = dict(zip(token_to_id.values(), token_to_id.keys()))\n",
    "    return \"\".join([id_to_token[x] for x in token_ids]).replace(\"Ġ\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "703e1c3e-4732-4cb8-9c19-449d39bd9367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'Ġis', 'Ġ', 'n', 'o', 't', 'Ġa', 'Ġtoken', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[38, 44, 30, 19, 20, 24, 34, 42, 2]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenize(\"This is not a token.\")\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1f1f6f6-eaff-44ce-8d46-29b9491b0f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is not a token.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1f503-ff84-465c-8e92-0995b8743ab8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### From Text to Input Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2522dd00-c84d-4637-9acc-8376dffd13ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import os\n",
    "import urllib.request\n",
    "input_file = \"../data/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(input_file):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = input_file\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d42410da-3540-4a89-8930-2b68a2e50fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, context_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of context_length\n",
    "        for i in range(0, len(token_ids) - context_length, stride):\n",
    "            input_chunk = token_ids[i:i + context_length]\n",
    "            target_chunk = token_ids[i + 1: i + context_length + 1] # Shift target by 1\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca9f632-5057-458a-b961-32fc178e9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, context_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, context_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f384788-2e3f-4160-a708-cc7ffc6a9c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "context_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, context_length=context_length,\n",
    "    stride=context_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "851a9375-9745-4b4f-9d3e-6d6b78cc13a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([8, 4, 256]) \n",
      "\n",
      "Positional Embedding Shape: torch.Size([4, 256]) \n",
      "\n",
      "Input + Positional Embedding Shape: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tiktoken.get_encoding(\"gpt2\").n_vocab\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(\"Input Shape:\", token_embeddings.shape, '\\n') # batch_size x context_length x output_dim\n",
    "\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(\"Positional Embedding Shape:\", pos_embeddings.shape, '\\n')\n",
    "\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(\"Input + Positional Embedding Shape:\", input_embeddings.shape) # Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cd6bea-1bdb-4f10-8adb-d43d68a92e74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "291a74b4-9afb-4f37-9e6e-160d93731904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ce60bba-9e37-4d1f-8ac1-f66668467060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"Get tokens of the BERT input sequence and their segment IDs.\"\"\"\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    # 0 and 1 are marking segment A and B, respectively\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99525e07-8bf2-4883-aaaa-16188897ea15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[ 1.5738, -0.9070, -0.8343, -0.0515],\n",
       "         [-0.9063,  1.3813, -0.1734, -0.5101],\n",
       "         [-0.2838, -0.6132, -1.6051, -1.1009],\n",
       "         [ 1.0590,  0.5096,  0.7580,  0.4336],\n",
       "         [-0.7619,  0.3362, -0.1701,  1.7476]]], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Parameter(torch.randn(1, 5,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa611775-834e-4efe-b94c-15942169e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"BERT encoder.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n",
    "                 num_blks, dropout, max_len=1000, **kwargs):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(f\"{i}\", d2l.TransformerEncoderBlock(\n",
    "                num_hiddens, ffn_num_hiddens, num_heads, dropout, True))\n",
    "        # In BERT, positional embeddings are learnable, thus we create a\n",
    "        # parameter of positional embeddings that are long enough\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,\n",
    "                                                      num_hiddens))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        # Shape of `X` remains unchanged in the following code snippet:\n",
    "        # (batch size, max sequence length, `num_hiddens`)\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X = X + self.pos_embedding[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6a0f3c6-7145-438b-925b-25848a092945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\n",
    "ffn_num_input, num_blks, dropout = 768, 2, 0.2\n",
    "encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n",
    "                      num_blks, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61ac27c5-8dce-4244-88ee-525cc8fb56e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = torch.randint(0, vocab_size, (2, 8)) # 2 sequences with 8 tokens each\n",
    "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "encoded_X = encoder(tokens, segments, None) \n",
    "encoded_X.shape # hidden/embedding dimension is 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb2931d3-2918-4adf-8254-b3bed43f655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskLM(nn.Module):\n",
    "    \"\"\"The masked language model task of BERT.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, **kwargs):\n",
    "        super(MaskLM, self).__init__(**kwargs)\n",
    "        self.mlp = nn.Sequential(nn.LazyLinear(num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.LayerNorm(num_hiddens),\n",
    "                                 nn.LazyLinear(vocab_size))\n",
    "\n",
    "    def forward(self, X, pred_positions):\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "        batch_size = X.shape[0]\n",
    "        batch_idx = torch.arange(0, batch_size)\n",
    "        # Suppose that `batch_size` = 2, `num_pred_positions` = 3, then\n",
    "        # `batch_idx` is `torch.tensor([0, 0, 0, 1, 1, 1])`\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n",
    "        masked_X = X[batch_idx, pred_positions]\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        return mlm_Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee7c3bf1-203f-47b0-b9e1-15e309af3981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm = MaskLM(vocab_size, num_hiddens)\n",
    "mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])\n",
    "mlm_Y_hat = mlm(encoded_X, mlm_positions)\n",
    "mlm_Y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65f63e17-cec4-4d2f-abb4-e70ec5955f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\n",
    "mlm_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f70d0a5-784b-47d3-9db2-4e2919969709",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextSentencePred(nn.Module):\n",
    "    \"\"\"The next sentence prediction task of BERT.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(NextSentencePred, self).__init__(**kwargs)\n",
    "        self.output = nn.LazyLinear(2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # `X` shape: (batch size, `num_hiddens`)\n",
    "        return self.output(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c2eb345-2838-4301-b816-e3a3b14fe13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch by default will not flatten the tensor as seen in mxnet where, if\n",
    "# flatten=True, all but the first axis of input data are collapsed together\n",
    "encoded_X = torch.flatten(encoded_X, start_dim=1)\n",
    "# input_shape for NSP: (batch size, `num_hiddens`)\n",
    "nsp = NextSentencePred()\n",
    "nsp_Y_hat = nsp(encoded_X)\n",
    "nsp_Y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91230e58-1d84-4db6-8304-07c415be55e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsp_y = torch.tensor([0, 1])\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "nsp_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55fd1077-d724-451e-b336-e7a1d8013b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTModel(nn.Module):\n",
    "    \"\"\"The BERT model.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,\n",
    "                 num_heads, num_blks, dropout, max_len=1000):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens,\n",
    "                                   num_heads, num_blks, dropout,\n",
    "                                   max_len=max_len)\n",
    "        self.fc = nn.Sequential(nn.LazyLinear(num_hiddens),\n",
    "                                    nn.Tanh())\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens)\n",
    "        self.nsp = NextSentencePred()\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        # encoded_X is of shape (batch_size, seq_len, num_hiddens)\n",
    "        nsp_input = self.fc(encoded_X[:, 0, :])\n",
    "        nsp_Y_hat = self.nsp(nsp_input)\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f665f07e-eced-4f33-a76e-ac3a56662230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads input in the form we want it\n",
    "# Processing is done in https://d2l.ai/chapter_natural-language-processing-pretraining/bert-dataset.html, this is important to understand but we skip for brevity\n",
    "\n",
    "import os\n",
    "import random\n",
    "batch_size, max_len = 512, 64\n",
    "data_dir = '../data/WikiText' # Download wikitext-2-v1 from https://www.kaggle.com/datasets/bestwater/wikitext-2-v1 to this directory\n",
    "paragraphs = d2l._read_wiki(data_dir)\n",
    "train_set = d2l._WikiTextDataset(paragraphs, max_len)\n",
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
    "                                    shuffle=True, num_workers=d2l.get_dataloader_workers())\n",
    "vocab = train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36d42680-d169-4122-af5b-e7ce0f63986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = d2l.BERTModel(len(vocab), num_hiddens=128,\n",
    "                    ffn_num_hiddens=256, num_heads=2, num_blks=2, dropout=0.2)\n",
    "devices = d2l.try_all_gpus()\n",
    "loss = nn.CrossEntropyLoss(reduction = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "56285172-bad2-458e-b19b-87032697d0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_batch_loss_bert(net, loss, vocab_size, tokens_X,\n",
    "                         segments_X, valid_lens_x,\n",
    "                         pred_positions_X, mlm_weights_X,\n",
    "                         mlm_Y, nsp_y):\n",
    "    # Forward pass\n",
    "    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,\n",
    "                                  valid_lens_x.reshape(-1),\n",
    "                                  pred_positions_X)\n",
    "    # Compute masked language model loss\n",
    "    # mlm_weights_X here prevents us from computing loss on padded tokens\n",
    "    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\\\n",
    "    mlm_weights_X.reshape(-1, 1)\n",
    "    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)\n",
    "    # Compute next sentence prediction loss\n",
    "    nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "    l = mlm_l + nsp_l\n",
    "    return mlm_l, nsp_l, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e977623-8f8f-4d31-8f3f-85b32b304e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):\n",
    "    net(*next(iter(train_iter))[:4])\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "    step, timer = 0, d2l.Timer()\n",
    "    animator = d2l.Animator(xlabel='step', ylabel='loss',\n",
    "                            xlim=[1, num_steps], legend=['mlm', 'nsp'])\n",
    "    # Sum of masked language modeling losses, sum of next sentence prediction\n",
    "    # losses, no. of sentence pairs, count\n",
    "    metric = d2l.Accumulator(4)\n",
    "    num_steps_reached = False\n",
    "    device = torch.device('cpu') if not devices else devices[0]\n",
    "    while step < num_steps and not num_steps_reached:\n",
    "        for tokens_X, segments_X, valid_lens_x, pred_positions_X,\\\n",
    "            mlm_weights_X, mlm_Y, nsp_y in train_iter:\n",
    "            tokens_X = tokens_X.to(device)\n",
    "            segments_X = segments_X.to(device)\n",
    "            valid_lens_x = valid_lens_x.to(device) # This marks the padded indices, to be used in masking in self-attention\n",
    "            pred_positions_X = pred_positions_X.to(device)\n",
    "            mlm_weights_X = mlm_weights_X.to(device) # This \n",
    "            mlm_Y, nsp_y = mlm_Y.to(device), nsp_y.to(device)\n",
    "            trainer.zero_grad()\n",
    "            timer.start()\n",
    "            mlm_l, nsp_l, l = _get_batch_loss_bert(\n",
    "                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,\n",
    "                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)\n",
    "            timer.stop()\n",
    "            animator.add(step + 1,\n",
    "                         (metric[0] / metric[3], metric[1] / metric[3]))\n",
    "            step += 1\n",
    "            if step == num_steps:\n",
    "                num_steps_reached = True\n",
    "                break\n",
    "\n",
    "    print(f'MLM loss {metric[0] / metric[3]:.3f}, '\n",
    "          f'NSP loss {metric[1] / metric[3]:.3f}')\n",
    "    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '\n",
    "          f'{str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40397aae-1a35-4079-9c7c-08c5d97e52df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bert(train_iter, net, loss, len(vocab), devices, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6cf627dc-3361-49bb-acb9-2c2058f609af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_encoding(net, tokens_a, tokens_b=None):\n",
    "    tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
    "    device = torch.device('cpu') if not devices else devices[0]\n",
    "    token_ids = torch.tensor(vocab[tokens], device=device).unsqueeze(0)\n",
    "    segments = torch.tensor(segments, device=device).unsqueeze(0)\n",
    "    valid_len = torch.tensor(len(tokens), device=device).unsqueeze(0)\n",
    "    encoded_X, _, _ = net(token_ids, segments, valid_len)\n",
    "    return encoded_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e93a559f-d3a1-4fe9-867b-9cbdbbdc8ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 128]),\n",
       " torch.Size([1, 128]),\n",
       " tensor([0.0901, 0.3364, 0.9336], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_a = ['a', 'crane', 'is', 'flying']\n",
    "encoded_text = get_bert_encoding(net, tokens_a)\n",
    "# Tokens: '<cls>', 'a', 'crane', 'is', 'flying', '<sep>'\n",
    "encoded_text_cls = encoded_text[:, 0, :]\n",
    "encoded_text_crane = encoded_text[:, 2, :]\n",
    "encoded_text.shape, encoded_text_cls.shape, encoded_text_crane[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "faa7827a-7e3d-43fb-b000-8b3baadb9735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 128]),\n",
       " torch.Size([1, 128]),\n",
       " tensor([0.3325, 0.0918, 1.0524], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_a, tokens_b = ['a', 'crane', 'driver', 'came'], ['he', 'just', 'left']\n",
    "encoded_pair = get_bert_encoding(net, tokens_a, tokens_b)\n",
    "# Tokens: '<cls>', 'a', 'crane', 'driver', 'came', '<sep>', 'he', 'just',\n",
    "# 'left', '<sep>'\n",
    "encoded_pair_cls = encoded_pair[:, 0, :]\n",
    "encoded_pair_crane = encoded_pair[:, 2, :]\n",
    "encoded_pair.shape, encoded_pair_cls.shape, encoded_pair_crane[0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768911c-a6f8-4fc1-9e7d-6cdc294de87f",
   "metadata": {},
   "source": [
    "Notice how the first 3 values (out of 128) of \"crane\" is now different due to context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05490fdc-6025-4ea8-9dfe-7c94b454ac5f",
   "metadata": {},
   "source": [
    "## GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fcc662-61c1-4fb9-81b8-3e00f2766069",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ab3b9e-7130-4da7-b91b-8ba366245ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "import matplotlib\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f09d271-4fe0-4142-8fef-046df0bfc0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall structure\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "        self.tok_emb.weight = self.out_head.weight # weight_tying, Raschka suggests that the model is easier to train without weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/((2 * cfg[\"n_layers\"]))**0.5)\n",
    "\n",
    "    def _init_weights(self, module, std = 0.02):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.dropout(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce40ef4a-fbae-4a6d-968c-8274339889cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "396f0d6b-11e0-440c-a577-6eadb001795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GELU approximation \n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "580947be-98a2-4f8c-ba15-f0ed54fe5f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP in Transformer Block\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"])\n",
    "        self.gelu = GELU()\n",
    "        self.proj = nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(self.gelu(self.fc(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0138b475-81d4-4ee0-8f07-a26aff7b0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07fa8e4a-1674-482d-ba11-f35bc996965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length, dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_out % num_heads == 0, \"embed_dim is indivisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.context_length = context_length\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, embed_dim = x.shape\n",
    "\n",
    "        # (b, num_tokens, embed_dim) --> (b, num_tokens, 3 * embed_dim)\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.view(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "        # (b, num_heads, num_tokens, head_dim)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # (b, num_heads, num_tokens, num_tokens)\n",
    "        attn_scores = torch.einsum('bhij,bhkj->bhik', q, k)\n",
    "        attn_scores = attn_scores.masked_fill(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
    "        )\n",
    "        attn_weights = torch.softmax(attn_scores / k.shape[-1]**-0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        # (b, num_heads, num_tokens, head_dim)\n",
    "        context_vec = torch.einsum('bhij,bhjk->bhik', attn_weights, v)\n",
    "\n",
    "        # (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = context_vec.transpose(1, 2)\n",
    "        context_vec = context_vec.contiguous().view(batch_size, num_tokens, embed_dim)\n",
    "        context_vec = self.proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbe91230-9281-46f8-8926-5903ee305a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "308b5985-ef4a-4b16-a112-5f04e0e6d9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[ 0.8853,  0.0331, -0.4912,  ...,  0.2467, -0.5241, -0.0369],\n",
      "        [-0.0903,  0.3258, -0.2128,  ..., -0.0771, -0.1042, -0.0471],\n",
      "        [-0.0910,  0.3692,  0.3642,  ...,  0.6514,  0.0632,  0.0331]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out[0,:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4acebfad-f89c-455c-888a-ecea0d8a01b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 123,822,336\n",
      "Total size of the model: 472.34 MB\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "# Calculate the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "total_size_bytes = total_params * 4\n",
    "# Convert to megabytes\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "76ff5a37-5c27-4975-a99c-8451432e1469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature = 1, top_k = None, eos_id = None):\n",
    "    # idx has shape (batch, n_tokens) \n",
    "    # We will continually append to idx\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop current context to supported context size\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        # (batch, n_tokens, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "        if top_k is not None: # relevant when temperature is high\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "        logits /= temperature\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples = 1)  # (batch, 1)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ebbcad95-f39b-4fe2-8bea-4a55b379de81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "c0496fd3-9de3-43a9-b4df-1db766d71884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 14849, 41272, 26225, 42482, 15737, 18450]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() # disable dropout\n",
    "out = generate(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "92a4ab49-8c4b-4b73-80c9-873a03f19568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I amgary Meng airlines NX dealers Imagine\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3c6c5f76-7dae-4e97-9d7e-bdbfa1ca12e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For later sections\n",
    "\n",
    "seed = 123\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.mps.manual_seed(seed)\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "device = torch.device(\"mps\") if torch.mps.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c297556e-7b94-4eec-9306-25befdcd5867",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60d20f12-d0d7-4ade-9def-c30c58bd50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   \n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        \n",
    "    \"n_heads\": 12,        \n",
    "    \"n_layers\": 12,        \n",
    "    \"drop_rate\": 0.1,      \n",
    "    \"qkv_bias\": False      \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0787fa59-9e20-4769-8651-64a960625261",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"../data/the-verdict.txt\"\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0158e690-23c5-4023-83e7-00fe24b37af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a57aafc-363f-4c31-884a-fa1beb650be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "set_seed(seed)\n",
    "\n",
    "# Defined above\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    context_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    context_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3847e02c-7e4b-4059-98df-62452385fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3023159-6cd6-4e28-ae50-a6ec3d087efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_set(data_loader, model, device, num_batches=None):\n",
    "    '''\n",
    "    num_batches : maximum number of batches to compute loss over\n",
    "    '''\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the num_batches if data loader doesn't have as many\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c263b61-a578-4713-92db-1a8b74a03f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.9887482325236\n",
      "Validation loss: 10.981653213500977\n"
     ]
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "set_seed(seed)\n",
    "with torch.no_grad(): \n",
    "    train_loss = calc_loss_set(train_loader, model, device)\n",
    "    val_loss = calc_loss_set(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "486f739f-46be-4991-9f95-bdc87e931637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_set(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_set(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context, temperature = 1):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size, temperature = temperature\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "de826e1b-851a-4aaa-8148-1433c6d487d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "# Remove weight tying and weight init code, which sees to make training harder for this example\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 30\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5657c57e-f784-411b-8f2c-5d723292f004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f6b3fa42-e920-43da-8f8d-86a08a992bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "\n",
      "  through the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard,\n",
      "\n",
      "Actual Text:\n",
      "\n",
      "through the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own trade hardly a murmur. Professional jealousy? Perhaps. If it were, the ho\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated Text:\\n\")\n",
    "generate_and_print_sample(model, tokenizer, device, ' ', temperature = 0.01)\n",
    "print(\"\\nActual Text:\\n\")\n",
    "train_text = \"\"\n",
    "for input_batch, target_batch in train_loader:\n",
    "    for batch in input_batch:\n",
    "        train_text += token_ids_to_text(batch, tokenizer)\n",
    "phrase = 'through the prism '\n",
    "print(train_text[train_text.index(phrase) : train_text.index(phrase) + 300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc614653-9e80-420e-9a2c-0a08cd836364",
   "metadata": {},
   "source": [
    "We see that the model just learnt to memorize the training sample!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a1fe47-664a-4904-a3a5-5901f3b2fdad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### PreTrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06559152-14be-488d-a01a-dff09fcad9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-from-scratch/main/ch05/\"\n",
    "    \"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split(\"/\")[-1]\n",
    "urllib.request.urlretrieve(url, f'./raschka_files/{filename}')\n",
    "from raschka_files.gpt_download import download_and_load_gpt2\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4ba4ca55-3897-4d64-9105-6988277d2c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n",
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())\n",
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "572bda9f-41d7-43af-b073-71ac82d086d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0be1f0bf-90de-4e30-bbc3-1f600f699ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "075759bc-d07f-4a33-966b-95ab812e7844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        gpt.trf_blocks[b].att.qkv.weight = assign(\n",
    "            gpt.trf_blocks[b].att.qkv.weight, \n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.qkv.bias = assign(\n",
    "            gpt.trf_blocks[b].att.qkv.bias, \n",
    "            ((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"]))\n",
    "        gpt.trf_blocks[b].att.proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.fc.weight = assign(\n",
    "            gpt.trf_blocks[b].ff.fc.weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.fc.bias = assign(\n",
    "            gpt.trf_blocks[b].ff.fc.bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.proj.weight = assign(\n",
    "            gpt.trf_blocks[b].ff.proj.weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.proj.bias = assign(\n",
    "            gpt.trf_blocks[b].ff.proj.bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    \n",
    "load_weights_into_gpt(gpt, params)\n",
    "device = torch.device(\"mps\") if torch.mps.is_available() else torch.device(\"cpu\")\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7108d629-285f-4ff1-b546-f72b5799bc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you must pay attention to its right money, in the first is the winner\" to be exerted. All For Sure On My.\n"
     ]
    }
   ],
   "source": [
    "set_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bad3dd-ce57-4995-8e11-e5abc518aa4d",
   "metadata": {},
   "source": [
    "## GPT-2 to Llama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "88c6c80c-8563-4a06-b816-2913bab560d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# To replace Layer Norm\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.emb_dim = emb_dim\n",
    "        self.weight = nn.Parameter(torch.ones(emb_dim)).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        means = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        x_normed = x * torch.rsqrt(means + self.eps)\n",
    "        return (x_normed * self.weight).to(dtype=x.dtype)\n",
    "\n",
    "# To replace GELU\n",
    "class SiLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiLU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "# In attention block\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.proj = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.silu = SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = self.silu(x_fc1) * x_fc2\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26863c78-b6da-42cd-8e15-70c8b67cf202",
   "metadata": {},
   "source": [
    "See [RoPE notes](../08_attention_transformers/notes.md) for details if next cell is confusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9fcfb622-88d9-4c37-a31b-21cc4a7747ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_rope_params(head_dim, theta_base=10_000, context_length=4096, freq_config=None):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "    thetas = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "    ms = torch.arange(context_length)\n",
    "    # Shape: (context_length, head_dim // 2)\n",
    "    angles = ms[:, None] * thetas[None, :]  \n",
    "    # Expand angles to match the head_dim, Shape: (context_length, head_dim)\n",
    "    angles = torch.cat([angles, angles], dim=1)\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "    return cos, sin # matrices\n",
    "\n",
    "def compute_rope(x, cos, sin):\n",
    "    # x: (batch_size, num_heads, num_tokens, head_dim)\n",
    "    batch_size, num_heads, num_tokens, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    # Adjust sin and cos shapes\n",
    "    cos = cos[:num_tokens, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, num_tokens, head_dim)\n",
    "    sin = sin[:num_tokens, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Reorder matrix columns\n",
    "    x1 = x[..., : head_dim // 2]  # First half\n",
    "    x2 = x[..., head_dim // 2 :]  # Second half\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b09c699a-7dbb-40ff-8116-e9f607a71d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the mask, cos, and sin tensors in the transformer blocks to improve efficiency\n",
    "class SharedBuffers:\n",
    "    _buffers = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_buffers(context_length, head_dim, dtype=torch.float32, rope_base = 10_000, freq_config = None):\n",
    "        key = (context_length, head_dim, dtype, rope_base, tuple(freq_config.values()) if freq_config else freq_config)\n",
    "\n",
    "        if key not in SharedBuffers._buffers:\n",
    "            mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "            cos, sin = precompute_rope_params(head_dim, rope_base, context_length, freq_config)\n",
    "            if dtype is not None:\n",
    "                cos = cos.to(dtype)\n",
    "                sin = sin.to(dtype)\n",
    "            SharedBuffers._buffers[key] = (mask, cos, sin)\n",
    "\n",
    "        return SharedBuffers._buffers[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2605cd10-f41b-4f2c-a336-37b4b0ce6f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compared to GPTBlock, we \n",
    "# - Remove bias\n",
    "# - Add dtype\n",
    "# - Add RoPE\n",
    "# - Remove dropout\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length, dtype=None):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_out % num_heads == 0, \"embed_dim is indivisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.qkv = nn.Linear(d_in, 3 * d_out, bias = False, dtype=dtype)\n",
    "        self.proj = nn.Linear(d_out, d_out, bias = False, dtype=dtype)\n",
    "\n",
    "        mask, cos, sin = SharedBuffers.get_buffers(context_length, self.head_dim, dtype)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "        self.register_buffer(\"cos\", cos)\n",
    "        self.register_buffer(\"sin\", sin)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, embed_dim = x.shape\n",
    "\n",
    "        # (b, num_tokens, embed_dim) --> (b, num_tokens, 3 * embed_dim)\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.view(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "        # (b, num_heads, num_tokens, head_dim)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
    "        # RoPE\n",
    "        k = compute_rope(k, self.cos, self.sin)\n",
    "        q = compute_rope(q, self.cos, self.sin)\n",
    "        \n",
    "        # (b, num_heads, num_tokens, num_tokens)\n",
    "        attn_scores = torch.einsum('bhij,bhkj->bhik', q, k)\n",
    "        attn_scores = attn_scores.masked_fill(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
    "        )\n",
    "        attn_weights = torch.softmax(attn_scores / k.shape[-1]**-0.5, dim=-1)\n",
    "        # (b, num_heads, num_tokens, head_dim)\n",
    "        context_vec = torch.einsum('bhij,bhjk->bhik', attn_weights, v)\n",
    "\n",
    "        # (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = context_vec.transpose(1, 2)\n",
    "        context_vec = context_vec.contiguous().view(batch_size, num_tokens, embed_dim)\n",
    "        context_vec = self.proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "673c9ac8-d789-4be3-9036-8a4a47597dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compared to GPTBlock, we \n",
    "# - Remove dropout\n",
    "# - Remove bias\n",
    "# - Use RMSNorm\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dtype=cfg[\"dtype\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = RMSNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = RMSNorm(cfg[\"emb_dim\"])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = x + shortcut  \n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = x + shortcut  \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d651a988-f1d1-491b-a1fa-59d857f5d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compared to GPTBlock, we \n",
    "# - Remove dropout\n",
    "# - Remove bias\n",
    "# - Use RMSNorm\n",
    "# - Remove mentions of positional embedding (MHA creates these automatically)\n",
    "class Llama2Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        x = tok_embeds  \n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0054c043-cfc5-47b7-841a-2467dee223a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compared to 1.5B parameter GPT Model\n",
    "# Note the multiples of 32/64!\n",
    "# We no longer specify dropout rate and qkv bias because these are removed\n",
    "LLAMA2_CONFIG_7B = {\n",
    "    \"vocab_size\": 32000,     # From 50257\n",
    "    \"context_length\": 4096,  # From 1024\n",
    "    \"emb_dim\": 4096,         # From 1600\n",
    "    \"n_heads\": 32,           # From 25\n",
    "    \"n_layers\": 32,          # From 48\n",
    "    \"hidden_dim\": 11008,     # NEW: Size of the intermediate dimension in FeedForward, used to be 4 x emb_dim\n",
    "    \"dtype\": torch.bfloat16  # NEW: Lower-precision dtype to reduce memory usage\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ce8c2ca2-1698-4af9-bee7-a8a68092f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Llama2Model(LLAMA2_CONFIG_7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2f73cd7b-f0ad-4f5f-8924-ebefc6d215e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 6,738,415,616\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d6aeddff-3364-4832-9bac-a7334d64782f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bfloat16: 25.14 GB\n"
     ]
    }
   ],
   "source": [
    "def model_memory_size(model, input_dtype=torch.float32):\n",
    "    total_params = 0\n",
    "    total_grads = 0\n",
    "    for param in model.parameters():\n",
    "        # Calculate total number of elements per parameter\n",
    "        param_size = param.numel()\n",
    "        total_params += param_size\n",
    "        # Check if gradients are stored for this parameter\n",
    "        if param.requires_grad:\n",
    "            total_grads += param_size\n",
    "\n",
    "    # Calculate buffer size (non-parameters that require memory)\n",
    "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
    "\n",
    "    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
    "    # We assume parameters and gradients are stored in the same type as input dtype\n",
    "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
    "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
    "\n",
    "    # Convert bytes to gigabytes\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "    return total_memory_gb\n",
    "\n",
    "# Uncommenting because local machine doesn't have much memory left but this should show almost 2x as much\n",
    "# print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\") \n",
    "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00f7fe7c-609c-4318-86dd-ea64358474d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1752939-f1b7-4152-aaec-95521e84228c",
   "metadata": {},
   "source": [
    "## Llama 2 to Llama 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8998b2c5-ec25-44fb-9d9a-679535c527f4",
   "metadata": {},
   "source": [
    "### Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7b98f989-ebb1-4f5f-9f57-d75de180b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA2_CONFIG_7B = {\n",
    "    \"vocab_size\": 32_000,    \n",
    "    \"context_length\": 4096,  \n",
    "    \"emb_dim\": 4096,         \n",
    "    \"n_heads\": 32,           \n",
    "    \"n_layers\": 32,          \n",
    "    \"hidden_dim\": 11_008,    \n",
    "    \"dtype\": torch.bfloat16  \n",
    "}\n",
    "LLAMA3_CONFIG_8B = {\n",
    "    \"vocab_size\": 128_256,   # NEW\n",
    "    \"context_length\": 8192,  # NEW\n",
    "    \"emb_dim\": 4096,         \n",
    "    \"n_heads\": 32,           \n",
    "    \"n_layers\": 32,          \n",
    "    \"hidden_dim\": 14_336,    # NEW\n",
    "    \"n_kv_groups\": 8,        # NEW: grouped-query attention\n",
    "    \"rope_base\": 500_000.0,  # NEW\n",
    "    \"rope_freq\": None,       \n",
    "    \"dtype\": torch.bfloat16,\n",
    "    \"weight_tying\": False\n",
    "}\n",
    "LLAMA31_CONFIG_8B = LLAMA3_CONFIG_8B.copy()\n",
    "LLAMA31_CONFIG_8B.update({\n",
    "    \"context_length\": 131_072, \n",
    "    \"rope_freq\": {              \n",
    "        \"factor\": 8.0,\n",
    "        \"low_freq_factor\": 1.0,\n",
    "        \"high_freq_factor\": 4.0,\n",
    "        \"original_context_length\": 8192,\n",
    "    }\n",
    "})\n",
    "LLAMA32_CONFIG_1B = LLAMA31_CONFIG_8B.copy()\n",
    "LLAMA32_CONFIG_1B.update({\n",
    "    \"emb_dim\": 2048,            # Half\n",
    "    \"n_layers\": 16,             # Half\n",
    "    \"hidden_dim\": 8192,         # Almost half\n",
    "    \"rope_freq\": {               \n",
    "        \"factor\": 32.0,         # Adjustment of the rescaling factor\n",
    "        \"low_freq_factor\": 1.0,\n",
    "        \"high_freq_factor\": 4.0,\n",
    "        \"original_context_length\": 8192,\n",
    "    },\n",
    "    \"weight_tying\": True        # New\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "007f87b5-0a3c-40f6-a82e-e54162f8e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale down low frequencies if context window is extrapolated\n",
    "def precompute_rope_params(head_dim, theta_base=10_000, context_length=4096, freq_config=None):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "    #thetas\n",
    "    freqs = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "    if freq_config is not None:\n",
    "        low_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"low_freq_factor\"]\n",
    "        high_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"high_freq_factor\"]\n",
    "        wavelens = 2 * torch.pi / freqs\n",
    "        # When wavelengths > context_length, decrease frequency\n",
    "        freqs_llama = torch.where(\n",
    "            wavelens > low_freq_wavelen, freqs / freq_config[\"factor\"], freqs\n",
    "        )\n",
    "        smooth_factor = (freq_config[\"original_context_length\"] / wavelens - freq_config[\"low_freq_factor\"]) / (\n",
    "            freq_config[\"high_freq_factor\"] - freq_config[\"low_freq_factor\"]\n",
    "        )\n",
    "        smoothed_freqs = (\n",
    "            (1 - smooth_factor) * (freqs / freq_config[\"factor\"]) + smooth_factor * freqs\n",
    "        )\n",
    "        is_medium_freq = (wavelens <= low_freq_wavelen) & (wavelens >= high_freq_wavelen)\n",
    "        # When 1/4*context_length < wavelengths < context_length, decrease frequency by a smoothed amount\n",
    "        freqs_llama = torch.where(is_medium_freq, smoothed_freqs, freqs_llama)\n",
    "        freqs = freqs_llama\n",
    "    ms = torch.arange(context_length)\n",
    "    # Shape: (context_length, head_dim // 2)\n",
    "    angles = ms[:, None] * freqs[None, :]  \n",
    "    # Expand angles to match the head_dim, Shape: (context_length, head_dim)\n",
    "    angles = torch.cat([angles, angles], dim=1)\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "    return cos, sin # matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a96d8bb2-8b81-43c7-84bc-f21895a7f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is oldf\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length, dtype=None):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_out % num_heads == 0, \"embed_dim is indivisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.qkv = nn.Linear(d_in, 3 * d_out, bias = False, dtype=dtype)\n",
    "        self.proj = nn.Linear(d_out, d_out, bias = False, dtype=dtype)\n",
    "\n",
    "        mask, cos, sin = SharedBuffers.get_buffers(context_length, self.head_dim, dtype)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "        self.register_buffer(\"cos\", cos)\n",
    "        self.register_buffer(\"sin\", sin)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, embed_dim = x.shape\n",
    "\n",
    "        # (b, num_tokens, embed_dim) --> (b, num_tokens, 3 * embed_dim)\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.view(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "        # (b, num_heads, num_tokens, head_dim)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
    "        # RoPE\n",
    "        k = compute_rope(k, self.cos, self.sin)\n",
    "        q = compute_rope(q, self.cos, self.sin)\n",
    "        \n",
    "        # (b, num_heads, num_tokens, num_tokens)\n",
    "        attn_scores = torch.einsum('bhij,bhkj->bhik', q, k)\n",
    "        attn_scores = attn_scores.masked_fill(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
    "        )\n",
    "        attn_weights = torch.softmax(attn_scores / k.shape[-1]**-0.5, dim=-1)\n",
    "        # (b, num_heads, num_tokens, head_dim)\n",
    "        context_vec = torch.einsum('bhij,bhjk->bhik', attn_weights, v)\n",
    "\n",
    "        # (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = context_vec.transpose(1, 2)\n",
    "        context_vec = context_vec.contiguous().view(batch_size, num_tokens, embed_dim)\n",
    "        context_vec = self.proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "86f8d1f1-3c01-40a8-b12d-2d1f72372ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reduces the number of parameters by around a third\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self, d_in, d_out, context_length, num_heads,\n",
    "            num_kv_groups,       # NEW\n",
    "            rope_base=10_000,    # NEW\n",
    "            rope_config=None,    # NEW\n",
    "            dtype=None\n",
    "        ):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"  # NEW\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.q_to_k_ratio = num_heads // num_kv_groups\n",
    "        self.qkv = nn.Linear(d_in, d_out + 2 * num_kv_groups * self.head_dim, bias = False, dtype=dtype)\n",
    "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)\n",
    "\n",
    "        mask, cos, sin = SharedBuffers.get_buffers(context_length, self.head_dim, dtype, rope_base, rope_config)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "        self.register_buffer(\"cos\", cos)\n",
    "        self.register_buffer(\"sin\", sin)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, embed_dim = x.shape\n",
    "\n",
    "        # (b, num_tokens, embed_dim) --> (b, num_tokens, embed_dim + 2*num_kv_groups*head_dim)\n",
    "        qkv = self.qkv(x)\n",
    "        # (b, num_heads, num_tokens, head_dim)\n",
    "        q = qkv[:,:,:embed_dim].view(batch_size, num_tokens, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        # (b, num_kv_groups, num_tokens, head_dim)\n",
    "        k, v = qkv[:,:,embed_dim:].view(batch_size, num_tokens, 2, self.num_kv_groups, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        # RoPE\n",
    "        k = compute_rope(k, self.cos, self.sin)\n",
    "        q = compute_rope(q, self.cos, self.sin)\n",
    "        # Expand keys and values to match the number of heads, Shape: (b, num_heads, num_tokens, head_dim)\n",
    "        # Repeat_interleave does [K1, K1, K2, K2], while repeat does [K1, K2, K1, K2]\n",
    "        k = k.repeat_interleave(self.q_to_k_ratio, dim=1) \n",
    "        v = v.repeat_interleave(self.q_to_k_ratio, dim=1)  \n",
    "\n",
    "        # (b, num_heads, num_tokens, num_tokens)\n",
    "        attn_scores = torch.einsum('bhij,bhkj->bhik', q, k)\n",
    "        attn_scores = attn_scores.masked_fill(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
    "        )\n",
    "        attn_weights = torch.softmax(attn_scores / k.shape[-1]**-0.5, dim=-1)\n",
    "        # (b, num_heads, num_tokens, head_dim)\n",
    "        context_vec = torch.einsum('bhij,bhjk->bhik', attn_weights, v)\n",
    "\n",
    "        # (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = context_vec.transpose(1, 2)\n",
    "        context_vec = context_vec.contiguous().view(batch_size, num_tokens, embed_dim)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "8d143d56-e224-412b-a12e-9860b6844c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att =  GroupedQueryAttention(  \n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            num_kv_groups=cfg[\"n_kv_groups\"],  # NEW\n",
    "            rope_base=cfg[\"rope_base\"],        # NEW\n",
    "            rope_config=cfg[\"rope_freq\"],      # NEW\n",
    "            dtype=cfg[\"dtype\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "        self.norm2 = RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x.to(torch.bfloat16)) \n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x.to(torch.bfloat16))\n",
    "        x = x + shortcut  \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "a9d47294-d2fb-4f46-b322-ed019e356240",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama3Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "        if cfg[\"weight_tying\"]:\n",
    "            self.tok_emb.weight = self.out_head.weight\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        x = tok_embeds\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x.to(torch.bfloat16))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb6ba25-7a37-4cc2-8499-adb57e32c62d",
   "metadata": {},
   "source": [
    "### Pre-Trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9e5a9341-8c03-4718-8221-f320f1f21048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New RoPE theta: 31250.0\n"
     ]
    }
   ],
   "source": [
    "# Rescale base wavelength to run on smaller machine\n",
    "def rescale_theta(theta_old, context_length_old, context_length_new):\n",
    "    scaling_factor = context_length_new / context_length_old\n",
    "    theta_new = theta_old * scaling_factor\n",
    "    return theta_new\n",
    "\n",
    "# Change context length\n",
    "old_context_length = LLAMA32_CONFIG_1B[\"context_length\"]\n",
    "LLAMA32_CONFIG_1B[\"context_length\"] = 8192\n",
    "\n",
    "LLAMA32_CONFIG_1B[\"rope_base\"] = rescale_theta(\n",
    "    LLAMA32_CONFIG_1B[\"rope_base\"],\n",
    "    old_context_length,\n",
    "    LLAMA32_CONFIG_1B[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"New RoPE theta:\", LLAMA32_CONFIG_1B[\"rope_base\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "509b2e94-6cab-4b67-9972-87d396f36ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, model_path):\n",
    "        assert os.path.isfile(model_path), f\"Model file {model_path} not found\"\n",
    "        mergeable_ranks = load_tiktoken_bpe(model_path)\n",
    "\n",
    "        self.special_tokens = {\n",
    "            \"<|begin_of_text|>\": 128000,\n",
    "            \"<|end_of_text|>\": 128001,\n",
    "            \"<|start_header_id|>\": 128006,\n",
    "            \"<|end_header_id|>\": 128007,\n",
    "            \"<|eot_id|>\": 128009,\n",
    "        }\n",
    "        self.special_tokens.update({\n",
    "            f\"<|reserved_{i}|>\": 128002 + i for i in range(256) if (128002 + i) not in self.special_tokens.values()\n",
    "        })\n",
    "\n",
    "        self.model = tiktoken.Encoding(\n",
    "            name=Path(model_path).name,\n",
    "            pat_str=r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\",\n",
    "            mergeable_ranks=mergeable_ranks,\n",
    "            special_tokens=self.special_tokens\n",
    "        )\n",
    "\n",
    "\n",
    "    def encode(self, text, bos=False, eos=False, allowed_special=set(), disallowed_special=()):\n",
    "        if bos:\n",
    "            tokens = [self.special_tokens[\"<|begin_of_text|>\"]]\n",
    "        else:\n",
    "            tokens = []\n",
    "\n",
    "        tokens += self.model.encode(text, allowed_special=allowed_special, disallowed_special=disallowed_special)\n",
    "\n",
    "        if eos:\n",
    "            tokens.append(self.special_tokens[\"<|end_of_text|>\"])\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return self.model.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "58b39da4-d277-416e-9764-fa642863d511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login(token=) # Fill this in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "afa43a2a-2651-44d0-b270-f392c90fa731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "tokenizer_file_path = hf_hub_download(\n",
    "    repo_id=\"meta-llama/Llama-3.2-1B\",\n",
    "    filename=\"original/tokenizer.model\",\n",
    "    local_dir=\"Llama-3.2-1B\"\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(tokenizer_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "d8037700-761b-4f79-b10f-0a66595989ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1,235,814,400\n"
     ]
    }
   ],
   "source": [
    "model = Llama3Model(LLAMA32_CONFIG_1B)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "060587bf-a28f-43c6-affa-954e42f8936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "# weights_file = hf_hub_download(\n",
    "#     repo_id=\"meta-llama/Llama-3.2-1B\",\n",
    "#     filename=f\"model.safetensors\",\n",
    "#     local_dir=\"Llama-3.2-1B\"\n",
    "# )\n",
    "weights_file = './Llama-3.2-1B/.cache/huggingface/download/original/model.safetensors'\n",
    "current_weights = load_file(weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1f4e9309-133b-42da-b0f3-fbfe6d2f05eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trf_blocks.0.att.qkv.weight\n",
      "trf_blocks.0.att.out_proj.weight\n",
      "trf_blocks.0.ff.fc1.weight\n",
      "trf_blocks.0.ff.fc2.weight\n",
      "trf_blocks.0.ff.proj.weight\n",
      "trf_blocks.0.norm1.weight\n",
      "trf_blocks.0.norm2.weight\n"
     ]
    }
   ],
   "source": [
    "for pn, p in model.named_parameters():\n",
    "    if \".0.\" in pn:\n",
    "        print(pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b0ec5ab8-3f2a-4914-a7eb-5a53d75b9791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights_into_llama(model, param_config, params):\n",
    "    model.tok_emb.weight = assign(model.tok_emb.weight, params[\"model.embed_tokens.weight\"])\n",
    "\n",
    "    for l in range(param_config[\"n_layers\"]):\n",
    "\n",
    "        # Load attention weights\n",
    "        model.trf_blocks[l].att.qkv.weight = assign(\n",
    "            model.trf_blocks[l].att.qkv.weight,\n",
    "            torch.cat([params[f\"model.layers.{l}.self_attn.q_proj.weight\"], \n",
    "                       params[f\"model.layers.{l}.self_attn.k_proj.weight\"], \n",
    "                       params[f\"model.layers.{l}.self_attn.v_proj.weight\"]], axis = 0)\n",
    "        )\n",
    "        model.trf_blocks[l].att.out_proj.weight = assign(\n",
    "            model.trf_blocks[l].att.out_proj.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.o_proj.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].norm1.weight = assign(\n",
    "            model.trf_blocks[l].norm1.weight,\n",
    "            params[f\"model.layers.{l}.input_layernorm.weight\"]\n",
    "        )\n",
    "\n",
    "        # Load FeedForward weights\n",
    "        model.trf_blocks[l].ff.fc1.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc1.weight,\n",
    "            params[f\"model.layers.{l}.mlp.gate_proj.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].ff.fc2.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc2.weight,\n",
    "            params[f\"model.layers.{l}.mlp.up_proj.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].ff.proj.weight = assign(\n",
    "            model.trf_blocks[l].ff.proj.weight,\n",
    "            params[f\"model.layers.{l}.mlp.down_proj.weight\"]\n",
    "        )\n",
    "        model.trf_blocks[l].norm2.weight = assign(\n",
    "            model.trf_blocks[l].norm2.weight,\n",
    "            params[f\"model.layers.{l}.post_attention_layernorm.weight\"]\n",
    "        )\n",
    "\n",
    "    # Load output layer weights\n",
    "    model.final_norm.weight = assign(model.final_norm.weight, params[\"model.norm.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b6256777-3d42-4f51-bbdf-c8d2c0980f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_88865/1629613674.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.nn.Parameter(torch.tensor(right))\n"
     ]
    }
   ],
   "source": [
    "load_weights_into_llama(model, LLAMA32_CONFIG_1B, current_weights)\n",
    "del current_weights  # free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "116b4032-2361-4cd0-addd-7e9a5f996773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature = 1, top_k = None, eos_id = None):\n",
    "    # idx has shape (batch, n_tokens) \n",
    "    # We will continually append to idx\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop current context to supported context size\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # (batch, n_tokens, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "        if top_k is not None: # relevant when temperature is high\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "        logits /= temperature\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples = 1)  # (batch, 1)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "bc2f106f-5d25-4e23-9c80-91618e2aca27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort is made to ensure that the information on this website is accurate. However, we cannot guarantee that the information is accurate, complete\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=LLAMA32_CONFIG_1B[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=1.\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
