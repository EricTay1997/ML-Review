{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b607b5ba-ed4f-46ba-b0cd-f571b1b947d5",
   "metadata": {},
   "source": [
    "# Threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c0ae781-4c36-46c0-8369-d780b20aa6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synchronous\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import concurrent.futures\n",
    "import threading\n",
    "\n",
    "def download_site(url):\n",
    "    requests.get(url)\n",
    "    indicator = \"J\" if \"jython\" in url else \"R\"\n",
    "    print(indicator, sep='', end='', flush=True)\n",
    "\n",
    "def download_site_verbose(url):\n",
    "    with requests.Session() as session:\n",
    "        response = session.get(url)\n",
    "        indicator = \"J\" if \"jython\" in url else \"R\"\n",
    "        print(indicator, sep='', end='', flush=True)\n",
    "\n",
    "sites = [\n",
    "        \"https://www.jython.org\",\n",
    "        \"http://olympus.realpython.org/dice\",\n",
    "    ] * 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03da6992-527f-41d1-baaa-23da96aab984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRJRDownloaded 160 sites in 8.650328458010335 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "for url in sites:\n",
    "    download_site(url)\n",
    "duration = time.perf_counter() - start\n",
    "print(f\"Downloaded {len(sites)} sites in {duration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37602932-3ad9-4ffe-933c-cd178b141314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRJJJRRRJRRJJJJRRRJRJRJJJRRJRJJRRJRJRJRJRJRJRJJJRJRRRJRJRJJRJRRJJRJRRRJJRJRJJJRRRJRJJRRJJJRRJRJRJRRJJRJRRJRJJRJRJRJRJRRJRJJRRJRJRJJRJRRJJJRRRJJRRJRJRJJRJJRRJRJRDownloaded 160 sites in 1.5454192079923814 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    executor.map(download_site, sites)\n",
    "duration = time.perf_counter() - start\n",
    "print(f\"Downloaded {len(sites)} sites in {duration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24912a2a-2bae-487e-9533-809a9c43355b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JRRJRRJJRJJRRRJJJRRJRJRJRJRJRRJRJJRRJJJRRRJJRRJJRJRJJRJRJRRRJRJJRJRRJRJRJRRJJJRJRJRJRJRJJRRJRJJRRJJRRJRJRJRJRRJJJRRRJRRJJJRRJRJRRJJJRJJRRRJJRJRJJRJRRRJJRJRJRJRJDownloaded 160 sites in 1.9429268749954645 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    executor.map(download_site_verbose, sites)\n",
    "duration = time.perf_counter() - start\n",
    "print(f\"Downloaded {len(sites)} sites in {duration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17686ad9-7cbf-469a-a0dc-c7279a181f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRJJJJRRJRJRJJRRJRRJRJJRJRRJRJJRJRJJRJJRRRJJRJRRJJRRJRRJJRRJJRRJJRRJJRRJJJRJRRRJJRJJRRJJRRJJRJRRRJJRJJJRRRJRRJJJRRRRJJJJRRJJJRRRRJJRJJRRJJRJRRRJJRJJRRRJJJRRJRRJDownloaded 160 sites in 0.8504934589873301 seconds\n"
     ]
    }
   ],
   "source": [
    "# Connection reusage leads to speed up\n",
    "\n",
    "from itertools import repeat\n",
    "def download_site_verbose(session, url):\n",
    "    response = session.get(url)\n",
    "    indicator = \"J\" if \"jython\" in url else \"R\"\n",
    "    print(indicator, sep='', end='', flush=True)\n",
    "\n",
    "start = time.perf_counter()\n",
    "with requests.Session() as session:\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        executor.map(download_site_verbose, repeat(session), sites)\n",
    "duration = time.perf_counter() - start\n",
    "print(f\"Downloaded {len(sites)} sites in {duration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "132a0b6f-d0ee-4884-bf74-aa66da625bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting with balance of 100\n",
      "deposit thread updating...\n",
      "withdrawal thread updating...\n",
      "deposit thread finishing...\n",
      "withdrawal thread finishing...\n",
      "ending balance of 0\n"
     ]
    }
   ],
   "source": [
    "# Locking, submit\n",
    "class Account:\n",
    "    def __init__(self):\n",
    "        self.balance = 100 # shared data\n",
    "        self.lock = threading.Lock()\n",
    "    def update(self, transaction, amount):\n",
    "        print(f'{transaction} thread updating...')\n",
    "        with self.lock:\n",
    "            local_copy = self.balance\n",
    "            local_copy += amount\n",
    "            time.sleep(1)\n",
    "            self.balance = local_copy\n",
    "        print(f'{transaction} thread finishing...')\n",
    "\n",
    "account = Account()\n",
    "print(f'starting with balance of {account.balance}')\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as ex:\n",
    "    for transaction, amount in [('deposit', 50), ('withdrawal', -150)]:\n",
    "        ex.submit(account.update, transaction, amount)\n",
    "print(f'ending balance of {account.balance}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e422d92e-e83d-4814-b412-ef8ef48d1683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Event\n",
    "event = threading.Event()\n",
    "print(event.is_set())\n",
    "event.set()\n",
    "print(event.is_set())\n",
    "event.clear()\n",
    "print(event.is_set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4d6225f7-62e7-47e5-a4ff-6f23b0222847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "s = threading.Semaphore(value=10)\n",
    "s.acquire()\n",
    "print(s._value)\n",
    "s.release()\n",
    "print(s._value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "16173f69-1621-4226-babc-f30194d07895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "welcome visitor #0\n",
      "[monitor] semaphore=9\n",
      "welcome visitor #1\n",
      "welcome visitor #2\n",
      "[monitor] semaphore=1\n",
      "welcome visitor #3\n",
      "welcome visitor #4\n",
      "welcome visitor #5\n",
      "[monitor] semaphore=4\n",
      "welcome visitor #6\n",
      "welcome visitor #7\n",
      "welcome visitor #8\n",
      "[monitor] semaphore=1\n",
      "welcome visitor #9\n",
      "welcome visitor #10\n",
      "[monitor] reached max users!\n",
      "[monitor] kicking a user out...\n",
      "[monitor] semaphore=0\n",
      "welcome visitor #11\n",
      "[monitor] semaphore=1\n",
      "[monitor] reached max users!\n",
      "[monitor] kicking a user out...\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import random\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def welcome(semaphore, stop):\n",
    "    visitor_number = 0\n",
    "    while True and not stop.is_set():\n",
    "        print(f'welcome visitor #{visitor_number}')\n",
    "        semaphore.acquire() # reduces value, is blocked when the counter is zero until release is called\n",
    "        visitor_number += 1\n",
    "        time.sleep(random.random())\n",
    "    \n",
    "def monitor(semaphore, stop):\n",
    "    while True and not stop.is_set():\n",
    "        print(f'[monitor] semaphore={semaphore._value}')\n",
    "        time.sleep(3)\n",
    "        if semaphore._value == 0:\n",
    "            print('[monitor] reached max users!')\n",
    "            print('[monitor] kicking a user out...')\n",
    "            semaphore.release() # increases value\n",
    "            time.sleep(0.05)\n",
    "\n",
    "stop = threading.Event()\n",
    "semaphore = threading.Semaphore(value=10)\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    executor.submit(welcome, semaphore, stop)\n",
    "    executor.submit(monitor, semaphore, stop)\n",
    "    time.sleep(7)\n",
    "    stop.set()\n",
    "\n",
    "# Counting is atomic. This means that there is a guarantee that the operating system will not swap out the thread in the middle of incrementing or decrementing the counter.\n",
    "# If a thread calls .acquire() when the counter is zero, that thread will block until a different thread calls .release() and increments the counter to one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d157b7-683c-40cd-bf1e-9fc2019769e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MultiProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "031c117c-82e3-4b2b-bb6f-a08d2b1b5c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "start = time.perf_counter()\n",
    "with multiprocessing.Pool() as pool:\n",
    "    pool.map(download_site, sites)\n",
    "duration = time.perf_counter() - start\n",
    "print(f\"Downloaded {len(sites)} sites in {duration} seconds\")\n",
    "\n",
    "# multiprocessing.Pool(initializer=set_global_session)\n",
    "# If initializer is not None then each worker process will call initializer(*initargs) when it starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04ef6b89-e03e-4e88-b95a-73e1167f37a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration 4.790480667004886 seconds\n"
     ]
    }
   ],
   "source": [
    "# Synchronous\n",
    "import time\n",
    "\n",
    "def calculate(limit):\n",
    "    return sum(i * i for i in range(limit))\n",
    "\n",
    "numbers = [5_000_000 + x for x in range(20)]\n",
    "start = time.perf_counter()\n",
    "for number in numbers:\n",
    "    calculate(number)\n",
    "duration = time.perf_counter() - start\n",
    "print(f\"Duration {duration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2948c67-295e-48fd-972d-9cd8b9c0fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concurrent\n",
    "print(multiprocessing.cpu_count())\n",
    "start = time.perf_counter()\n",
    "with multiprocessing.Pool() as pool:\n",
    "    pool.map(calculate, numbers)\n",
    "duration = time.perf_counter() - start\n",
    "print(f\"Duration {duration} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06154361-cc4f-46a5-8502-8878fa04df8f",
   "metadata": {},
   "source": [
    "# Asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af90823d-5b41-4748-9d04-b4452aa20a53",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10700aba-2190-46c8-851c-161646252706",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producer 0 sleeping for 4 seconds.\n",
      "Producer 2 sleeping for 7 seconds.\n",
      "Producer 3 sleeping for 4 seconds.\n",
      "Producer 4 sleeping for 10 seconds.\n",
      "Consumer 0 sleeping for 7 seconds.\n",
      "Consumer 1 sleeping for 8 seconds.\n",
      "Consumer 2 sleeping for 4 seconds.\n",
      "Consumer 3 sleeping for 7 seconds.\n",
      "Consumer 4 sleeping for 1 seconds.\n",
      "Consumer 5 sleeping for 6 seconds.\n",
      "Consumer 6 sleeping for 9 seconds.\n",
      "Consumer 7 sleeping for 3 seconds.\n",
      "Consumer 8 sleeping for 9 seconds.\n",
      "Consumer 9 sleeping for 7 seconds.\n",
      "Producer 0 added <2f1167ddfd> to queue.\n",
      "Producer 0 sleeping for 10 seconds.\n",
      "Producer 3 added <37bae73876> to queue.\n",
      "Producer 3 sleeping for 0 seconds.\n",
      "Consumer 2 got element <2f1167ddfd> in 0.00060 seconds.\n",
      "Consumer 2 sleeping for 1 seconds.\n",
      "Consumer 4 got element <37bae73876> in 0.00055 seconds.\n",
      "Consumer 4 sleeping for 0 seconds.\n",
      "Producer 3 added <2656374d36> to queue.\n",
      "Producer 3 sleeping for 1 seconds.\n",
      "Consumer 4 got element <2656374d36> in 0.00007 seconds.\n",
      "Consumer 4 sleeping for 9 seconds.\n",
      "Producer 3 added <693d412345> to queue.\n",
      "Producer 3 sleeping for 0 seconds.\n",
      "Consumer 7 got element <693d412345> in 0.00094 seconds.\n",
      "Consumer 7 sleeping for 5 seconds.\n",
      "Producer 3 added <cb53fdbb49> to queue.\n",
      "Consumer 2 got element <cb53fdbb49> in 0.00007 seconds.\n",
      "Consumer 2 sleeping for 10 seconds.\n",
      "Producer 2 added <732f6674d9> to queue.\n",
      "Producer 2 sleeping for 5 seconds.\n",
      "Consumer 0 got element <732f6674d9> in 0.00043 seconds.\n",
      "Consumer 0 sleeping for 8 seconds.\n",
      "Producer 4 added <2ca3fbbd6c> to queue.\n",
      "Producer 4 sleeping for 2 seconds.\n",
      "Consumer 3 got element <2ca3fbbd6c> in 0.00093 seconds.\n",
      "Consumer 3 sleeping for 5 seconds.\n",
      "Producer 4 added <2a33cbd6c5> to queue.\n",
      "Producer 4 sleeping for 5 seconds.\n",
      "Producer 2 added <e5212e1e60> to queue.\n",
      "Producer 2 sleeping for 0 seconds.\n",
      "Consumer 9 got element <2a33cbd6c5> in 0.00097 seconds.\n",
      "Consumer 9 sleeping for 3 seconds.\n",
      "Consumer 5 got element <e5212e1e60> in 0.00068 seconds.\n",
      "Consumer 5 sleeping for 1 seconds.\n",
      "Producer 2 added <4e7d76c730> to queue.\n",
      "Producer 2 sleeping for 5 seconds.\n",
      "Consumer 1 got element <4e7d76c730> in 0.00009 seconds.\n",
      "Consumer 1 sleeping for 6 seconds.\n",
      "Producer 0 added <20b88586a3> to queue.\n",
      "Producer 0 sleeping for 10 seconds.\n",
      "Consumer 6 got element <20b88586a3> in 0.00094 seconds.\n",
      "Consumer 6 sleeping for 5 seconds.\n",
      "Producer 4 added <ca8b9bc511> to queue.\n",
      "Producer 4 sleeping for 6 seconds.\n",
      "Producer 2 added <7522ce183d> to queue.\n",
      "Consumer 8 got element <ca8b9bc511> in 0.00095 seconds.\n",
      "Consumer 8 sleeping for 4 seconds.\n",
      "Consumer 7 got element <7522ce183d> in 0.00065 seconds.\n",
      "Consumer 7 sleeping for 10 seconds.\n",
      "Producer 4 added <b3c94672f6> to queue.\n",
      "Producer 4 sleeping for 8 seconds.\n",
      "Consumer 4 got element <b3c94672f6> in 0.00161 seconds.\n",
      "Consumer 4 sleeping for 10 seconds.\n",
      "Producer 0 added <ac047f168f> to queue.\n",
      "Producer 0 sleeping for 10 seconds.\n",
      "Consumer 5 got element <ac047f168f> in 0.00104 seconds.\n",
      "Consumer 5 sleeping for 7 seconds.\n",
      "Producer 4 added <041005b5f2> to queue.\n",
      "Producer 4 sleeping for 6 seconds.\n",
      "Consumer 0 got element <041005b5f2> in 0.00074 seconds.\n",
      "Consumer 0 sleeping for 8 seconds.\n",
      "Producer 0 added <31e8260ddb> to queue.\n",
      "Consumer 3 got element <31e8260ddb> in 0.00087 seconds.\n",
      "Consumer 3 sleeping for 3 seconds.\n",
      "Producer 4 added <92e8f7cc70> to queue.\n",
      "Producer 4 sleeping for 6 seconds.\n",
      "Consumer 3 got element <92e8f7cc70> in 0.00030 seconds.\n",
      "Consumer 3 sleeping for 9 seconds.\n",
      "Producer 4 added <3ee9e3f9d6> to queue.\n",
      "Producer 4 sleeping for 3 seconds.\n",
      "Consumer 2 got element <3ee9e3f9d6> in 0.00461 seconds.\n",
      "Consumer 2 sleeping for 2 seconds.\n",
      "Producer 4 added <aa3152cffb> to queue.\n",
      "Consumer 1 got element <aa3152cffb> in 0.00079 seconds.\n",
      "Consumer 1 sleeping for 10 seconds.\n",
      "Program completed in 46.01886 seconds.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import itertools as it\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "async def makeitem(size: int = 5) -> str:\n",
    "    return os.urandom(size).hex()\n",
    "\n",
    "async def randsleep(caller=None) -> None:\n",
    "    i = random.randint(0, 10)\n",
    "    if caller:\n",
    "        print(f\"{caller} sleeping for {i} seconds.\")\n",
    "    await asyncio.sleep(i)\n",
    "\n",
    "async def produce(name: int, q: asyncio.Queue) -> None:\n",
    "    n = random.randint(0, 10)\n",
    "    for _ in it.repeat(None, n):  # Synchronous loop for each single producer\n",
    "        await randsleep(caller=f\"Producer {name}\")\n",
    "        i = await makeitem()\n",
    "        t = time.perf_counter()\n",
    "        await q.put((i, t))\n",
    "        print(f\"Producer {name} added <{i}> to queue.\")\n",
    "\n",
    "async def consume(name: int, q: asyncio.Queue) -> None:\n",
    "    while True:\n",
    "        await randsleep(caller=f\"Consumer {name}\")\n",
    "        i, t = await q.get()\n",
    "        now = time.perf_counter()\n",
    "        print(f\"Consumer {name} got element <{i}>\"\n",
    "              f\" in {now-t:0.5f} seconds.\")\n",
    "        q.task_done()\n",
    "\n",
    "async def main(nprod: int, ncon: int):\n",
    "    q = asyncio.Queue()\n",
    "    producers = [asyncio.create_task(produce(n, q)) for n in range(nprod)]\n",
    "    consumers = [asyncio.create_task(consume(n, q)) for n in range(ncon)]\n",
    "    await asyncio.gather(*producers)\n",
    "    await q.join()  # Implicitly awaits consumers, too\n",
    "    for c in consumers:\n",
    "        c.cancel()\n",
    "\n",
    "import argparse\n",
    "random.seed(444)\n",
    "start = time.perf_counter()\n",
    "# asyncio.run(main(5,10))\n",
    "await main(5,10)\n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"Program completed in {elapsed:0.5f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f14ab6-5b85-4e1e-9aa0-752bd345a946",
   "metadata": {},
   "source": [
    "## aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c803d4f-4427-489f-a509-61d0d838db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [\n",
    "        \"https://www.jython.org\",\n",
    "        \"http://olympus.realpython.org/dice\",\n",
    "    ] * 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd67ff6c-6eca-4f39-9fd6-6bf37aad07ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting downloads\n",
      "JJJJJJJRJJRJRRJRRRRJRRJJRJRJJJRRRJJJJJJJJJJJRJJJRJRJRRJRRRJRJRRJRRJJJJRRRRRJRRRRRRRJRRRRRJJRRJRRJRJJJJJJRJJJJJJJJJJJRRRJRRRRRRJRJRRRRJJJJJJJJRRRJRRRRRJRRJJRRRRR\n",
      "Downloaded 160 sites in 0.1582791249966249 seconds\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import aiohttp\n",
    "\n",
    "async def download_site(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        indicator = \"J\" if \"jython\" in url else \"R\"\n",
    "        print(indicator, sep='', end='', flush=True)\n",
    "\n",
    "async def download_all_sites(sites):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [download_site(session, url) for url in sites]\n",
    "        await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "print(\"Starting downloads\")\n",
    "start = time.perf_counter()\n",
    "# asyncio.run(download_all_sites(sites))\n",
    "await download_all_sites(sites)\n",
    "duration = time.perf_counter() - start\n",
    "print(f\"\\nDownloaded {len(sites)} sites in {duration} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a09179-f6d0-434a-849c-3d5eeefc2097",
   "metadata": {},
   "source": [
    "## aiofiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcf7e34-3cf9-4d4a-9fca-b13bf0f4b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import re\n",
    "import sys\n",
    "from typing import IO\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "\n",
    "import aiofiles\n",
    "import aiohttp\n",
    "from aiohttp import ClientSession\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s:%(name)s: %(message)s\",\n",
    "    level=logging.DEBUG,\n",
    "    datefmt=\"%H:%M:%S\",\n",
    "    stream=sys.stderr,\n",
    ")\n",
    "logger = logging.getLogger(\"areq\")\n",
    "logging.getLogger(\"chardet.charsetprober\").disabled = True\n",
    "\n",
    "HREF_RE = re.compile(r'href=\"(.*?)\"')\n",
    "\n",
    "async def fetch_html(url: str, session: ClientSession, **kwargs) -> str:\n",
    "    \"\"\"GET request wrapper to fetch page HTML.\n",
    "\n",
    "    kwargs are passed to `session.request()`.\n",
    "    \"\"\"\n",
    "\n",
    "    resp = await session.request(method=\"GET\", url=url, **kwargs)\n",
    "    resp.raise_for_status()\n",
    "    logger.info(\"Got response [%s] for URL: %s\", resp.status, url)\n",
    "    html = await resp.text()\n",
    "    return html\n",
    "\n",
    "async def parse(url: str, session: ClientSession, **kwargs) -> set:\n",
    "    \"\"\"Find HREFs in the HTML of `url`.\"\"\"\n",
    "    found = set()\n",
    "    try:\n",
    "        html = await fetch_html(url=url, session=session, **kwargs)\n",
    "    except (\n",
    "        aiohttp.ClientError,\n",
    "        aiohttp.http_exceptions.HttpProcessingError,\n",
    "    ) as e:\n",
    "        logger.error(\n",
    "            \"aiohttp exception for %s [%s]: %s\",\n",
    "            url,\n",
    "            getattr(e, \"status\", None),\n",
    "            getattr(e, \"message\", None),\n",
    "        )\n",
    "        return found\n",
    "    except Exception as e:\n",
    "        logger.exception(\n",
    "            \"Non-aiohttp exception occured:  %s\", getattr(e, \"__dict__\", {})\n",
    "        )\n",
    "        return found\n",
    "    else:\n",
    "        for link in HREF_RE.findall(html):\n",
    "            try:\n",
    "                abslink = urllib.parse.urljoin(url, link)\n",
    "            except (urllib.error.URLError, ValueError):\n",
    "                logger.exception(\"Error parsing URL: %s\", link)\n",
    "                pass\n",
    "            else:\n",
    "                found.add(abslink)\n",
    "        logger.info(\"Found %d links for %s\", len(found), url)\n",
    "        return found\n",
    "\n",
    "async def write_one(file: IO, url: str, **kwargs) -> None:\n",
    "    \"\"\"Write the found HREFs from `url` to `file`.\"\"\"\n",
    "    res = await parse(url=url, **kwargs)\n",
    "    if not res:\n",
    "        return None\n",
    "    async with aiofiles.open(file, \"a\") as f:\n",
    "        for p in res:\n",
    "            await f.write(f\"{url}\\t{p}\\n\")\n",
    "        logger.info(\"Wrote results for source URL: %s\", url)\n",
    "\n",
    "async def bulk_crawl_and_write(file: IO, urls: set, **kwargs) -> None:\n",
    "    \"\"\"Crawl & write concurrently to `file` for multiple `urls`.\"\"\"\n",
    "    async with ClientSession() as session:\n",
    "        tasks = []\n",
    "        for url in urls:\n",
    "            tasks.append(\n",
    "                write_one(file=file, url=url, session=session, **kwargs)\n",
    "            )\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pathlib\n",
    "    import sys\n",
    "\n",
    "    assert sys.version_info >= (3, 7), \"Script requires Python 3.7+.\"\n",
    "    here = pathlib.Path(__file__).parent\n",
    "\n",
    "    with open(here.joinpath(\"urls.txt\")) as infile:\n",
    "        urls = set(map(str.strip, infile))\n",
    "\n",
    "    outpath = here.joinpath(\"foundurls.txt\")\n",
    "    with open(outpath, \"w\") as outfile:\n",
    "        outfile.write(\"source_url\\tparsed_url\\n\")\n",
    "\n",
    "    asyncio.run(bulk_crawl_and_write(file=outpath, urls=urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2896a07-1c23-4e56-a7f5-03b3f862ea8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
