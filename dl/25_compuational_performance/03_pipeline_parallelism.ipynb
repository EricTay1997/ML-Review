{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfd658be-1fd2-40a3-a8c0-934560d1e121",
   "metadata": {},
   "source": [
    "# Code adapted from https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/scaling/JAX/pipeline_parallel_simple.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ada676-04bb-4659-a5bf-6cd793620b8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a53fc92-dc9c-43e1-b909-a7c3ed59accf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/phlippe/uvadlc_notebooks/master/docs/tutorial_notebooks/scaling/JAX/data_parallel.py...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "# Github URL where python scripts are stored.\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/uvadlc_notebooks/master/docs/tutorial_notebooks/scaling/JAX/\"\n",
    "# Files to download.\n",
    "python_files = [\"single_gpu.py\", \"data_parallel.py\", \"utils.py\"]\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in python_files:\n",
    "    if not os.path.isfile(file_name):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_name)\n",
    "        except HTTPError as e:\n",
    "            print(\n",
    "                \"Something went wrong. Please try to download the file directly from the GitHub repository, or contact the author with the full output including the following error:\\n\",\n",
    "                e,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e35819c-9115-47c1-9006-e6c2abf7dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import simulate_CPU_devices\n",
    "\n",
    "simulate_CPU_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "828a2c00-b679-48e6-b4b6-96d36b32f7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from pprint import pprint\n",
    "from typing import Any, Callable, Dict, Tuple\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.core.frozen_dict import FrozenDict\n",
    "from jax.experimental.shard_map import shard_map\n",
    "from jax.sharding import Mesh\n",
    "from jax.sharding import PartitionSpec as P\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "# Helper types\n",
    "PyTree = Any\n",
    "Parameter = jax.Array | nn.Partitioned\n",
    "Metrics = Dict[str, Tuple[jax.Array, ...]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "938d5ddb-0aaf-4b1b-8a25-e8bee41df708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_parallel import fold_rng_over_axis, sync_gradients\n",
    "from single_gpu import (\n",
    "    Batch,\n",
    "    TrainState,\n",
    "    accumulate_gradients,\n",
    "    get_num_params,\n",
    "    print_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c53f0e5-6d7c-4785-a964-fd1e4ad7bac6",
   "metadata": {},
   "source": [
    "## Pipeline Parallelism with Micro-Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e190212-9138-4b82-be3a-e208f62f8496",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    config: ConfigDict\n",
    "    train: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        input_features = x.shape[-1]\n",
    "        residual = x\n",
    "        x = nn.LayerNorm(dtype=self.config.dtype, name=\"pre_norm\")(x)\n",
    "        x = nn.Dense(\n",
    "            features=self.config.hidden_size * self.config.mlp_expansion,\n",
    "            dtype=self.config.dtype,\n",
    "            name=\"input_dense\",\n",
    "        )(x)\n",
    "        x = nn.silu(x)\n",
    "        x = nn.Dropout(rate=self.config.dropout_rate, deterministic=not self.train)(x)\n",
    "        x = nn.Dense(features=input_features, dtype=self.config.dtype, name=\"output_dense\")(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa079050-c8bc-4909-8cf1-250c0c9128cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPLayers(nn.Module):\n",
    "    config: ConfigDict\n",
    "    train: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        # Scan version\n",
    "        block_class = MLPBlock\n",
    "        if \"MLP\" in self.config.remat:\n",
    "            block_class = nn.remat(block_class, prevent_cse=False)\n",
    "        block = block_class(config=self.config, train=self.train, name=\"block\")\n",
    "        x, _ = nn.scan(\n",
    "            lambda module, carry, _: (module(carry), ()),\n",
    "            variable_axes={\"params\": 0},\n",
    "            split_rngs={\"params\": True, \"dropout\": True},\n",
    "            length=self.config.num_layers,\n",
    "        )(block, x, ())\n",
    "        # Non-scanned version\n",
    "        # for i in range(self.config.num_layers):\n",
    "        #     x = block_class(self.config, train=train, name=f\"block_{i}\")(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99575074-2696-4844-9820-e8ff3e68e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_params(\n",
    "    params: PyTree, axis_name: str, axis: int = 0, mask_except: jax.Array | int | None = None\n",
    ") -> PyTree:\n",
    "    \"\"\"Stacks sharded parameters along a given axis name.\n",
    "\n",
    "    Args:\n",
    "        params: PyTree of parameters.\n",
    "        axis_name: Name of the axis to stack along.\n",
    "        axis: Index of the axis to stack along.\n",
    "        mask_except: If not None, only the `mask_except`-th shard will be non-zero.\n",
    "\n",
    "    Returns:\n",
    "        PyTree of parameters with the same structure as `params`, but with the leaf\n",
    "        nodes replaced by `nn.Partitioned` objects with sharding over axis name added\n",
    "        to `axis`-th axis of parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def _stack(x: Parameter) -> Parameter:\n",
    "        if isinstance(x, nn.Partitioned):\n",
    "            value, names = x.value, x.names\n",
    "        else:\n",
    "            value, names = x, (None,) * x.ndim\n",
    "        if mask_except is not None:\n",
    "            axis_index = jax.lax.axis_index(axis_name)\n",
    "            value = jnp.where(axis_index == mask_except, value, 0.0)\n",
    "        value = jnp.expand_dims(value, axis)\n",
    "        names = names[:axis] + (axis_name,) + names[axis:]\n",
    "        return nn.Partitioned(value, names=names)\n",
    "\n",
    "    return jax.tree_map(_stack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
    "\n",
    "\n",
    "def unstack_params(params: PyTree, axis_name: str) -> PyTree:\n",
    "    \"\"\"Unstacks parameters along a given axis name.\n",
    "\n",
    "    Inverse operation to `stack_params`.\n",
    "\n",
    "    Args:\n",
    "        params: PyTree of parameters.\n",
    "        axis_name: Name of the axis to unstack along.\n",
    "\n",
    "    Returns:\n",
    "        PyTree of parameters with the same structure as `params`, but\n",
    "        with the leaf nodes having the sharding over the axis name removed.\n",
    "    \"\"\"\n",
    "\n",
    "    def _unstack(x: Parameter) -> Parameter:\n",
    "        if isinstance(x, nn.Partitioned) and axis_name in x.names:\n",
    "            value = x.value\n",
    "            names = x.names\n",
    "            axis_idx = names.index(axis_name)\n",
    "            value = value.squeeze(axis_idx)\n",
    "            names = names[:axis_idx] + names[axis_idx + 1 :]\n",
    "            if all([n is None for n in names]):\n",
    "                return value\n",
    "            else:\n",
    "                return nn.Partitioned(value, names=names)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    return jax.tree_map(_unstack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce81578d-7214-4825-a0bc-abcd791e2bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_pipeline_step(\n",
    "    module: nn.Module,\n",
    "    state: jax.Array,\n",
    "    input: jax.Array,\n",
    "    *args,\n",
    "    model_axis_name: str,\n",
    "    **kwargs,\n",
    ") -> Tuple[jax.Array, jax.Array]:\n",
    "    \"\"\"Single micro-batch pipeline step.\n",
    "\n",
    "    Args:\n",
    "        module: Flax module representing the stage to execute.\n",
    "        state: Last communicated features between stages. Used as input to the module for all stages except the first.\n",
    "        input: Original micro-batch input to the pipeline stage. Used as input to the module for the first stage.\n",
    "        *args: Additional arguments to the module.\n",
    "        model_axis_name: Name of the model axis in the mesh/shard_map.\n",
    "        **kwargs: Additional keyword arguments to the module.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of the new state (after communication) and the output of the module.\n",
    "    \"\"\"\n",
    "    num_stages = jax.lax.psum(1, model_axis_name)\n",
    "    stage_index = jax.lax.axis_index(model_axis_name)\n",
    "    # For the first stage, we use the microbatches as input.\n",
    "    # For all other stages, we use the last state from the\n",
    "    # previous stage as input.\n",
    "    state = jnp.where(stage_index == 0, input, state)\n",
    "    state = module(state, *args, **kwargs)\n",
    "    # For the last stage, we return the state as output.\n",
    "    # For all other stages, we return zeros.\n",
    "    output = jnp.where(\n",
    "        stage_index == num_stages - 1,\n",
    "        state,\n",
    "        jnp.zeros_like(state),\n",
    "    )\n",
    "    # Communicate the last state to the next stage.\n",
    "    state = jax.lax.ppermute(\n",
    "        state,\n",
    "        model_axis_name,\n",
    "        perm=[(i, (i + 1) % num_stages) for i in range(num_stages)],\n",
    "    )\n",
    "    return (state, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0deea7f5-9e50-41b7-9954-e13da185e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.named_scope(\"pipeline\")  # Naming scope for profiling.\n",
    "def execute_pipeline(\n",
    "    module: nn.Module, x: jax.Array, *args, num_microbatches: int, model_axis_name: str, **kwargs\n",
    ") -> jax.Array:\n",
    "    \"\"\"Execute a pipeline of stages on a batch of data.\n",
    "\n",
    "    Uses the principle of GPipe in splitting the batch into micro-batches\n",
    "    and running the pipeline stages in parallel.\n",
    "\n",
    "    Args:\n",
    "        module: Flax module representing the pipeline stage to execute.\n",
    "        x: Batch of input data, only needed on device of the first stage. Data will be split into micro-batches.\n",
    "        *args: Additional arguments to the module.\n",
    "        num_microbatches: Number of micro-batches to split the batch into.\n",
    "        model_axis_name: Name of the model axis in the mesh/shard_map.\n",
    "        **kwargs: Additional keyword arguments to the module.\n",
    "\n",
    "    Returns:\n",
    "        Output of the last stage of the pipeline. For devices that are not\n",
    "        the last stage, the output is zeros.\n",
    "    \"\"\"\n",
    "    num_stages = jax.lax.psum(1, model_axis_name)\n",
    "    # Structure the input data into micro-batches.\n",
    "    batch_size = x.shape[0]\n",
    "    assert (\n",
    "        batch_size % num_microbatches == 0\n",
    "    ), f\"Batch size {batch_size} must be divisible by number of microbatches {num_microbatches}\"\n",
    "    microbatch_size = batch_size // num_microbatches\n",
    "    microbatches = jnp.reshape(x, (num_microbatches, microbatch_size, *x.shape[1:]))\n",
    "    inputs = jnp.concatenate(  # Add zeros for unused computation blocks in first stage.\n",
    "        [\n",
    "            microbatches,\n",
    "            jnp.zeros((num_stages - 1, *microbatches.shape[1:]), dtype=microbatches.dtype),\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "    state = jnp.zeros_like(microbatches[0])\n",
    "    num_iterations = inputs.shape[0]\n",
    "    # Run loop over pipeline steps.\n",
    "    _, outputs = nn.scan(\n",
    "        functools.partial(\n",
    "            execute_pipeline_step,\n",
    "            *args,\n",
    "            model_axis_name=model_axis_name,\n",
    "            **kwargs,\n",
    "        ),\n",
    "        variable_broadcast={\"params\": True},\n",
    "        split_rngs={\"params\": False, \"dropout\": True},\n",
    "        length=num_iterations,\n",
    "        in_axes=0,\n",
    "        out_axes=0,\n",
    "    )(module, state, inputs)\n",
    "    # Take last N outputs (first ones are zeros from unused computation blocks in last stage).\n",
    "    outputs = jnp.concatenate(outputs[-num_microbatches:], axis=0)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88aa0b8a-7fe4-4f12-a657-3e4b68c111a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineModule(nn.Module):\n",
    "    model_axis_name: str\n",
    "    num_microbatches: int\n",
    "    module_fn: Callable[..., nn.Module]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        module = self.module_fn()\n",
    "        return execute_pipeline(\n",
    "            module,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "            num_microbatches=self.num_microbatches,\n",
    "            model_axis_name=self.model_axis_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "565db811-e8d1-4a14-b9e9-0894ce997a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParallelismWrapper(nn.Module):\n",
    "    \"\"\"Wrapper for adding model parallelism to a module.\n",
    "\n",
    "    This wrapper adds sharding over the model axis to the parameters of the module\n",
    "    and initializes the module with different parameters across the model axis.\n",
    "\n",
    "    Args:\n",
    "        model_axis_name: Name of the model axis to shard over.\n",
    "        module_fn: Function that returns the Flax module to wrap.\n",
    "        mask_except_model_idx: If not None, only the `mask_except_model_idx`-th shard will be non-zero.\n",
    "        split_rngs: If True, split the random number generators across the model axis.\n",
    "        module_kwargs: Additional keyword arguments to pass to the module function.\n",
    "    \"\"\"\n",
    "\n",
    "    model_axis_name: str\n",
    "    module_fn: Callable[..., nn.Module]\n",
    "    mask_except_model_idx: int | None = None\n",
    "    split_rngs: bool = True\n",
    "    module_kwargs: FrozenDict[str, Any] = FrozenDict({})\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if self.is_initializing() and self.split_rngs:\n",
    "            # Initialize each module across the model axis with different parameters.\n",
    "            self.scope.rngs[\"params\"] = self.scope.rngs[\"params\"].replace(\n",
    "                rng=fold_rng_over_axis(self.scope.rngs[\"params\"].rng, self.model_axis_name)\n",
    "            )\n",
    "        # Wrap variables in nn.Partitioned objects to add sharding over the model axis.\n",
    "        module = nn.map_variables(\n",
    "            target=functools.partial(\n",
    "                self.module_fn,\n",
    "                name=\"sharded\",\n",
    "                **self.module_kwargs,\n",
    "            ),\n",
    "            trans_in_fn=functools.partial(unstack_params, axis_name=self.model_axis_name),\n",
    "            trans_out_fn=functools.partial(\n",
    "                stack_params,\n",
    "                axis_name=self.model_axis_name,\n",
    "                mask_except=self.mask_except_model_idx,\n",
    "            ),\n",
    "            mapped_collections=\"params\",\n",
    "            mutable=True,\n",
    "        )()\n",
    "        return module(\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f2de49b-4475-4be1-82ab-d6bcd8d2b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPClassifier(nn.Module):\n",
    "    config: ConfigDict\n",
    "    pipeline_module_class: Callable[..., nn.Module] = PipelineModule\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jax.Array, train: bool) -> jax.Array:\n",
    "        # Input layer. Only needed in the first stage.\n",
    "        x = ModelParallelismWrapper(\n",
    "            module_fn=functools.partial(\n",
    "                nn.Dense,\n",
    "                features=self.config.hidden_size,\n",
    "                dtype=self.config.dtype,\n",
    "            ),\n",
    "            model_axis_name=self.config.model_axis_name,\n",
    "            mask_except_model_idx=0,\n",
    "            name=\"input_dense\",\n",
    "        )(x)\n",
    "        # Pipeline\n",
    "        stage_module_fn = functools.partial(\n",
    "            MLPLayers, config=self.config, train=train, name=\"mlp_layers\"\n",
    "        )\n",
    "        pipeline_module_fn = functools.partial(\n",
    "            self.pipeline_module_class,\n",
    "            model_axis_name=self.config.model_axis_name,\n",
    "            num_microbatches=self.config.num_microbatches,\n",
    "            module_fn=stage_module_fn,\n",
    "        )\n",
    "        module = ModelParallelismWrapper(\n",
    "            module_fn=pipeline_module_fn,\n",
    "            model_axis_name=self.config.model_axis_name,\n",
    "            name=\"pipeline\",\n",
    "        )\n",
    "        x = module(x)\n",
    "        # Output layer. Only needed in the last stage.\n",
    "        output_wrapper = functools.partial(\n",
    "            ModelParallelismWrapper,\n",
    "            model_axis_name=self.config.model_axis_name,\n",
    "            mask_except_model_idx=self.config.model_axis_size - 1,\n",
    "        )\n",
    "        x = output_wrapper(\n",
    "            module_fn=functools.partial(nn.LayerNorm, dtype=self.config.dtype), name=\"output_norm\"\n",
    "        )(x)\n",
    "        x = output_wrapper(\n",
    "            module_fn=functools.partial(\n",
    "                nn.Dense, features=self.config.num_classes, dtype=self.config.dtype\n",
    "            ),\n",
    "            name=\"output_dense\",\n",
    "        )(x)\n",
    "        x = x.astype(jnp.float32)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0fae67b-fe79-42ff-8276-b44c70fd7166",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = ConfigDict(\n",
    "    dict(\n",
    "        batch_size=128,\n",
    "        num_classes=10,\n",
    "        input_size=784,\n",
    "    )\n",
    ")\n",
    "model_config = ConfigDict(\n",
    "    dict(\n",
    "        hidden_size=512,\n",
    "        mlp_expansion=1,\n",
    "        dropout_rate=0.1,\n",
    "        num_layers=8,\n",
    "        dtype=jnp.float32,\n",
    "        num_classes=data_config.num_classes,\n",
    "        remat=(),\n",
    "        data_axis_name=\"data\",\n",
    "        model_axis_name=\"model\",\n",
    "        model_axis_size=4,\n",
    "        num_microbatches=8,\n",
    "    )\n",
    ")\n",
    "model_config.num_layers //= model_config.model_axis_size  # Layers distributed over model axis.\n",
    "optimizer_config = ConfigDict(\n",
    "    dict(\n",
    "        learning_rate=1e-3,\n",
    "        num_minibatches=1,\n",
    "    )\n",
    ")\n",
    "config = ConfigDict(\n",
    "    dict(\n",
    "        model=model_config,\n",
    "        optimizer=optimizer_config,\n",
    "        data=data_config,\n",
    "        data_axis_name=model_config.data_axis_name,\n",
    "        model_axis_name=model_config.model_axis_name,\n",
    "        model_axis_size=model_config.model_axis_size,\n",
    "        seed=42,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83afa824-438e-447d-af4f-3c904529c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_array = np.array(jax.devices()).reshape(-1, config.model_axis_size)\n",
    "mesh = Mesh(device_array, (config.data_axis_name, config.model_axis_name))\n",
    "model_pp = PPClassifier(config=model_config)\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=config.optimizer.learning_rate,\n",
    ")\n",
    "rng = jax.random.PRNGKey(config.seed)\n",
    "model_init_rng, data_inputs_rng, data_labels_rng = jax.random.split(rng, 3)\n",
    "batch = Batch(\n",
    "    inputs=jax.random.normal(data_inputs_rng, (config.data.batch_size, config.data.input_size)),\n",
    "    labels=jax.random.randint(\n",
    "        data_labels_rng, (config.data.batch_size,), 0, config.data.num_classes\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e10ae45-a85c-4902-9270-e94adba726f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_fn(rng: jax.random.PRNGKey, x: jax.Array, model: nn.Module) -> TrainState:\n",
    "    init_rng, rng = jax.random.split(rng)\n",
    "    variables = model.init({\"params\": init_rng}, x, train=False)\n",
    "    params = variables.pop(\"params\")\n",
    "    state = TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        tx=optimizer,\n",
    "        rng=rng,\n",
    "    )\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38bac9ed-4f3f-40a8-b1a0-822e47fbfb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:61: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_unstack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:30: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_stack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:30: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_stack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:61: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_unstack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:30: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_stack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:61: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_unstack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n"
     ]
    }
   ],
   "source": [
    "init_pp_fn = shard_map(\n",
    "    functools.partial(init_fn, model=model_pp),\n",
    "    mesh,\n",
    "    in_specs=(P(), P(config.data_axis_name)),\n",
    "    out_specs=P(),\n",
    "    check_rep=False,\n",
    ")\n",
    "state_pp_shapes = jax.eval_shape(init_pp_fn, model_init_rng, batch.inputs)\n",
    "state_pp_specs = nn.get_partition_spec(state_pp_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6daaab53-993e-487a-9f7c-ca1bde053623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_dense': {'sharded': {'bias': PartitionSpec('model', None),\n",
      "                             'kernel': PartitionSpec('model', None, None)}},\n",
      " 'output_dense': {'sharded': {'bias': PartitionSpec('model', None),\n",
      "                              'kernel': PartitionSpec('model', None, None)}},\n",
      " 'output_norm': {'sharded': {'bias': PartitionSpec('model', None),\n",
      "                             'scale': PartitionSpec('model', None)}},\n",
      " 'pipeline': {'sharded': {'mlp_layers': {'block': {'input_dense': {'bias': PartitionSpec('model', None, None),\n",
      "                                                                   'kernel': PartitionSpec('model', None, None, None)},\n",
      "                                                   'output_dense': {'bias': PartitionSpec('model', None, None),\n",
      "                                                                    'kernel': PartitionSpec('model', None, None, None)},\n",
      "                                                   'pre_norm': {'bias': PartitionSpec('model', None, None),\n",
      "                                                                'scale': PartitionSpec('model', None, None)}}}}}}\n"
     ]
    }
   ],
   "source": [
    "pprint(state_pp_specs.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9c25cfef-df04-450d-90fa-5ca4e881b554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:61: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_unstack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:30: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_stack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:30: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_stack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:61: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_unstack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n"
     ]
    }
   ],
   "source": [
    "init_pp_fn = jax.jit(\n",
    "    shard_map(\n",
    "        functools.partial(init_fn, model=model_pp),\n",
    "        mesh,\n",
    "        in_specs=(P(), P(config.data_axis_name)),\n",
    "        out_specs=state_pp_specs,\n",
    "        check_rep=False,\n",
    "    ),\n",
    ")\n",
    "state_pp = init_pp_fn(model_init_rng, batch.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "872151a7-44aa-48c8-8ba2-ede71ce725ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_dense': {'bias': Partitioned(value=(4, 2, 512),\n",
      "                                     names=('model', None, None),\n",
      "                                     mesh=None),\n",
      "                 'kernel': Partitioned(value=(4, 2, 512, 512),\n",
      "                                       names=('model', None, None, None),\n",
      "                                       mesh=None)},\n",
      " 'output_dense': {'bias': Partitioned(value=(4, 2, 512),\n",
      "                                      names=('model', None, None),\n",
      "                                      mesh=None),\n",
      "                  'kernel': Partitioned(value=(4, 2, 512, 512),\n",
      "                                        names=('model', None, None, None),\n",
      "                                        mesh=None)},\n",
      " 'pre_norm': {'bias': Partitioned(value=(4, 2, 512),\n",
      "                                  names=('model', None, None),\n",
      "                                  mesh=None),\n",
      "              'scale': Partitioned(value=(4, 2, 512),\n",
      "                                   names=('model', None, None),\n",
      "                                   mesh=None)}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/676993966.py:2: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  jax.tree_map(lambda x: x.shape, state_pp.params[\"pipeline\"][\"sharded\"][\"mlp_layers\"][\"block\"])\n"
     ]
    }
   ],
   "source": [
    "pprint(\n",
    "    jax.tree_map(lambda x: x.shape, state_pp.params[\"pipeline\"][\"sharded\"][\"mlp_layers\"][\"block\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38282fe1-9d1a-41b1-a312-8c4c5654066b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array([[ 0.01044598, -0.07416785],\n",
      "       [-0.04605146,  0.0008348 ],\n",
      "       [-0.00904123, -0.00018691],\n",
      "       [ 0.00661926, -0.06117292]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "pprint(\n",
    "    state_pp.params[\"pipeline\"][\"sharded\"][\"mlp_layers\"][\"block\"][\"input_dense\"][\"kernel\"].value[\n",
    "        :, :, 0, 0\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "325a7874-5a61-45b8-a04a-f545f1b69195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Layer\n",
      "Array([-0.0754908,  0.       ,  0.       ,  0.       ], dtype=float32)\n",
      "\n",
      "Output layer\n",
      "Array([ 0.        ,  0.        ,  0.        , -0.07138917], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Layer\")\n",
    "pprint(state_pp.params[\"input_dense\"][\"sharded\"][\"kernel\"].value[:, 0, 0])\n",
    "print(\"\\nOutput layer\")\n",
    "pprint(state_pp.params[\"output_dense\"][\"sharded\"][\"kernel\"].value[:, 0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a9c0afef-c27a-4912-a9c8-7ce2903437f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(\n",
    "    params: PyTree, apply_fn: Any, batch: Batch, rng: jax.Array\n",
    ") -> Tuple[jax.Array, Dict[str, Any]]:\n",
    "    # Since dropout masks vary across the batch dimension, we want each device to generate a\n",
    "    # different mask. We can achieve this by folding the rng over the data axis, so that each\n",
    "    # device gets a different rng and thus mask.\n",
    "    dropout_rng = fold_rng_over_axis(rng, (config.data_axis_name, config.model_axis_name))\n",
    "    # Remaining computation is the same as before for single device.\n",
    "    logits = apply_fn(\n",
    "        {\"params\": params},\n",
    "        batch.inputs,\n",
    "        train=True,\n",
    "        rngs={\"dropout\": dropout_rng},\n",
    "    )\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch.labels)\n",
    "    correct_pred = jnp.equal(jnp.argmax(logits, axis=-1), batch.labels)\n",
    "    batch_size = batch.inputs.shape[0]\n",
    "    # Mask out loss and accuracy for pipeline stages except last one.\n",
    "    model_idx = jax.lax.axis_index(config.model_axis_name)\n",
    "    model_size = jax.lax.psum(1, config.model_axis_name)\n",
    "    loss = jnp.where(model_idx != model_size - 1, 0.0, loss)\n",
    "    correct_pred = jnp.where(model_idx != model_size - 1, False, correct_pred)\n",
    "    batch_size = jnp.where(model_idx != model_size - 1, 0, batch_size)\n",
    "    # Collect metrics and return loss.\n",
    "    step_metrics = {\n",
    "        \"loss\": (loss.sum(), batch_size),\n",
    "        \"accuracy\": (correct_pred.sum(), batch_size),\n",
    "    }\n",
    "    loss = loss.mean()\n",
    "    return loss, step_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "57eb7b89-fb22-4c27-975d-6bacba32017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_pp(\n",
    "    state: TrainState,\n",
    "    metrics: Metrics | None,\n",
    "    batch: Batch,\n",
    ") -> Tuple[TrainState, Metrics]:\n",
    "    rng, step_rng = jax.random.split(state.rng)\n",
    "    grads, step_metrics = accumulate_gradients(\n",
    "        state,\n",
    "        batch,\n",
    "        step_rng,\n",
    "        config.optimizer.num_minibatches,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    # Update parameters. We need to sync the gradients across data devices before updating.\n",
    "    with jax.named_scope(\"sync_gradients\"):\n",
    "        grads = sync_gradients(grads, (config.data_axis_name, config.model_axis_name))\n",
    "    new_state = state.apply_gradients(grads=grads, rng=rng)\n",
    "    # Sum metrics across replicas (both model and data axes).\n",
    "    with jax.named_scope(\"sync_metrics\"):\n",
    "        step_metrics = jax.tree_map(\n",
    "            lambda x: jax.lax.psum(x, axis_name=(config.data_axis_name, config.model_axis_name)),\n",
    "            step_metrics,\n",
    "        )\n",
    "    if metrics is None:\n",
    "        metrics = step_metrics\n",
    "    else:\n",
    "        metrics = jax.tree_map(jnp.add, metrics, step_metrics)\n",
    "    return new_state, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7f2fc534-5bf5-4974-a7e5-5494a6f2e396",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:61: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_unstack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:30: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_stack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:61: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_unstack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:30: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_stack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:61: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_unstack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:30: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_stack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:61: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_unstack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:30: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_stack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/618094791.py:20: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  step_metrics = jax.tree_map(\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/3314455163.py:17: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  metrics_pp = jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:61: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_unstack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:30: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_stack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:61: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_unstack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:30: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_stack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:61: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_unstack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:30: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_stack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:61: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_unstack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2586646196.py:30: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(_stack, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/618094791.py:20: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  step_metrics = jax.tree_map(\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/618094791.py:27: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  metrics = jax.tree_map(jnp.add, metrics, step_metrics)\n"
     ]
    }
   ],
   "source": [
    "train_step_pp_fn = jax.jit(\n",
    "    shard_map(\n",
    "        train_step_pp,\n",
    "        mesh,\n",
    "        in_specs=(state_pp_specs, P(), P(config.data_axis_name)),\n",
    "        out_specs=(state_pp_specs, P()),\n",
    "        check_rep=False,\n",
    "    ),\n",
    "    donate_argnames=(\"state\", \"metrics\"),\n",
    ")\n",
    "_, metric_shapes = jax.eval_shape(\n",
    "    train_step_pp_fn,\n",
    "    state_pp,\n",
    "    None,\n",
    "    batch,\n",
    ")\n",
    "metrics_pp = jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)\n",
    "state_pp, metrics_pp = train_step_pp_fn(state_pp, metrics_pp, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20e5d19b-0178-4999-90b0-09fb66ba87db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 5_842_984\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parameters: {get_num_params(state_pp):_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "314ca0f0-fbbf-4f4f-bb30-3f40152f46bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(15):\n",
    "    state_pp, metrics_pp = train_step_pp_fn(state_pp, metrics_pp, batch)\n",
    "final_metrics_pp = jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)\n",
    "state_pp, final_metrics_pp = train_step_pp_fn(state_pp, final_metrics_pp, batch)\n",
    "print_metrics(final_metrics_pp, title=\"Final Metrics - Pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab9bd1-53b8-46d3-87ef-e5490546c2f9",
   "metadata": {},
   "source": [
    "## Pipeline Parallelism with Looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ddc7748e-4248-4cb0-a878-b87ec2ccc208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/phlippe/uvadlc_notebooks/master/docs/tutorial_notebooks/scaling/JAX/pipeline_parallel.py...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "# Github URL where python scripts are stored.\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/uvadlc_notebooks/master/docs/tutorial_notebooks/scaling/JAX/\"\n",
    "# Files to download.\n",
    "python_files = [\"pipeline_parallel.py\"]\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in python_files:\n",
    "    if not os.path.isfile(file_name):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_name)\n",
    "        except HTTPError as e:\n",
    "            print(\n",
    "                \"Something went wrong. Please try to download the file directly from the GitHub repository, or contact the author with the full output including the following error:\\n\",\n",
    "                e,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d0ce0e32-cbaf-492a-b14c-8d1d5d928ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_parallel import (\n",
    "    PPClassifier,\n",
    "    get_default_pp_classifier_config,\n",
    "    train_pipeline_model,\n",
    "    train_step_pp,\n",
    ")\n",
    "from flax.struct import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b3c2e508-fbc7-469f-9434-ceedbf33b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PipelineState:\n",
    "    inputs: jax.Array\n",
    "    outputs: jax.Array\n",
    "    input_indices: jax.Array\n",
    "    output_indices: jax.Array\n",
    "    update_indices: jax.Array\n",
    "    params_indices: jax.Array\n",
    "    last_state: jax.Array\n",
    "    rngs: PyTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "263c7c1c-1a8e-48da-bbac-5563f060896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_looping_pipeline_step(\n",
    "    index: jax.Array | int,\n",
    "    state: PipelineState,\n",
    "    *args,\n",
    "    module: nn.Module,\n",
    "    params: PyTree,\n",
    "    model_axis_name: str,\n",
    "    **kwargs,\n",
    ") -> PipelineState:\n",
    "    \"\"\"Single micro-batch pipeline step with loopback communication.\n",
    "\n",
    "    Args:\n",
    "        index: Pipeline step index (between 0 and num_loops * num_microbatches + num_stages - 2).\n",
    "        state: State of the pipeline, including indices for controlling the execution.\n",
    "        *args: Additional arguments to the module.\n",
    "        module: Flax module representing the stage layer to execute.\n",
    "        params: PyTree of parameters. The params for all layers should be stacked along the first axis.\n",
    "        model_axis_name: Name of the model axis in the mesh/shard_map.\n",
    "        **kwargs: Additional keyword arguments to the module.\n",
    "\n",
    "    Returns:\n",
    "        New state of the pipeline after the execution of the pipeline step, with potentially updated\n",
    "        inputs, outputs, rngs, and last_state arrays.\n",
    "    \"\"\"\n",
    "    num_stages = jax.lax.psum(1, model_axis_name)\n",
    "    input_index = state.input_indices[index]\n",
    "    output_index = state.output_indices[index]\n",
    "    update_index = state.update_indices[index]\n",
    "    params_index = state.params_indices[index]\n",
    "    # Update inputs with last state. If update_index is -1, do not update.\n",
    "    # This is used to buffer the communications back to first stage.\n",
    "    clipped_update_index = jnp.clip(update_index, 0, state.inputs.shape[0] - 1)\n",
    "    inputs = jax.lax.dynamic_update_index_in_dim(\n",
    "        state.inputs,\n",
    "        jnp.where(update_index >= 0, state.last_state, state.inputs[clipped_update_index]),\n",
    "        clipped_update_index,\n",
    "        axis=0,\n",
    "    )\n",
    "    # Select input of the current stage. For all stages except the first stage,\n",
    "    # the input is the last output of the previous stage (i.e. last_state).\n",
    "    step_input = jnp.where(\n",
    "        input_index >= 0,\n",
    "        inputs[input_index],\n",
    "        state.last_state,\n",
    "    )\n",
    "    # Apply the module to the input. Select the right set of parameters based\n",
    "    # on the loop index.\n",
    "    rngs = jax.tree_map(lambda rng: jax.random.split(rng, 2), state.rngs)\n",
    "    rngs, step_rngs = jax.tree_map(lambda x: x[0], rngs), jax.tree_map(lambda x: x[1], rngs)\n",
    "    params = jax.tree_map(lambda x: x[params_index], params)\n",
    "    output = module.apply(params, step_input, *args, **kwargs, rngs=step_rngs)\n",
    "    # Update outputs with the output of the current stage. If output_index is -1,\n",
    "    # do not update. This is used to buffer the final outputs of the last stage.\n",
    "    clipped_output_index = jnp.clip(output_index, 0, state.outputs.shape[0] - 1)\n",
    "    outputs = jax.lax.dynamic_update_index_in_dim(\n",
    "        state.outputs,\n",
    "        jnp.where(output_index >= 0, output, state.outputs[clipped_output_index]),\n",
    "        clipped_output_index,\n",
    "        axis=0,\n",
    "    )\n",
    "    # Communicate the last output to the next stage.\n",
    "    last_state = jax.lax.ppermute(\n",
    "        output,\n",
    "        model_axis_name,\n",
    "        perm=[(i, (i + 1) % num_stages) for i in range(num_stages)],\n",
    "    )\n",
    "    return state.replace(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        last_state=last_state,\n",
    "        rngs=rngs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "21efc82a-d5f6-4471-ac32-0f357ccf254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_looping_pipeline_indices(\n",
    "    num_loops: int, num_microbatches: int, num_stages: int, stage_index: jax.Array | int\n",
    ") -> Dict[str, jax.Array]:\n",
    "    \"\"\"Prepare indices for controlling the execution of the looping pipeline.\n",
    "\n",
    "    Args:\n",
    "        num_loops: Number of loops in the pipeline, or separate stage layers per device. num_loops=1 is equivalent to a non-looping pipeline.\n",
    "        num_microbatches: Number of microbatches to split the batch into.\n",
    "        num_stages: Number of stages/devices the pipeline is distributed over.\n",
    "        stage_index: Index of the stage/device in the pipeline.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of indices for controlling the execution of the pipeline.\n",
    "    \"\"\"\n",
    "    num_iterations = num_loops * num_microbatches + num_stages - 1\n",
    "    index_array = -jnp.ones((num_iterations,), dtype=jnp.int32)\n",
    "    # Only first stage uses inputs. Looping communications from last\n",
    "    # stage are buffered in the inputs, so we repeatedly iterate over\n",
    "    # the inputs.\n",
    "    input_indices = jnp.where(\n",
    "        stage_index == 0,\n",
    "        index_array.at[: num_loops * num_microbatches].set(\n",
    "            jnp.tile(jnp.arange(num_microbatches), reps=(num_loops,))\n",
    "        ),\n",
    "        index_array,\n",
    "    )\n",
    "    # For the first stage, identify input indices that we use to buffer\n",
    "    # the communications from the last stage. For all other stages, we\n",
    "    # use the last state from the previous stage as input.\n",
    "    update_indices = jnp.where(\n",
    "        stage_index == 0,\n",
    "        index_array.at[num_stages : num_stages + (num_loops - 1) * num_microbatches].set(\n",
    "            jnp.tile(jnp.arange(num_microbatches), reps=(num_loops - 1,))\n",
    "        ),\n",
    "        index_array,\n",
    "    )\n",
    "    # For the last stage, we use the outputs of the last loop as the\n",
    "    # final outputs.\n",
    "    output_indices = jnp.where(\n",
    "        stage_index == num_stages - 1,\n",
    "        index_array.at[-num_microbatches:].set(jnp.arange(num_microbatches)),\n",
    "        index_array,\n",
    "    )\n",
    "    # For all stages, we iterate over the parameters of the different loops.\n",
    "    # We use the 0-index for indices that fall into the pipeline bubble.\n",
    "    params_indices = jnp.zeros_like(index_array)\n",
    "    for i in range(num_loops):\n",
    "        start_index = stage_index + i * num_microbatches\n",
    "        params_indices = jax.lax.dynamic_update_slice_in_dim(\n",
    "            params_indices,\n",
    "            jnp.full(shape=(num_microbatches,), fill_value=i, dtype=params_indices.dtype),\n",
    "            start_index,\n",
    "            axis=0,\n",
    "        )\n",
    "    return {\n",
    "        \"input\": input_indices,\n",
    "        \"output\": output_indices,\n",
    "        \"update\": update_indices,\n",
    "        \"params\": params_indices,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "94a60cdf-e7bb-49b7-be26-c553f677621e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Stage Index 0 ===========\n",
      "step  :  0  1  2  3  4  5  6  7  8  9\n",
      "input :  0  1  2  3  0  1  2  3 -1 -1\n",
      "output: -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "update: -1 -1 -1  0  1  2  3 -1 -1 -1\n",
      "params:  0  0  0  0  1  1  1  1  0  0\n",
      "\n",
      "=========== Stage Index 1 ===========\n",
      "step  :  0  1  2  3  4  5  6  7  8  9\n",
      "input : -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "output: -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "update: -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "params:  0  0  0  0  0  1  1  1  1  0\n",
      "\n",
      "=========== Stage Index 2 ===========\n",
      "step  :  0  1  2  3  4  5  6  7  8  9\n",
      "input : -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "output: -1 -1 -1 -1 -1 -1  0  1  2  3\n",
      "update: -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "params:  0  0  0  0  0  0  1  1  1  1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_stages = 3\n",
    "num_loops = 2\n",
    "num_microbatches = 4\n",
    "for i in range(num_stages):\n",
    "    indices = prepare_looping_pipeline_indices(\n",
    "        num_loops=num_loops,\n",
    "        num_microbatches=num_microbatches,\n",
    "        num_stages=num_stages,\n",
    "        stage_index=i,\n",
    "    )\n",
    "    s = [\"step  : \" + \" \".join(f\"{t:2d}\" for t in range(len(indices[\"input\"])))]\n",
    "    for k, v in indices.items():\n",
    "        s.append(f\"{k:6s}: \" + \" \".join(f\"{t:2d}\" for t in v))\n",
    "    max_len = max(map(len, s))\n",
    "    s.insert(0, (f\" Stage Index {i} \").center(max_len, \"=\"))\n",
    "    print(\"\\n\".join(s) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "db637630-71c3-4702-b656-7b8a63aa3d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.named_scope(\"pipeline\")\n",
    "def execute_looping_pipeline(\n",
    "    module: nn.Module,\n",
    "    params: PyTree,\n",
    "    x: jax.Array,\n",
    "    rngs: PyTree,\n",
    "    *args,\n",
    "    num_loops: int,\n",
    "    num_microbatches: int,\n",
    "    model_axis_name: str,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Execute a looping pipeline of stages on a batch of data.\n",
    "\n",
    "    Uses a breadth-first strategy to execute the pipeline stages in parallel.\n",
    "\n",
    "    Args:\n",
    "        module: Flax module representing a single pipeline stage to execute.\n",
    "        params: PyTree of parameters for the pipeline stages.\n",
    "        x: Batch of input data, only needed on device of the first stage. Data will be split into micro-batches.\n",
    "        rngs: PyTree of random number generators for the pipeline stages.\n",
    "        *args: Additional arguments to the module.\n",
    "        num_loops: Number of loops in the pipeline, or separate stage layers per device. num_loops=1 is equivalent to a non-looping pipeline.\n",
    "        num_microbatches: Number of micro-batches to split the batch into.\n",
    "        model_axis_name: Name of the model axis in the mesh/shard_map.\n",
    "        **kwargs: Additional keyword arguments to the module.\n",
    "\n",
    "    Returns:\n",
    "        Output of the last stage of the pipeline, with equivalent shape to input x. For devices that are not\n",
    "        the last stage, the output is zeros.\n",
    "    \"\"\"\n",
    "    num_stages = jax.lax.psum(1, model_axis_name)\n",
    "    assert num_stages > 1, \"Pipeline must have at least 2 stages.\"\n",
    "    stage_index = jax.lax.axis_index(model_axis_name)\n",
    "    # Structure the input data into micro-batches.\n",
    "    batch_size = x.shape[0]\n",
    "    assert (\n",
    "        batch_size % num_microbatches == 0\n",
    "    ), f\"Batch size {batch_size} must be divisible by number of microbatches {num_microbatches}\"\n",
    "    microbatch_size = batch_size // num_microbatches\n",
    "    microbatches = jnp.reshape(x, (num_microbatches, microbatch_size, *x.shape[1:]))\n",
    "    last_state = jnp.zeros_like(microbatches[0])\n",
    "    outputs = jnp.zeros_like(microbatches)\n",
    "    # Prepare indices for each stage.\n",
    "    indices = prepare_looping_pipeline_indices(\n",
    "        num_loops=num_loops,\n",
    "        num_microbatches=num_microbatches,\n",
    "        num_stages=num_stages,\n",
    "        stage_index=stage_index,\n",
    "    )\n",
    "    num_iterations = indices[\"input\"].shape[0]\n",
    "    pipeline_state = PipelineState(\n",
    "        inputs=microbatches,\n",
    "        outputs=outputs,\n",
    "        input_indices=indices[\"input\"],\n",
    "        output_indices=indices[\"output\"],\n",
    "        update_indices=indices[\"update\"],\n",
    "        params_indices=indices[\"params\"],\n",
    "        last_state=last_state,\n",
    "        rngs=rngs,\n",
    "    )\n",
    "    # Execute the pipeline via a jax fori_loop. Alternatively, a\n",
    "    # scan could be used to execute the pipeline.\n",
    "    pipeline_fn = functools.partial(\n",
    "        execute_looping_pipeline_step,\n",
    "        *args,\n",
    "        module=module,\n",
    "        params=params,\n",
    "        model_axis_name=model_axis_name,\n",
    "        **kwargs,\n",
    "    )\n",
    "    pipeline_state = jax.lax.fori_loop(\n",
    "        0,\n",
    "        num_iterations,\n",
    "        body_fun=pipeline_fn,\n",
    "        init_val=pipeline_state,\n",
    "    )\n",
    "    # Return the final outputs, reshaped as original input.\n",
    "    outputs = pipeline_state.outputs\n",
    "    return jnp.reshape(outputs, (batch_size, *outputs.shape[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "085dcefe-aca6-4161-bd07-ebcb029d1db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopingPipelineModule(nn.Module):\n",
    "    num_loops: int\n",
    "    model_axis_name: str\n",
    "    num_microbatches: int\n",
    "    module_fn: Callable[..., nn.Module]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jax.Array, *args, **kwargs):\n",
    "        if self.is_initializing():\n",
    "            # During initialization, we want to create a separate set of parameters\n",
    "            # for each loop. We do this by scanning the module during init. Note that\n",
    "            # we do not need to execute the pipeline, since we only need to create the\n",
    "            # parameters.\n",
    "            sample_microbatch = x[:: self.num_microbatches]\n",
    "            module = self.module_fn()\n",
    "            scan_fn = nn.scan(\n",
    "                lambda module, carry, _: (module(carry, *args, **kwargs), None),\n",
    "                variable_axes={\"params\": 0},\n",
    "                split_rngs={\"params\": True, \"dropout\": True},\n",
    "                length=self.num_loops,\n",
    "            )\n",
    "            out, _ = scan_fn(module, sample_microbatch, ())\n",
    "            return jnp.repeat(out, self.num_microbatches, axis=0)\n",
    "        else:\n",
    "            # During the forward pass, we extract the initialized parameters for\n",
    "            # all loops. In the pipeline, we then sub-index the parameters based on\n",
    "            # the loop index.\n",
    "            module = self.module_fn()\n",
    "            params = module.variables\n",
    "            # Since we make use of a non-flax transformation, we need to pass the\n",
    "            # RNGs explicitly to the pipeline.\n",
    "            rngs = {name: self.make_rng(name) for name in self.scope.rngs}\n",
    "            return execute_looping_pipeline(\n",
    "                module=module,\n",
    "                params=params,\n",
    "                x=x,\n",
    "                rngs=rngs,\n",
    "                *args,\n",
    "                num_loops=self.num_loops,\n",
    "                num_microbatches=self.num_microbatches,\n",
    "                model_axis_name=self.model_axis_name,\n",
    "                **kwargs,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e63f1df6-6ca1-4425-a629-faf3f174267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_looping_classifier(config: ConfigDict) -> nn.Module:\n",
    "    looping_model_config = config.copy_and_resolve_references()\n",
    "    looping_model_config.num_layers = 1\n",
    "    looping_module_class = functools.partial(\n",
    "        LoopingPipelineModule,\n",
    "        num_loops=config.num_layers,\n",
    "    )\n",
    "    return PPClassifier(config=looping_model_config, pipeline_module_class=looping_module_class)\n",
    "\n",
    "\n",
    "config = get_default_pp_classifier_config()\n",
    "model_lpp = get_looping_classifier(config.model)\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=config.optimizer.learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "12d139b8-1b28-4044-94a3-6ea968a44d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_array = np.array(jax.devices()).reshape(-1, config.model_axis_size)\n",
    "mesh = Mesh(device_array, (config.data_axis_name, config.model_axis_name))\n",
    "rng = jax.random.PRNGKey(config.seed)\n",
    "model_init_rng, data_inputs_rng, data_labels_rng = jax.random.split(rng, 3)\n",
    "batch = Batch(\n",
    "    inputs=jax.random.normal(data_inputs_rng, (config.data.batch_size, config.data.input_size)),\n",
    "    labels=jax.random.randint(\n",
    "        data_labels_rng, (config.data.batch_size,), 0, config.data.num_classes\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f1e385f5-392f-400b-83ab-3ad81a88dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_fn(rng: jax.random.PRNGKey, x: jax.Array, model: nn.Module) -> TrainState:\n",
    "    init_rng, rng = jax.random.split(rng)\n",
    "    variables = model.init({\"params\": init_rng}, x, train=False)\n",
    "    params = variables.pop(\"params\")\n",
    "    state = TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        tx=optimizer,\n",
    "        rng=rng,\n",
    "    )\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1b2a9666-f649-41a2-8c04-4cbaf8bff79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_dense': {'sharded': {'bias': PartitionSpec('model', None),\n",
      "                             'kernel': PartitionSpec('model', None, None)}},\n",
      " 'output_dense': {'sharded': {'bias': PartitionSpec('model', None),\n",
      "                              'kernel': PartitionSpec('model', None, None)}},\n",
      " 'output_norm': {'sharded': {'bias': PartitionSpec('model', None),\n",
      "                             'scale': PartitionSpec('model', None)}},\n",
      " 'pipeline': {'sharded': {'mlp_layers': {'block': {'input_dense': {'bias': PartitionSpec('model', None, None, None),\n",
      "                                                                   'kernel': PartitionSpec('model', None, None, None, None)},\n",
      "                                                   'output_dense': {'bias': PartitionSpec('model', None, None, None),\n",
      "                                                                    'kernel': PartitionSpec('model', None, None, None, None)},\n",
      "                                                   'pre_norm': {'bias': PartitionSpec('model', None, None, None),\n",
      "                                                                'scale': PartitionSpec('model', None, None, None)}}}}}}\n"
     ]
    }
   ],
   "source": [
    "init_lpp_fn = shard_map(\n",
    "    functools.partial(init_fn, model=model_lpp),\n",
    "    mesh,\n",
    "    in_specs=(P(), P(config.data_axis_name)),\n",
    "    out_specs=P(),\n",
    "    check_rep=False,\n",
    ")\n",
    "state_lpp_shapes = jax.eval_shape(init_lpp_fn, model_init_rng, batch.inputs)\n",
    "state_lpp_specs = nn.get_partition_spec(state_lpp_shapes)\n",
    "pprint(state_lpp_specs.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3ec0c97f-67e2-484e-b153-63f077def138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_dense': {'bias': Partitioned(value=(4, 2, 1, 512),\n",
      "                                     names=('model', None, None, None),\n",
      "                                     mesh=None),\n",
      "                 'kernel': Partitioned(value=(4, 2, 1, 512, 512),\n",
      "                                       names=('model', None, None, None, None),\n",
      "                                       mesh=None)},\n",
      " 'output_dense': {'bias': Partitioned(value=(4, 2, 1, 512),\n",
      "                                      names=('model', None, None, None),\n",
      "                                      mesh=None),\n",
      "                  'kernel': Partitioned(value=(4, 2, 1, 512, 512),\n",
      "                                        names=('model', None, None, None, None),\n",
      "                                        mesh=None)},\n",
      " 'pre_norm': {'bias': Partitioned(value=(4, 2, 1, 512),\n",
      "                                  names=('model', None, None, None),\n",
      "                                  mesh=None),\n",
      "              'scale': Partitioned(value=(4, 2, 1, 512),\n",
      "                                   names=('model', None, None, None),\n",
      "                                   mesh=None)}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/1277199832.py:13: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  jax.tree_map(lambda x: x.shape, state_lpp.params[\"pipeline\"][\"sharded\"][\"mlp_layers\"][\"block\"])\n"
     ]
    }
   ],
   "source": [
    "init_lpp_fn = jax.jit(\n",
    "    shard_map(\n",
    "        functools.partial(init_fn, model=model_lpp),\n",
    "        mesh,\n",
    "        in_specs=(P(), P(config.data_axis_name)),\n",
    "        out_specs=state_lpp_specs,\n",
    "        check_rep=False,\n",
    "    ),\n",
    ")\n",
    "state_lpp = init_lpp_fn(model_init_rng, batch.inputs)\n",
    "\n",
    "pprint(\n",
    "    jax.tree_map(lambda x: x.shape, state_lpp.params[\"pipeline\"][\"sharded\"][\"mlp_layers\"][\"block\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5c9789fb-76c3-4308-a585-8434efd5afc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/1887405646.py:48: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  rngs = jax.tree_map(lambda rng: jax.random.split(rng, 2), state.rngs)\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/1887405646.py:49: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  rngs, step_rngs = jax.tree_map(lambda x: x[0], rngs), jax.tree_map(lambda x: x[1], rngs)\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/1887405646.py:50: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  params = jax.tree_map(lambda x: x[params_index], params)\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/2795158517.py:17: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  metrics_lpp = jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/1887405646.py:48: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  rngs = jax.tree_map(lambda rng: jax.random.split(rng, 2), state.rngs)\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/1887405646.py:49: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  rngs, step_rngs = jax.tree_map(lambda x: x[0], rngs), jax.tree_map(lambda x: x[1], rngs)\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_68565/1887405646.py:50: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  params = jax.tree_map(lambda x: x[params_index], params)\n"
     ]
    }
   ],
   "source": [
    "train_step_lpp_fn = jax.jit(\n",
    "    shard_map(\n",
    "        functools.partial(train_step_pp, config=config),\n",
    "        mesh,\n",
    "        in_specs=(state_lpp_specs, P(), P(config.data_axis_name)),\n",
    "        out_specs=(state_lpp_specs, P()),\n",
    "        check_rep=False,\n",
    "    ),\n",
    "    donate_argnames=(\"state\", \"metrics\"),\n",
    ")\n",
    "state_shapes, metric_shapes = jax.eval_shape(\n",
    "    train_step_lpp_fn,\n",
    "    state_lpp,\n",
    "    None,\n",
    "    batch,\n",
    ")\n",
    "metrics_lpp = jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)\n",
    "state_lpp, metrics_lpp = train_step_lpp_fn(state_lpp, metrics_lpp, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b9e1b057-8abb-41ce-abe2-06b16bc6f294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 5_842_984\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of parameters: {get_num_params(state_lpp):_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "524ab850-6491-4719-90cc-b8ae12544f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(15):\n",
    "    state_lpp, metrics_lpp = train_step_lpp_fn(state_lpp, metrics_lpp, batch)\n",
    "final_metrics_lpp = jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)\n",
    "state_lpp, final_metrics_lpp = train_step_lpp_fn(state_lpp, final_metrics_lpp, batch)\n",
    "print_metrics(final_metrics_lpp, title=\"Final Metrics - Looping Pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df9d1a7-d658-4123-ad64-183058b7719e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
