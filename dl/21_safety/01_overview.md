# Overview

I'll be the first to admit that I currently do not have an ["inside view about AI Safety."](https://www.neelnanda.io/blog/47-inside-views) I do think it's important, but lack the knowledge about both the field and myself to truly understand which arguments truly resonate with me. 

To that end, this document will pen down arguments made by others, which will hopefully guide me to having a more independent view on this topic in the future. 

## The Importance of Safety

- Catastrophic AI risks can be grouped under the following categories: [(Main Source)](https://www.safe.ai/ai-risk)
  - Malicious use
  - Disruptive social impact
  - Accidents 
  - Rogue AIs
    - Potential to optimize flawed objectives
    - Potential to become power-seeking, resist shutdown, and engage in deception as a result of developing harmful secondary goals.
- Compounding effects
  - We're making advances in AI extremely quickly, and concerningly, _by accident_
  - We do not know how to train systems to robustly behave well. Relatedly, there's a lack of interpretability when it comes to these large models. 
  - AI safety work is largely neglected, especially compared to the focus on performance

## Categorizations

The additional notes in this folder function to categorize papers that I have read. Here, I have adopted [Anthropic's categorizations](https://www.anthropic.com/research), but may change this in the future especially if I encounter papers that don't neatly fit into these categorizations.