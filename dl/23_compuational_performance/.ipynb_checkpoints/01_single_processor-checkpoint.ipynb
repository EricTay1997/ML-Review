{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3292874c-79db-445d-b97c-18302bb1d25c",
   "metadata": {},
   "source": [
    "# Code adapted from https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial2/Introduction_to_JAX.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7fd3876b-ebb4-4213-9d6b-00b5957469f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_62240/537004603.py:18: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "from flax.training import checkpoints\n",
    "from typing import Any, Callable, Dict, Tuple\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "import torch.utils.data as data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgba\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf76ab76-b160-40da-ac0f-561ebbd1cd12",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# JAX Example Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44194f8e-cce1-4fd0-b7a7-6da1cd0f7d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifierCompact(nn.Module):\n",
    "    num_hidden : int   # Number of hidden neurons\n",
    "    num_outputs : int  # Number of output neurons\n",
    "\n",
    "    @nn.compact  # Tells Flax to look for defined submodules\n",
    "    def __call__(self, x):\n",
    "        # Perform the calculation of the model to determine the prediction\n",
    "        # while defining necessary layers\n",
    "        x = nn.Dense(features=self.num_hidden)(x)\n",
    "        x = nn.tanh(x)\n",
    "        x = nn.Dense(features=self.num_outputs)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "394de098-7a46-4498-a4f3-d1c10d30b404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': {'Dense_0': {'kernel': Array([[-0.8734889 ,  0.03292416,  0.45095628,  0.9860286 ,  0.9650168 ,\n",
      "        -0.50356966, -0.567441  , -0.32092765],\n",
      "       [ 0.6106076 , -0.8035141 , -0.8497237 , -1.0364467 ,  0.11642699,\n",
      "        -0.37274948, -0.06301995,  0.23880544]], dtype=float32), 'bias': Array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)}, 'Dense_1': {'kernel': Array([[-0.08973367],\n",
      "       [-0.15572299],\n",
      "       [ 0.12597609],\n",
      "       [-0.02248076],\n",
      "       [ 0.48822802],\n",
      "       [ 0.19107282],\n",
      "       [-0.32372728],\n",
      "       [-0.04857434]], dtype=float32), 'bias': Array([0.], dtype=float32)}}}\n"
     ]
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(42)\n",
    "model = SimpleClassifierCompact(num_hidden=8, num_outputs=1)\n",
    "rng, inp_rng, init_rng = jax.random.split(rng, 3)\n",
    "inp = jax.random.normal(inp_rng, (8, 2))  \n",
    "params = model.init(init_rng, inp)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0260d40-9cb6-46ab-b52f-eaf7c439975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XORDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, size, seed, std=0.1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            size - Number of data points we want to generate\n",
    "            seed - The seed to use to create the PRNG state with which we want to generate the data points\n",
    "            std - Standard deviation of the noise (see generate_continuous_xor function)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.np_rng = np.random.RandomState(seed=seed)\n",
    "        self.std = std\n",
    "        self.generate_continuous_xor()\n",
    "\n",
    "    def generate_continuous_xor(self):\n",
    "        data = self.np_rng.randint(low=0, high=2, size=(self.size, 2)).astype(np.float32)\n",
    "        # If x=y, the label is 0.\n",
    "        label = (data.sum(axis=1) == 1).astype(np.int32)\n",
    "        # Add gaussian noise to the data points.\n",
    "        data += self.np_rng.normal(loc=0.0, scale=self.std, size=data.shape)\n",
    "\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the idx-th data point of the dataset\n",
    "        # If we have multiple things to return (data point and label), we can return them as tuple\n",
    "        data_point = self.data[idx]\n",
    "        data_label = self.label[idx]\n",
    "        return data_point, data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "244e8219-2783-483a-938f-35109c5922a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bef8ccd9-3400-4350-acc9-601d00169ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importantly, we define these in numpy \n",
    "train_dataset = XORDataset(size=2500, seed=42)\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=numpy_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bb04fd6-2ad7-4d64-84b0-f10d3cd688a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.sgd(learning_rate=0.1)\n",
    "model_state = train_state.TrainState.create(apply_fn=model.apply,\n",
    "                                            params=params,\n",
    "                                            tx=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb05e809-5199-41c9-ba0b-61802e63d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_acc(state, params, batch):\n",
    "    data_input, labels = batch\n",
    "    # Obtain the logits and predictions of the model for the input data\n",
    "    logits = state.apply_fn(params, data_input).squeeze(axis=-1)\n",
    "    pred_labels = (logits > 0).astype(jnp.float32)\n",
    "    # Calculate the loss and accuracy\n",
    "    loss = optax.sigmoid_binary_cross_entropy(logits, labels).mean()\n",
    "    acc = (pred_labels == labels).mean()\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71160fca-8a28-44be-9744-c760a38014a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit  # Jit the function for efficiency\n",
    "def train_step(state, batch):\n",
    "    grad_fn = jax.value_and_grad(calculate_loss_acc,  # Function to calculate the loss\n",
    "                                 argnums=1,  # Parameters are second argument of the function\n",
    "                                 has_aux=True  # Function has additional outputs, here accuracy\n",
    "                                )\n",
    "    (loss, acc), grads = grad_fn(state, state.params, batch)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "658b44a1-dae7-4c29-8e68-408de77be372",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit  # Jit the function for efficiency\n",
    "def eval_step(state, batch):\n",
    "    _, acc = calculate_loss_acc(state, state.params, batch)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e609cee6-1bed-4451-8131-6f334d1e8b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(state, data_loader, num_epochs=100):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for batch in data_loader:\n",
    "            state, loss, acc = train_step(state, batch)\n",
    "            # We could use the loss and accuracy for logging here, e.g. in TensorBoard\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f89804a-9cba-433a-a012-9982ab345991",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_state = train_model(model_state, train_data_loader, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ea59b59-1700-40da-8054-5b68952224e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints.save_checkpoint(ckpt_dir='my_checkpoints/',  # Folder to save checkpoint in\n",
    "                            target=trained_model_state,  # What to save. To only save parameters, use model_state.params\n",
    "                            step=100,  # Training step or other metric to save best model on\n",
    "                            prefix='my_model',  # Checkpoint file name prefix\n",
    "                            overwrite=True   # Overwrite existing checkpoint files\n",
    "                           )\n",
    "\n",
    "loaded_model_state = checkpoints.restore_checkpoint(\n",
    "                                             ckpt_dir='my_checkpoints/',   # Folder with the checkpoints\n",
    "                                             target=model_state,   # (optional) matching object to rebuild state in\n",
    "                                             prefix='my_model'  # Checkpoint file name prefix\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5f0146e-2f48-408b-a5f6-fb2a1de0903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = XORDataset(size=500, seed=123)\n",
    "# drop_last -> Don't drop the last batch although it is smaller than 128\n",
    "test_data_loader = data.DataLoader(test_dataset,\n",
    "                                   batch_size=128,\n",
    "                                   shuffle=False,\n",
    "                                   drop_last=False,\n",
    "                                   collate_fn=numpy_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "37af7d71-34a9-4d44-bae9-5fe1afebfad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(state, data_loader):\n",
    "    all_accs, batch_sizes = [], []\n",
    "    for batch in data_loader:\n",
    "        batch_acc = eval_step(state, batch)\n",
    "        all_accs.append(batch_acc)\n",
    "        batch_sizes.append(batch[0].shape[0])\n",
    "    # Weighted average since some batches might be smaller\n",
    "    acc = sum([a*b for a,b in zip(all_accs, batch_sizes)]) / sum(batch_sizes)\n",
    "    print(f\"Accuracy of the model: {100.0*acc:4.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1f34c75-48e7-4f75-bc49-71e1e7591cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(trained_model_state, test_data_loader)\n",
    "# trained_model = model.bind(trained_model_state.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58bea65b-cffe-4d6e-b3c6-9b3ae74249db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For dropout random state\n",
    "# See https://flax-linen.readthedocs.io/en/latest/guides/training_techniques/dropout.html\n",
    "\n",
    "# For batch norm running stats\n",
    "# See https://flax-linen.readthedocs.io/en/latest/guides/training_techniques/batch_norm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23aaf8e-857e-4399-95e7-5d61246a3f62",
   "metadata": {},
   "source": [
    "# Memory Reduction Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d396b388-8cc1-42b3-9d40-01d7b4d47c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"XLA_FLAGS\"] = (\n",
    "    \"--xla_gpu_enable_triton_softmax_fusion=true \"\n",
    "    \"--xla_gpu_triton_gemm_any=false \"\n",
    "    \"--xla_gpu_enable_async_collectives=true \"\n",
    "    \"--xla_gpu_enable_latency_hiding_scheduler=true \"\n",
    "    \"--xla_gpu_enable_highest_priority_async_stream=true \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff976a92-c753-4f33-99d8-92b574cab601",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mixed Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea44829a-c5a9-4e9c-85ff-40e2e4071ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    dtype: Any\n",
    "    hidden_size: int = 256\n",
    "    num_classes: int = 100\n",
    "    dropout_rate: float = 0.1\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jax.Array, train: bool) -> jax.Array:\n",
    "        x = nn.Dense(\n",
    "            features=self.hidden_size,\n",
    "            dtype=self.dtype,  # Computation in specified dtype, params stay in float32\n",
    "        )(x)\n",
    "        x = nn.LayerNorm(dtype=self.dtype)(x)\n",
    "        x = nn.silu(x)\n",
    "        x = nn.Dropout(rate=self.dropout_rate, deterministic=not train)(x)\n",
    "        x = nn.Dense(\n",
    "            features=self.num_classes,\n",
    "            dtype=self.dtype,\n",
    "        )(x)\n",
    "        x = x.astype(jnp.float32)\n",
    "        x = nn.log_softmax(x, axis=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e43360ae-e576-4989-9f2d-584a765ce38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                      MLPClassifier Summary                                       </span>\n",
       "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> path        </span>┃<span style=\"font-weight: bold\"> module        </span>┃<span style=\"font-weight: bold\"> inputs             </span>┃<span style=\"font-weight: bold\"> outputs          </span>┃<span style=\"font-weight: bold\"> params                   </span>┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│             │ MLPClassifier │ - <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,128] │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,100] │                          │\n",
       "│             │               │ - train: True      │                  │                          │\n",
       "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
       "│ Dense_0     │ Dense         │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,128]   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,256] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]       │\n",
       "│             │               │                    │                  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,256] │\n",
       "│             │               │                    │                  │                          │\n",
       "│             │               │                    │                  │ <span style=\"font-weight: bold\">33,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(132.1 KB)</span>        │\n",
       "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
       "│ LayerNorm_0 │ LayerNorm     │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,256]   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,256] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]       │\n",
       "│             │               │                    │                  │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]      │\n",
       "│             │               │                    │                  │                          │\n",
       "│             │               │                    │                  │ <span style=\"font-weight: bold\">512 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(2.0 KB)</span>             │\n",
       "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
       "│ Dropout_0   │ Dropout       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,256]   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,256] │                          │\n",
       "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
       "│ Dense_1     │ Dense         │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,256]   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,100] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[100]       │\n",
       "│             │               │                    │                  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256,100] │\n",
       "│             │               │                    │                  │                          │\n",
       "│             │               │                    │                  │ <span style=\"font-weight: bold\">25,700 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(102.8 KB)</span>        │\n",
       "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
       "│<span style=\"font-weight: bold\">             </span>│<span style=\"font-weight: bold\">               </span>│<span style=\"font-weight: bold\">                    </span>│<span style=\"font-weight: bold\">            Total </span>│<span style=\"font-weight: bold\"> 59,236 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(236.9 KB)</span><span style=\"font-weight: bold\">        </span>│\n",
       "└─────────────┴───────────────┴────────────────────┴──────────────────┴──────────────────────────┘\n",
       "<span style=\"font-weight: bold\">                                                                                                  </span>\n",
       "<span style=\"font-weight: bold\">                               Total Parameters: 59,236 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(236.9 KB)</span><span style=\"font-weight: bold\">                                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                      MLPClassifier Summary                                       \u001b[0m\n",
       "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mpath       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams                  \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│             │ MLPClassifier │ - \u001b[2mfloat32\u001b[0m[512,128] │ \u001b[2mfloat32\u001b[0m[512,100] │                          │\n",
       "│             │               │ - train: True      │                  │                          │\n",
       "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
       "│ Dense_0     │ Dense         │ \u001b[2mfloat32\u001b[0m[512,128]   │ \u001b[2mfloat32\u001b[0m[512,256] │ bias: \u001b[2mfloat32\u001b[0m[256]       │\n",
       "│             │               │                    │                  │ kernel: \u001b[2mfloat32\u001b[0m[128,256] │\n",
       "│             │               │                    │                  │                          │\n",
       "│             │               │                    │                  │ \u001b[1m33,024 \u001b[0m\u001b[1;2m(132.1 KB)\u001b[0m        │\n",
       "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
       "│ LayerNorm_0 │ LayerNorm     │ \u001b[2mfloat32\u001b[0m[512,256]   │ \u001b[2mfloat32\u001b[0m[512,256] │ bias: \u001b[2mfloat32\u001b[0m[256]       │\n",
       "│             │               │                    │                  │ scale: \u001b[2mfloat32\u001b[0m[256]      │\n",
       "│             │               │                    │                  │                          │\n",
       "│             │               │                    │                  │ \u001b[1m512 \u001b[0m\u001b[1;2m(2.0 KB)\u001b[0m             │\n",
       "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
       "│ Dropout_0   │ Dropout       │ \u001b[2mfloat32\u001b[0m[512,256]   │ \u001b[2mfloat32\u001b[0m[512,256] │                          │\n",
       "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
       "│ Dense_1     │ Dense         │ \u001b[2mfloat32\u001b[0m[512,256]   │ \u001b[2mfloat32\u001b[0m[512,100] │ bias: \u001b[2mfloat32\u001b[0m[100]       │\n",
       "│             │               │                    │                  │ kernel: \u001b[2mfloat32\u001b[0m[256,100] │\n",
       "│             │               │                    │                  │                          │\n",
       "│             │               │                    │                  │ \u001b[1m25,700 \u001b[0m\u001b[1;2m(102.8 KB)\u001b[0m        │\n",
       "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1m           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                  \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m           Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m59,236 \u001b[0m\u001b[1;2m(236.9 KB)\u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\n",
       "└─────────────┴───────────────┴────────────────────┴──────────────────┴──────────────────────────┘\n",
       "\u001b[1m                                                                                                  \u001b[0m\n",
       "\u001b[1m                               Total Parameters: 59,236 \u001b[0m\u001b[1;2m(236.9 KB)\u001b[0m\u001b[1m                                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = jnp.ones((512, 128), dtype=jnp.float32)\n",
    "rngs = {\"params\": jax.random.PRNGKey(0), \"dropout\": jax.random.PRNGKey(1)}\n",
    "model_float32 = MLPClassifier(dtype=jnp.float32)\n",
    "model_float32.tabulate(rngs, x, train=True, console_kwargs={\"force_jupyter\": True});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "552d3430-1f52-438f-94ae-e4889ee7351e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                       MLPClassifier Summary                                       </span>\n",
       "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> path        </span>┃<span style=\"font-weight: bold\"> module        </span>┃<span style=\"font-weight: bold\"> inputs             </span>┃<span style=\"font-weight: bold\"> outputs           </span>┃<span style=\"font-weight: bold\"> params                   </span>┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│             │ MLPClassifier │ - <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,128] │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,100]  │                          │\n",
       "│             │               │ - train: True      │                   │                          │\n",
       "├─────────────┼───────────────┼────────────────────┼───────────────────┼──────────────────────────┤\n",
       "│ Dense_0     │ Dense         │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,128]   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">bfloat16</span>[512,256] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]       │\n",
       "│             │               │                    │                   │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,256] │\n",
       "│             │               │                    │                   │                          │\n",
       "│             │               │                    │                   │ <span style=\"font-weight: bold\">33,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(132.1 KB)</span>        │\n",
       "├─────────────┼───────────────┼────────────────────┼───────────────────┼──────────────────────────┤\n",
       "│ LayerNorm_0 │ LayerNorm     │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">bfloat16</span>[512,256]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">bfloat16</span>[512,256] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]       │\n",
       "│             │               │                    │                   │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]      │\n",
       "│             │               │                    │                   │                          │\n",
       "│             │               │                    │                   │ <span style=\"font-weight: bold\">512 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(2.0 KB)</span>             │\n",
       "├─────────────┼───────────────┼────────────────────┼───────────────────┼──────────────────────────┤\n",
       "│ Dropout_0   │ Dropout       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">bfloat16</span>[512,256]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">bfloat16</span>[512,256] │                          │\n",
       "├─────────────┼───────────────┼────────────────────┼───────────────────┼──────────────────────────┤\n",
       "│ Dense_1     │ Dense         │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">bfloat16</span>[512,256]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">bfloat16</span>[512,100] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[100]       │\n",
       "│             │               │                    │                   │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256,100] │\n",
       "│             │               │                    │                   │                          │\n",
       "│             │               │                    │                   │ <span style=\"font-weight: bold\">25,700 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(102.8 KB)</span>        │\n",
       "├─────────────┼───────────────┼────────────────────┼───────────────────┼──────────────────────────┤\n",
       "│<span style=\"font-weight: bold\">             </span>│<span style=\"font-weight: bold\">               </span>│<span style=\"font-weight: bold\">                    </span>│<span style=\"font-weight: bold\">             Total </span>│<span style=\"font-weight: bold\"> 59,236 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(236.9 KB)</span><span style=\"font-weight: bold\">        </span>│\n",
       "└─────────────┴───────────────┴────────────────────┴───────────────────┴──────────────────────────┘\n",
       "<span style=\"font-weight: bold\">                                                                                                   </span>\n",
       "<span style=\"font-weight: bold\">                                Total Parameters: 59,236 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(236.9 KB)</span><span style=\"font-weight: bold\">                                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                       MLPClassifier Summary                                       \u001b[0m\n",
       "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mpath       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams                  \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│             │ MLPClassifier │ - \u001b[2mfloat32\u001b[0m[512,128] │ \u001b[2mfloat32\u001b[0m[512,100]  │                          │\n",
       "│             │               │ - train: True      │                   │                          │\n",
       "├─────────────┼───────────────┼────────────────────┼───────────────────┼──────────────────────────┤\n",
       "│ Dense_0     │ Dense         │ \u001b[2mfloat32\u001b[0m[512,128]   │ \u001b[2mbfloat16\u001b[0m[512,256] │ bias: \u001b[2mfloat32\u001b[0m[256]       │\n",
       "│             │               │                    │                   │ kernel: \u001b[2mfloat32\u001b[0m[128,256] │\n",
       "│             │               │                    │                   │                          │\n",
       "│             │               │                    │                   │ \u001b[1m33,024 \u001b[0m\u001b[1;2m(132.1 KB)\u001b[0m        │\n",
       "├─────────────┼───────────────┼────────────────────┼───────────────────┼──────────────────────────┤\n",
       "│ LayerNorm_0 │ LayerNorm     │ \u001b[2mbfloat16\u001b[0m[512,256]  │ \u001b[2mbfloat16\u001b[0m[512,256] │ bias: \u001b[2mfloat32\u001b[0m[256]       │\n",
       "│             │               │                    │                   │ scale: \u001b[2mfloat32\u001b[0m[256]      │\n",
       "│             │               │                    │                   │                          │\n",
       "│             │               │                    │                   │ \u001b[1m512 \u001b[0m\u001b[1;2m(2.0 KB)\u001b[0m             │\n",
       "├─────────────┼───────────────┼────────────────────┼───────────────────┼──────────────────────────┤\n",
       "│ Dropout_0   │ Dropout       │ \u001b[2mbfloat16\u001b[0m[512,256]  │ \u001b[2mbfloat16\u001b[0m[512,256] │                          │\n",
       "├─────────────┼───────────────┼────────────────────┼───────────────────┼──────────────────────────┤\n",
       "│ Dense_1     │ Dense         │ \u001b[2mbfloat16\u001b[0m[512,256]  │ \u001b[2mbfloat16\u001b[0m[512,100] │ bias: \u001b[2mfloat32\u001b[0m[100]       │\n",
       "│             │               │                    │                   │ kernel: \u001b[2mfloat32\u001b[0m[256,100] │\n",
       "│             │               │                    │                   │                          │\n",
       "│             │               │                    │                   │ \u001b[1m25,700 \u001b[0m\u001b[1;2m(102.8 KB)\u001b[0m        │\n",
       "├─────────────┼───────────────┼────────────────────┼───────────────────┼──────────────────────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1m           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                  \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m            Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m59,236 \u001b[0m\u001b[1;2m(236.9 KB)\u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\n",
       "└─────────────┴───────────────┴────────────────────┴───────────────────┴──────────────────────────┘\n",
       "\u001b[1m                                                                                                   \u001b[0m\n",
       "\u001b[1m                                Total Parameters: 59,236 \u001b[0m\u001b[1;2m(236.9 KB)\u001b[0m\u001b[1m                                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bfloat16 = MLPClassifier(dtype=jnp.bfloat16)\n",
    "model_bfloat16.tabulate(rngs, x, train=True, console_kwargs={\"force_jupyter\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94618b6-3d63-44b8-8b5d-57df61bbc5e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Gradient Checkpointing / Activation Recomputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f499a3f5-a221-4bb9-95f1-b2136112c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x: jax.Array) -> jax.Array:\n",
    "    jax.debug.print(\"Executing GeLU\")\n",
    "    x3 = jnp.power(x, 3)\n",
    "    tanh_input = np.sqrt(2 / np.pi) * (x + 0.044715 * x3)\n",
    "    return 0.5 * x * (1 + jnp.tanh(tanh_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eefcebdf-1b07-49bd-afa5-5a8bc32558f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(x: jax.Array, remat: bool) -> jax.Array:\n",
    "    act_fn = gelu\n",
    "    if remat:\n",
    "        act_fn = jax.remat(act_fn)\n",
    "    return jnp.mean(act_fn(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "07552555-7029-45e0-af3b-94bb89f29d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing GeLU\n",
      "Executing GeLU\n"
     ]
    }
   ],
   "source": [
    "x = jax.random.normal(jax.random.PRNGKey(0), (100,))\n",
    "grad_fn = jax.grad(loss_fn)\n",
    "_ = grad_fn(x, remat=True) # remat controls which tensors are stored and which are recomputed during the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7c98e1af-dfa8-4fcd-a567-f31efb10af9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing GeLU\n"
     ]
    }
   ],
   "source": [
    "_ = loss_fn(x, remat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f9c1a-f438-4441-a2d9-7b955fb84d36",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Gradient Accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "086447f6-24f1-4b62-b033-78f03f30c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from pprint import pprint\n",
    "from typing import Any, Callable, Dict, Tuple\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.struct import dataclass\n",
    "from flax.training import train_state\n",
    "\n",
    "# Type aliases\n",
    "PyTree = Any\n",
    "Metrics = Dict[str, Tuple[jax.Array, ...]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2f6b0911-ae28-404c-8b3d-aaaeb65e0f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    rng: jax.Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1bade8ab-d132-4c77-b971-0e39e93e75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Batch:\n",
    "    inputs: jax.Array\n",
    "    labels: jax.Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a5eb12da-6f98-4e69-b3af-afb2dae0fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_loss_fn(\n",
    "    params: PyTree, apply_fn: Any, batch: Batch, rng: jax.Array\n",
    ") -> Tuple[PyTree, Metrics]:\n",
    "    \"\"\"Classification loss function with cross-entropy.\"\"\"\n",
    "    logits = apply_fn({\"params\": params}, batch.inputs, train=True, rngs={\"dropout\": rng})\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch.labels)\n",
    "    correct_pred = jnp.equal(jnp.argmax(logits, axis=-1), batch.labels)\n",
    "    batch_size = batch.inputs.shape[0]\n",
    "    step_metrics = {\"loss\": (loss.sum(), batch_size), \"accuracy\": (correct_pred.sum(), batch_size)}\n",
    "    loss = loss.mean()\n",
    "    return loss, step_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4cfbfe8d-78d0-47bd-9482-0f42e0f1f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_gradients_loop(\n",
    "    state: TrainState,\n",
    "    batch: Batch,\n",
    "    rng: jax.random.PRNGKey,\n",
    "    num_minibatches: int,\n",
    "    loss_fn: Callable,\n",
    ") -> Tuple[PyTree, Metrics]:\n",
    "    \"\"\"Calculate gradients and metrics for a batch using gradient accumulation.\n",
    "\n",
    "    Args:\n",
    "        state: Current training state.\n",
    "        batch: Full training batch.\n",
    "        rng: Random number generator to use.\n",
    "        num_minibatches: Number of minibatches to split the batch into. Equal to the number of gradient accumulation steps.\n",
    "        loss_fn: Loss function to calculate gradients and metrics.\n",
    "\n",
    "    Returns:\n",
    "        Tuple with accumulated gradients and metrics over the minibatches.\n",
    "    \"\"\"\n",
    "    batch_size = batch.inputs.shape[0]\n",
    "    minibatch_size = batch_size // num_minibatches\n",
    "    rngs = jax.random.split(rng, num_minibatches)\n",
    "    # Define gradient function for single minibatch.\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    # Prepare loop variables.\n",
    "    grads = None\n",
    "    metrics = None\n",
    "    for minibatch_idx in range(num_minibatches):\n",
    "        with jax.named_scope(f\"minibatch_{minibatch_idx}\"):\n",
    "            # Split the batch into minibatches.\n",
    "            start = minibatch_idx * minibatch_size\n",
    "            end = start + minibatch_size\n",
    "            minibatch = jax.tree_map(lambda x: x[start:end], batch)\n",
    "            # Calculate gradients and metrics for the minibatch.\n",
    "            (_, step_metrics), step_grads = grad_fn(\n",
    "                state.params, state.apply_fn, minibatch, rngs[minibatch_idx]\n",
    "            )\n",
    "            # Accumulate gradients and metrics across minibatches.\n",
    "            if grads is None:\n",
    "                grads = step_grads\n",
    "                metrics = step_metrics\n",
    "            else:\n",
    "                grads = jax.tree_map(jnp.add, grads, step_grads)\n",
    "                metrics = jax.tree_map(jnp.add, metrics, step_metrics)\n",
    "    # Average gradients over minibatches.\n",
    "    grads = jax.tree_map(lambda g: g / num_minibatches, grads)\n",
    "    return grads, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "da004bae-1679-4231-a25b-0b9d97c4d7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_gradients_scan(\n",
    "    state: TrainState,\n",
    "    batch: Batch,\n",
    "    rng: jax.random.PRNGKey,\n",
    "    num_minibatches: int,\n",
    "    loss_fn: Callable,\n",
    ") -> Tuple[PyTree, Metrics]:\n",
    "    \"\"\"Calculate gradients and metrics for a batch using gradient accumulation.\n",
    "\n",
    "    In this version, we use `jax.lax.scan` to loop over the minibatches. This is more efficient in terms of compilation time.\n",
    "\n",
    "    Args:\n",
    "        state: Current training state.\n",
    "        batch: Full training batch.\n",
    "        rng: Random number generator to use.\n",
    "        num_minibatches: Number of minibatches to split the batch into. Equal to the number of gradient accumulation steps.\n",
    "        loss_fn: Loss function to calculate gradients and metrics.\n",
    "\n",
    "    Returns:\n",
    "        Tuple with accumulated gradients and metrics over the minibatches.\n",
    "    \"\"\"\n",
    "    batch_size = batch.inputs.shape[0]\n",
    "    minibatch_size = batch_size // num_minibatches\n",
    "    rngs = jax.random.split(rng, num_minibatches)\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "\n",
    "    def _minibatch_step(minibatch_idx: jax.Array | int) -> Tuple[PyTree, Metrics]:\n",
    "        \"\"\"Determine gradients and metrics for a single minibatch.\"\"\"\n",
    "        minibatch = jax.tree_map(\n",
    "            lambda x: jax.lax.dynamic_slice_in_dim(  # Slicing with variable index (jax.Array).\n",
    "                x, start_index=minibatch_idx * minibatch_size, slice_size=minibatch_size, axis=0\n",
    "            ),\n",
    "            batch,\n",
    "        )\n",
    "        (_, step_metrics), step_grads = grad_fn(\n",
    "            state.params, state.apply_fn, minibatch, rngs[minibatch_idx]\n",
    "        )\n",
    "        return step_grads, step_metrics\n",
    "\n",
    "    def _scan_step(\n",
    "        carry: Tuple[PyTree, Metrics], minibatch_idx: jax.Array | int\n",
    "    ) -> Tuple[Tuple[PyTree, Metrics], None]:\n",
    "        \"\"\"Scan step function for looping over minibatches.\"\"\"\n",
    "        step_grads, step_metrics = _minibatch_step(minibatch_idx)\n",
    "        carry = jax.tree_map(jnp.add, carry, (step_grads, step_metrics))\n",
    "        return carry, None\n",
    "\n",
    "    # Determine initial shapes for gradients and metrics.\n",
    "    grads_shapes, metrics_shape = jax.eval_shape(_minibatch_step, 0)\n",
    "    grads = jax.tree_map(lambda x: jnp.zeros(x.shape, x.dtype), grads_shapes)\n",
    "    metrics = jax.tree_map(lambda x: jnp.zeros(x.shape, x.dtype), metrics_shape)\n",
    "    # Loop over minibatches to determine gradients and metrics.\n",
    "    (grads, metrics), _ = jax.lax.scan(\n",
    "        _scan_step, init=(grads, metrics), xs=jnp.arange(num_minibatches), length=num_minibatches\n",
    "    )\n",
    "    # Average gradients over minibatches.\n",
    "    grads = jax.tree_map(lambda g: g / num_minibatches, grads)\n",
    "    return grads, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f44be969-f88f-464d-8535-4071159dc9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_gradients(*args, use_scan: bool = False, **kwargs) -> Tuple[PyTree, Metrics]:\n",
    "    if use_scan:\n",
    "        return accumulate_gradients_scan(*args, **kwargs)\n",
    "    else:\n",
    "        return accumulate_gradients_loop(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6e45bec8-5bed-4328-8070-554ecc1ba80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    state: TrainState,\n",
    "    metrics: Metrics | None,\n",
    "    batch: Batch,\n",
    "    num_minibatches: int,\n",
    ") -> Tuple[TrainState, Metrics]:\n",
    "    \"\"\"Training step function.\n",
    "\n",
    "    Executes a full training step with gradient accumulation.\n",
    "\n",
    "    Args:\n",
    "        state: Current training state.\n",
    "        metrics: Current metrics, accumulated from previous training steps.\n",
    "        batch: Training batch.\n",
    "        num_minibatches: Number of minibatches to split the batch into. Equal to the number of gradient accumulation steps.\n",
    "\n",
    "    Returns:\n",
    "        Tuple with updated training state (parameters, optimizer state, etc.) and metrics.\n",
    "    \"\"\"\n",
    "    # Split the random number generator for the current step.\n",
    "    rng, step_rng = jax.random.split(state.rng)\n",
    "    # Determine gradients and metrics for the full batch.\n",
    "    grads, step_metrics = accumulate_gradients(\n",
    "        state, batch, step_rng, num_minibatches, loss_fn=classification_loss_fn, use_scan=True\n",
    "    )\n",
    "    # Optimizer step.\n",
    "    new_state = state.apply_gradients(grads=grads, rng=rng)\n",
    "    # Accumulate metrics across training steps.\n",
    "    if metrics is None:\n",
    "        metrics = step_metrics\n",
    "    else:\n",
    "        metrics = jax.tree_map(jnp.add, metrics, step_metrics)\n",
    "    return new_state, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3e7665dd-3b3f-4827-9813-9382ce052a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "num_inputs = 128\n",
    "num_classes = 100\n",
    "rng_seed = 0\n",
    "\n",
    "rng = jax.random.PRNGKey(rng_seed)\n",
    "data_input_rng, data_label_rng, model_rng, state_rng = jax.random.split(rng, 4)\n",
    "batch = Batch(\n",
    "    inputs=jax.random.normal(data_input_rng, (batch_size, num_inputs)),\n",
    "    labels=jax.random.randint(data_label_rng, (batch_size,), 0, num_classes),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "88668549-b92b-498d-976a-f5339e0db113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero dropout for checking later equality between training with and without gradient accumulation.\n",
    "model = MLPClassifier(dtype=jnp.bfloat16, dropout_rate=0.0)\n",
    "params = model.init(model_rng, batch.inputs, train=False)[\"params\"]\n",
    "state = TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    tx=optax.adam(1e-3),\n",
    "    rng=state_rng,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "03ea3280-fc58-4181-b6c5-e289fb1d62e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric shapes:\n",
      "{'accuracy': (ShapeDtypeStruct(shape=(), dtype=int32),\n",
      "              ShapeDtypeStruct(shape=(), dtype=int32)),\n",
      " 'loss': (ShapeDtypeStruct(shape=(), dtype=float32),\n",
      "          ShapeDtypeStruct(shape=(), dtype=int32))}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_62240/281640800.py:29: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  minibatch = jax.tree_map(\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_62240/281640800.py:50: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  grads = jax.tree_map(lambda x: jnp.zeros(x.shape, x.dtype), grads_shapes)\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_62240/281640800.py:51: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  metrics = jax.tree_map(lambda x: jnp.zeros(x.shape, x.dtype), metrics_shape)\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_62240/281640800.py:29: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  minibatch = jax.tree_map(\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_62240/281640800.py:45: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  carry = jax.tree_map(jnp.add, carry, (step_grads, step_metrics))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_62240/281640800.py:57: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  grads = jax.tree_map(lambda g: g / num_minibatches, grads)\n"
     ]
    }
   ],
   "source": [
    "_, metric_shapes = jax.eval_shape(\n",
    "    functools.partial(train_step, num_minibatches=4),\n",
    "    state,\n",
    "    None,\n",
    "    batch,\n",
    ")\n",
    "print(\"Metric shapes:\")\n",
    "pprint(metric_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "08cc82a2-88ae-4d17-85da-078861f2ce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_jit = jax.jit(\n",
    "    train_step,\n",
    "    static_argnames=\"num_minibatches\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d9967d3a-5805-40c5-9aed-8f5ce40490f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_minibatches(\n",
    "    state: TrainState,\n",
    "    batch: Batch,\n",
    "    num_minibatches: int,\n",
    "    num_train_steps: int,\n",
    ") -> Tuple[TrainState, Metrics]:\n",
    "    \"\"\"Small helper function for training loop.\"\"\"\n",
    "    train_metrics = jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)\n",
    "    for _ in range(num_train_steps):\n",
    "        state, train_metrics = train_step_jit(state, train_metrics, batch, num_minibatches)\n",
    "    return state, train_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5a3965-1d2d-4947-9858-82ee4aa76f89",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## JAX Donation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d58da937-c1bc-49fc-80cd-17ad5acf22eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_donated = jax.jit(\n",
    "    train_step,\n",
    "    static_argnames=\"num_minibatches\",\n",
    "    donate_argnames=(\n",
    "        \"state\",\n",
    "        \"metrics\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52acdd0-93b6-49e1-a766-83ed4639abba",
   "metadata": {},
   "source": [
    "## Putting it Together - Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "05064610-c3a7-4e8a-8348-3764c7284a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/phlippe/uvadlc_notebooks/master/docs/tutorial_notebooks/scaling/JAX/single_gpu.py...\n",
      "Downloading https://raw.githubusercontent.com/phlippe/uvadlc_notebooks/master/docs/tutorial_notebooks/scaling/JAX/utils.py...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "# Github URL where python scripts are stored.\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/uvadlc_notebooks/master/docs/tutorial_notebooks/scaling/JAX/\"\n",
    "# Files to download.\n",
    "python_files = [\"single_gpu.py\", \"utils.py\"]\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in python_files:\n",
    "    if not os.path.isfile(file_name):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_name)\n",
    "        except HTTPError as e:\n",
    "            print(\n",
    "                \"Something went wrong. Please try to download the file directly from the GitHub repository, or contact the author with the full output including the following error:\\n\",\n",
    "                e,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "47c84987-4828-4170-a7ea-a2f1bbf67cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import set_XLA_flags_gpu\n",
    "set_XLA_flags_gpu()\n",
    "from single_gpu import Batch, TrainState, accumulate_gradients, print_metrics # Functions from above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a09ccd3-c68c-484f-9f72-84f5030d65fc",
   "metadata": {},
   "source": [
    "In general, \n",
    "- `self.config.dtype` allows us to use mixed precision training\n",
    "    - We keep softmax calculations in float32\n",
    "- `self.confit.remat` contains the blocks we want to explicitly remat\n",
    "- `scan` prevents recompilation of repeated blocks\n",
    "- `accumulate_gradients` defined earlier allows us to accumulate gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ce035503-280e-456a-8b51-40f874600fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    config: ConfigDict\n",
    "    train: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        input_features = x.shape[-1]\n",
    "        x = nn.LayerNorm(dtype=self.config.dtype, name=\"pre_norm\")(x)\n",
    "        x = nn.Dense(\n",
    "            features=self.config.mlp_expansion * input_features,\n",
    "            dtype=self.config.dtype,\n",
    "            name=\"input_layer\",\n",
    "        )(x)\n",
    "        x = nn.gelu(x)\n",
    "        x = nn.Dense(\n",
    "            features=input_features,\n",
    "            dtype=self.config.dtype,\n",
    "            name=\"output_layer\",\n",
    "        )(x)\n",
    "        x = nn.Dropout(rate=self.config.dropout_rate, deterministic=not self.train)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9b49aa10-4986-4a88-ae12-dec3939bab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attention(\n",
    "    query: jax.Array,\n",
    "    key: jax.Array,\n",
    "    value: jax.Array,\n",
    "    mask: jax.Array | None,\n",
    "    softmax_dtype: jnp.dtype = jnp.float32,\n",
    "):\n",
    "    \"\"\"Dot-product attention.\n",
    "\n",
    "    Follows the setup of https://flax.readthedocs.io/en/latest/api_reference/flax.linen/layers.html#flax.linen.dot_product_attention,\n",
    "    but supports switch to float32 for numerical stability during softmax.\n",
    "\n",
    "    Args:\n",
    "        query: The query array, shape [..., num queries, num heads, hidden size].\n",
    "        key: The key array, shape [..., num keys, num heads, hidden size].\n",
    "        value: The value array, shape [..., num keys, num heads, hidden size].\n",
    "        mask: The boolean mask array (0 for masked values, 1 for non-masked). If None, no masking is applied.\n",
    "        softmax_dtype: The dtype to use for the softmax and dot-product operation.\n",
    "\n",
    "    Returns:\n",
    "        The attention output array, shape [..., num queries, num heads, hidden size].\n",
    "    \"\"\"\n",
    "    num_features = query.shape[-1]\n",
    "    dtype = query.dtype\n",
    "    scale = num_features**-0.5\n",
    "    query = query * scale\n",
    "    # Switch dtype right before the dot-product for numerical stability.\n",
    "    query = query.astype(softmax_dtype)\n",
    "    key = key.astype(softmax_dtype)\n",
    "    weights = jnp.einsum(\"...qhd,...khd->...hqk\", query, key)\n",
    "    if mask is not None:\n",
    "        weights = jnp.where(mask, weights, jnp.finfo(softmax_dtype).min)\n",
    "    weights = nn.softmax(weights, axis=-1)\n",
    "    # After softmax, switch back to the original dtype\n",
    "    weights = weights.astype(dtype)\n",
    "    new_vals = jnp.einsum(\"...hqk,...khd->...qhd\", weights, value)\n",
    "    new_vals = new_vals.astype(dtype)\n",
    "    return new_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ebff5191-e4ba-428e-9f98-8ffc0b3e1f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    config: ConfigDict\n",
    "    mask: jax.Array | None\n",
    "    train: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        input_features = x.shape[-1]\n",
    "        x = nn.LayerNorm(dtype=self.config.dtype, name=\"pre_norm\")(x)\n",
    "        qkv = nn.DenseGeneral(\n",
    "            features=(self.config.num_heads, self.config.head_dim * 3),\n",
    "            dtype=self.config.dtype,\n",
    "            name=\"qkv\",\n",
    "        )(x)\n",
    "        q, k, v = jnp.split(qkv, 3, axis=-1)\n",
    "        x = dot_product_attention(q, k, v, mask=self.mask, softmax_dtype=self.config.softmax_dtype)\n",
    "        x = nn.DenseGeneral(\n",
    "            features=input_features,\n",
    "            axis=(-2, -1),\n",
    "            dtype=self.config.dtype,\n",
    "            name=\"output_layer\",\n",
    "        )(x)\n",
    "        x = nn.Dropout(rate=self.config.dropout_rate, deterministic=not self.train)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "adcd95ff-0f00-454c-bd0b-28e92c33c7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    config: ConfigDict\n",
    "    mask: jax.Array | None\n",
    "    train: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        # MLP block\n",
    "        mlp = MLPBlock\n",
    "        if \"MLP\" in self.config.remat:\n",
    "            mlp = nn.remat(mlp, prevent_cse=False)\n",
    "        x = x + mlp(config=self.config, train=self.train, name=\"mlp\")(x)\n",
    "        # Attention block\n",
    "        attn = AttentionBlock\n",
    "        if \"Attn\" in self.config.remat:\n",
    "            attn = nn.remat(attn, prevent_cse=False)\n",
    "        x = x + attn(config=self.config, mask=self.mask, train=self.train, name=\"attn\")(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "65264301-15c8-433f-a9d5-0682597f2074",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    config: ConfigDict\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self, x: jax.Array, mask: jax.Array | None = None, train: bool = True\n",
    "    ) -> jax.Array:\n",
    "        if mask is None and self.config.causal_mask:\n",
    "            mask = nn.make_causal_mask(x, dtype=jnp.bool_)\n",
    "        # Input layer.\n",
    "        x = nn.Embed(\n",
    "            num_embeddings=self.config.vocab_size,\n",
    "            features=self.config.hidden_size,\n",
    "            dtype=self.config.dtype,\n",
    "            name=\"embed\",\n",
    "        )(x)\n",
    "        pos_emb = self.param(\n",
    "            \"pos_emb\",\n",
    "            nn.initializers.normal(stddev=0.02),\n",
    "            (self.config.max_seq_len, self.config.hidden_size),\n",
    "        )\n",
    "        pos_emb = pos_emb.astype(self.config.dtype)\n",
    "        x = x + pos_emb[None, : x.shape[1]]\n",
    "        # Transformer blocks.\n",
    "        block_fn = functools.partial(TransformerBlock, config=self.config, mask=mask, train=train)\n",
    "        if \"Block\" in self.config.remat:\n",
    "            block_fn = nn.remat(block_fn, prevent_cse=False)\n",
    "        if self.config.scan_layers:\n",
    "            block = block_fn(name=\"block\")\n",
    "            x, _ = nn.scan(\n",
    "                lambda module, carry, _: (module(carry), None),\n",
    "                variable_axes={\"params\": 0},\n",
    "                split_rngs={\"params\": True, \"dropout\": True},\n",
    "                length=self.config.num_layers,\n",
    "            )(block, x, ())\n",
    "        else:\n",
    "            for l_idx in range(self.config.num_layers):\n",
    "                x = block_fn(name=f\"block_{l_idx}\")(x)\n",
    "        # Output layer.\n",
    "        x = nn.LayerNorm(dtype=self.config.dtype, name=\"post_norm\")(x)\n",
    "        x = nn.Dense(\n",
    "            features=self.config.num_outputs,\n",
    "            dtype=self.config.dtype,\n",
    "            name=\"output_layer\",\n",
    "        )(x)\n",
    "        x = x.astype(jnp.float32)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6665705e-15e3-4207-b142-99d552139063",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = ConfigDict(\n",
    "    dict(\n",
    "        batch_size=64,\n",
    "        seq_len=512,\n",
    "        vocab_size=2048,\n",
    "    )\n",
    ")\n",
    "model_config = ConfigDict(\n",
    "    dict(\n",
    "        hidden_size=1024,\n",
    "        dropout_rate=0.1,\n",
    "        mlp_expansion=4,\n",
    "        num_layers=12,\n",
    "        head_dim=128,\n",
    "        causal_mask=True,\n",
    "        max_seq_len=data_config.seq_len,\n",
    "        vocab_size=data_config.vocab_size,\n",
    "        num_outputs=data_config.vocab_size,\n",
    "        dtype=jnp.bfloat16,\n",
    "        softmax_dtype=jnp.float32,\n",
    "        scan_layers=True,\n",
    "        remat=(\"MLP\", \"Attn\"),\n",
    "    )\n",
    ")\n",
    "model_config.num_heads = model_config.hidden_size // model_config.head_dim\n",
    "optimizer_config = ConfigDict(\n",
    "    dict(\n",
    "        learning_rate=4e-4,\n",
    "        num_minibatches=4,\n",
    "    )\n",
    ")\n",
    "config = ConfigDict(\n",
    "    dict(\n",
    "        model=model_config,\n",
    "        optimizer=optimizer_config,\n",
    "        data=data_config,\n",
    "        seed=42,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bae6feae-6c02-4156-8a1e-6cc984cbf82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(config=config.model)\n",
    "optimizer = optax.adam(\n",
    "    learning_rate=optax.warmup_exponential_decay_schedule(\n",
    "        init_value=0,\n",
    "        peak_value=config.optimizer.learning_rate,\n",
    "        warmup_steps=10,\n",
    "        transition_steps=1,\n",
    "        decay_rate=0.99,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "204108f0-ad12-4154-855c-0b4ab9b7a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = jax.random.randint(\n",
    "    jax.random.PRNGKey(0),\n",
    "    (config.data.batch_size, config.data.seq_len),\n",
    "    1,\n",
    "    config.data.vocab_size,\n",
    ")\n",
    "batch_transformer = Batch(\n",
    "    inputs=jnp.pad(tokens[:, :-1], ((0, 0), (1, 0)), constant_values=0),\n",
    "    labels=tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e37fa023-5f10-448b-907e-1131866b3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rng, state_rng = jax.random.split(jax.random.PRNGKey(config.seed))\n",
    "params = model.init(\n",
    "    model_rng,\n",
    "    batch_transformer.inputs[: config.data.batch_size // config.optimizer.num_minibatches],\n",
    "    train=False,\n",
    ")[\"params\"]\n",
    "state = TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    tx=optimizer,\n",
    "    rng=state_rng,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ef44bd87-c836-4c72-bfb6-5e2449317e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_token_pred_loss(\n",
    "    params: PyTree, apply_fn: Any, batch: Batch, rng: jax.Array\n",
    ") -> Tuple[PyTree, Metrics]:\n",
    "    \"\"\"Next token prediction loss function.\"\"\"\n",
    "    logits = apply_fn({\"params\": params}, batch.inputs, train=True, rngs={\"dropout\": rng})\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch.labels)\n",
    "    correct_pred = jnp.equal(jnp.argmax(logits, axis=-1), batch.labels)\n",
    "    batch_size = np.prod(batch.labels.shape)\n",
    "    step_metrics = {\"loss\": (loss.sum(), batch_size), \"accuracy\": (correct_pred.sum(), batch_size)}\n",
    "    loss = loss.mean()\n",
    "    return loss, step_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d8b30c76-8f90-470c-afb2-6a917582a5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(\n",
    "    jax.jit,\n",
    "    donate_argnames=(\n",
    "        \"state\",\n",
    "        \"metrics\",\n",
    "    ),\n",
    ")\n",
    "def train_step_transformer(\n",
    "    state: TrainState,\n",
    "    metrics: Metrics | None,\n",
    "    batch: Batch,\n",
    ") -> Tuple[TrainState, Metrics]:\n",
    "    \"\"\"Training step function.\n",
    "\n",
    "    Executes a full training step with gradient accumulation for the next-token prediction task.\n",
    "\n",
    "    Args:\n",
    "        state: Current training state.\n",
    "        metrics: Current metrics, accumulated from previous training steps.\n",
    "        batch: Training batch.\n",
    "\n",
    "    Returns:\n",
    "        Tuple with updated training state (parameters, optimizer state, etc.) and metrics.\n",
    "    \"\"\"\n",
    "    # Split the random number generator for the current step.\n",
    "    rng, step_rng = jax.random.split(state.rng)\n",
    "    # Determine gradients and metrics for the full batch.\n",
    "    grads, step_metrics = accumulate_gradients( # This does the accumulation of gradients that was defined earlier\n",
    "        state,\n",
    "        batch,\n",
    "        step_rng,\n",
    "        config.optimizer.num_minibatches,\n",
    "        loss_fn=next_token_pred_loss,\n",
    "        use_scan=True,\n",
    "    )\n",
    "    # Optimizer step.\n",
    "    new_state = state.apply_gradients(grads=grads, rng=rng)\n",
    "    # Accumulate metrics across training steps.\n",
    "    if metrics is None:\n",
    "        metrics = step_metrics\n",
    "    else:\n",
    "        metrics = jax.tree_map(jnp.add, metrics, step_metrics)\n",
    "    return new_state, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "43b88ce8-7118-4e9a-9d9b-8930c113983f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_62240/281640800.py:29: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  minibatch = jax.tree_map(\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_62240/281640800.py:50: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  grads = jax.tree_map(lambda x: jnp.zeros(x.shape, x.dtype), grads_shapes)\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_62240/281640800.py:51: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  metrics = jax.tree_map(lambda x: jnp.zeros(x.shape, x.dtype), metrics_shape)\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_62240/281640800.py:29: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  minibatch = jax.tree_map(\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_62240/281640800.py:45: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  carry = jax.tree_map(jnp.add, carry, (step_grads, step_metrics))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_62240/281640800.py:57: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  grads = jax.tree_map(lambda g: g / num_minibatches, grads)\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_62240/2032904830.py:7: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  metrics = jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)\n"
     ]
    }
   ],
   "source": [
    "_, metric_shapes = jax.eval_shape(\n",
    "    train_step_transformer,\n",
    "    state,\n",
    "    None,\n",
    "    batch_transformer,\n",
    ")\n",
    "metrics = jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "50164d57-8c9a-4be3-a9fd-015498929c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(range(4)):\n",
    "    state, metrics = train_step_transformer(state, metrics, batch_transformer)\n",
    "final_metrics = jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)\n",
    "state, final_metrics = train_step_transformer(state, final_metrics, batch_transformer)\n",
    "print_metrics(final_metrics, \"Final metrics - Transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c7f61e23-14f6-4a58-a403-5ff1b4f27599",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.profiler.start_trace(\"traces/\")\n",
    "for i in range(3):\n",
    "    with jax.profiler.StepTraceAnnotation(\"train_step\", step_num=i + 1):\n",
    "        state, metrics = train_step_transformer(state, metrics, batch_transformer)\n",
    "metrics[\"loss\"][0].block_until_ready()\n",
    "jax.profiler.stop_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff11c522-a4bb-42c2-bbfa-16ca855794ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir traces/single_gpu_transformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
