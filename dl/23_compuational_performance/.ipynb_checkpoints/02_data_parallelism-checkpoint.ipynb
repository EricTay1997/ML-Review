{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04f00620-3f98-4b98-97ae-d891bd4d38ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Code adapted from \n",
    "- https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/scaling/JAX/data_parallel_intro.html\n",
    "- http://d2l.ai/chapter_computational-performance/multiple-gpus-concise.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b13984da-d74f-4fca-94c7-6bf34865eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set this to True to run the model on CPU only.\n",
    "USE_CPU_ONLY = True\n",
    "\n",
    "flags = os.environ.get(\"XLA_FLAGS\", \"\")\n",
    "if USE_CPU_ONLY:\n",
    "    flags += \" --xla_force_host_platform_device_count=8\"  # Simulate 8 devices\n",
    "    # Enforce CPU-only execution\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "else:\n",
    "    # GPU flags\n",
    "    flags += (\n",
    "        \"--xla_gpu_enable_triton_softmax_fusion=true \"\n",
    "        \"--xla_gpu_triton_gemm_any=false \"\n",
    "        \"--xla_gpu_enable_async_collectives=true \"\n",
    "        \"--xla_gpu_enable_latency_hiding_scheduler=true \"\n",
    "        \"--xla_gpu_enable_highest_priority_async_stream=true \"\n",
    "    )\n",
    "os.environ[\"XLA_FLAGS\"] = flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2efb2c09-83f1-4278-926b-7f18408053e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax.experimental.shard_map import shard_map\n",
    "from jax.sharding import Mesh, NamedSharding\n",
    "from jax.sharding import PartitionSpec as P\n",
    "\n",
    "PyTree = Any\n",
    "Metrics = Dict[str, Tuple[jax.Array, ...]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90b7e1aa-ecdf-43da-a784-e55d17efc3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  CPU 0  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">  CPU 1  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">  CPU 2  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">  CPU 3  </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">  CPU 4  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">  CPU 5  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">  CPU 6  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">  CPU 7  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">         </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m  \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107mCPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82mCPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mCPU 3\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148m  \u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148mCPU 4\u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148m  \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m  \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207mCPU 5\u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m  \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m  \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148mCPU 6\u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49mCPU 7\u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m         \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m         \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m         \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = jnp.arange(8)\n",
    "mesh = Mesh(np.array(jax.devices()), (\"i\",))\n",
    "sharding = NamedSharding(\n",
    "    mesh,\n",
    "    P(\"i\"),\n",
    ")\n",
    "a_sharded = jax.device_put(a, sharding)\n",
    "jax.debug.visualize_array_sharding(a_sharded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea15caf6-546e-4cd1-9e5e-751dfe6f7360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  CPU 0  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">  CPU 1  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">  CPU 2  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">  CPU 3  </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">  CPU 4  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">  CPU 5  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">  CPU 6  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">  CPU 7  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">         </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m  \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107mCPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82mCPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mCPU 3\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148m  \u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148mCPU 4\u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148m  \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m  \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207mCPU 5\u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m  \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m  \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148mCPU 6\u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49mCPU 7\u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m         \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m         \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m         \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = nn.tanh(a_sharded)\n",
    "jax.debug.visualize_array_sharding(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95a80127-e9d4-408b-8913-72afd5eb17a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape (192, 128)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  CPU 0  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">  CPU 1  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">  CPU 2  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">  CPU 3  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">         </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">  CPU 4  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">  CPU 5  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">  CPU 6  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">  CPU 7  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">         </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m  \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107mCPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;140;162;82m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;140;162;82m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82mCPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mCPU 3\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;140;162;82m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m         \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m  \u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148mCPU 4\u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148m  \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m  \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207mCPU 5\u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m  \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;165;81;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;165;81;148m  \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148mCPU 6\u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49mCPU 7\u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;165;81;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m         \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can shard the batch dimension of the input x over the i axis, and the output dimension of the weight matrix w and bias b over the j axis\n",
    "mesh = Mesh(np.array(jax.devices()).reshape(4, 2), (\"i\", \"j\"))\n",
    "batch_size = 192\n",
    "input_dim = 64\n",
    "output_dim = 128\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (batch_size, input_dim))\n",
    "w = jax.random.normal(jax.random.PRNGKey(1), (input_dim, output_dim))\n",
    "b = jax.random.normal(jax.random.PRNGKey(2), (output_dim,))\n",
    "x_sharded = jax.device_put(x, NamedSharding(mesh, P(\"i\", None)))\n",
    "w_sharded = jax.device_put(w, NamedSharding(mesh, P(None, \"j\")))\n",
    "b_sharded = jax.device_put(b, NamedSharding(mesh, P(\"j\")))\n",
    "out = jnp.dot(x_sharded, w_sharded) + b_sharded\n",
    "print(\"Output shape\", out.shape)\n",
    "jax.debug.visualize_array_sharding(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0be785d5-29c1-4203-b30a-554f764927a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local x shape (48, 64)\n",
      "Local w shape (64, 64)\n",
      "Local b shape (64,)\n",
      "Output shape (192, 128)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  CPU 0  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">  CPU 1  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">  CPU 2  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">  CPU 3  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">         </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">  CPU 4  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">  CPU 5  </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">  CPU 6  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">  CPU 7  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">         </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m  \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107mCPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;140;162;82m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;140;162;82m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82mCPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mCPU 3\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;140;162;82m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m         \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m  \u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148mCPU 4\u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148m  \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m  \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207mCPU 5\u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m  \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;165;81;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;165;81;148m  \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148mCPU 6\u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m  \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49mCPU 7\u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;165;81;148m         \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m         \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The transformation shard_map has been developed as an alternative to jax.pmap, which gives us more explicit control over the parallelization and communication\n",
    "\n",
    "def matmul_fn(x: jax.Array, w: jax.Array, b: jax.Array) -> jax.Array:\n",
    "    print(\"Local x shape\", x.shape)\n",
    "    print(\"Local w shape\", w.shape)\n",
    "    print(\"Local b shape\", b.shape)\n",
    "    return jnp.dot(x, w) + b\n",
    "\n",
    "matmul_sharded = shard_map(\n",
    "    matmul_fn, mesh, in_specs=(P(\"i\", None), P(None, \"j\"), P(\"j\")), out_specs=P(\"i\", \"j\")\n",
    ")\n",
    "\n",
    "y = matmul_sharded(x_sharded, w_sharded, b_sharded)\n",
    "print(\"Output shape\", y.shape)\n",
    "jax.debug.visualize_array_sharding(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7d01ca-8604-47c4-848b-bcc09d91ba1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Parallelizing operations, gathering and scattering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f15f389a-d06b-4497-8314-496bb0d2bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(shard_map, mesh=mesh, in_specs=P(\"i\", \"j\"), out_specs=P(\"i\", \"j\"))\n",
    "def parallel_normalize(x: jax.Array) -> jax.Array:\n",
    "    mean = jax.lax.pmean(x, axis_name=\"j\")\n",
    "    std = jax.lax.pmean((x - mean) ** 2, axis_name=\"j\") ** 0.5\n",
    "    return (x - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3f2ce13-d1cb-4096-a1fc-1138774f0d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean -4.149236e-08\n",
      "Std 1.0\n"
     ]
    }
   ],
   "source": [
    "out = parallel_normalize(x)\n",
    "out = jax.device_get(out)\n",
    "print(\"Mean\", out.mean())\n",
    "print(\"Std\", out.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b16efb3-5e3c-489d-baa5-7c31084cb6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original w shape (16, 128)\n",
      "Gathered w shape (64, 128)\n"
     ]
    }
   ],
   "source": [
    "@functools.partial(\n",
    "    shard_map, mesh=mesh, in_specs=(P(\"i\", None), P(\"i\", None)), out_specs=P(\"i\", None)\n",
    ")\n",
    "def matmul_with_weight_gather(x: jax.Array, w: jax.Array) -> jax.Array:\n",
    "    print(\"Original w shape\", w.shape)\n",
    "    w_gathered = jax.lax.all_gather(w, axis_name=\"i\", axis=0, tiled=True)\n",
    "    print(\"Gathered w shape\", w_gathered.shape)\n",
    "    y = jnp.dot(x, w_gathered)\n",
    "    return y\n",
    "\n",
    "\n",
    "out = matmul_with_weight_gather(x, w)\n",
    "out = jax.device_get(out)\n",
    "np.testing.assert_array_equal(out, jnp.dot(x, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83cd638b-7463-4eaa-81be-0c2912ce0861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output [22 20 12 17]\n"
     ]
    }
   ],
   "source": [
    "@functools.partial(shard_map, mesh=mesh, in_specs=P(\"i\", None), out_specs=P(\"i\", None))\n",
    "def scatter_example(x: jax.Array) -> jax.Array:\n",
    "    x_scatter = jax.lax.psum_scatter(x, axis_name=\"i\", scatter_dimension=1)\n",
    "    return x_scatter\n",
    "\n",
    "\n",
    "x_exmp = np.array(\n",
    "    [\n",
    "        [3, 1, 4, 1],\n",
    "        [5, 9, 2, 6],\n",
    "        [5, 3, 5, 8],\n",
    "        [9, 7, 1, 2],\n",
    "    ]\n",
    ")\n",
    "out = scatter_example(x_exmp)\n",
    "print(\"Output\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7dc7466-e3d0-47de-9078-3dfdf5ab9cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output [3 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "@functools.partial(shard_map, mesh=mesh, in_specs=P(\"i\"), out_specs=P(\"i\"))\n",
    "def ppermute_example(x: jax.Array) -> jax.Array:\n",
    "    axis_size = mesh.shape[\"i\"]\n",
    "    x_perm = jax.lax.ppermute(\n",
    "        x, axis_name=\"i\", perm=[(i, (i + 1) % axis_size) for i in range(axis_size)]\n",
    "    )\n",
    "    return x_perm\n",
    "\n",
    "\n",
    "x_exmp = np.arange(4)\n",
    "out = ppermute_example(x_exmp)\n",
    "print(\"Output\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c5ad4c-2427-4910-bef3-ecaa80a89443",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Axis Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70c09c14-9018-4ae8-b072-4ca349817a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0: i-axis=0, j-axis=0\n",
      "Device 1: i-axis=0, j-axis=1\n",
      "Device 2: i-axis=1, j-axis=0\n",
      "Device 3: i-axis=1, j-axis=1\n",
      "Device 4: i-axis=2, j-axis=0\n",
      "Device 5: i-axis=2, j-axis=1\n",
      "Device 6: i-axis=3, j-axis=0\n",
      "Device 7: i-axis=3, j-axis=1\n"
     ]
    }
   ],
   "source": [
    "axis_idx_fn = jax.jit(\n",
    "    shard_map(\n",
    "        lambda: jnp.stack(\n",
    "            [\n",
    "                jax.lax.axis_index(\"i\"),  # Device index in mesh along the \"i\" axis\n",
    "                jax.lax.axis_index(\"j\"),  # Device index in mesh along the \"j\" axis\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )[None],\n",
    "        mesh,\n",
    "        in_specs=P(),\n",
    "        out_specs=P(\n",
    "            (\"i\", \"j\"),\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "out = axis_idx_fn()\n",
    "out = jax.device_get(out)\n",
    "for i in range(out.shape[0]):\n",
    "    print(f\"Device {i}: i-axis={out[i, 0]}, j-axis={out[i, 1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05573fd0-f5b7-4cf7-a188-b1add87c721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_rng_over_axis(rng: jax.random.PRNGKey, axis_name: str) -> jax.random.PRNGKey:\n",
    "    \"\"\"Folds the random number generator over the given axis.\n",
    "\n",
    "    This is useful for generating a different random number for each device\n",
    "    across a certain axis (e.g. the model axis).\n",
    "\n",
    "    Args:\n",
    "        rng: The random number generator.\n",
    "        axis_name: The axis name to fold the random number generator over.\n",
    "\n",
    "    Returns:\n",
    "        A new random number generator, different for each device index along the axis.\n",
    "    \"\"\"\n",
    "    axis_index = jax.lax.axis_index(axis_name)\n",
    "    return jax.random.fold_in(rng, axis_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2135859-817e-4c2d-bde3-f87d4a43fc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0: RNG=[1797259609 2579123966]\n",
      "Device 1: RNG=[1797259609 2579123966]\n",
      "Device 2: RNG=[ 928981903 3453687069]\n",
      "Device 3: RNG=[ 928981903 3453687069]\n",
      "Device 4: RNG=[4146024105 2718843009]\n",
      "Device 5: RNG=[4146024105 2718843009]\n",
      "Device 6: RNG=[2467461003 3840466878]\n",
      "Device 7: RNG=[2467461003 3840466878]\n"
     ]
    }
   ],
   "source": [
    "fold_fn = jax.jit(\n",
    "    shard_map(\n",
    "        functools.partial(fold_rng_over_axis, axis_name=\"i\"),\n",
    "        mesh,\n",
    "        in_specs=P(),\n",
    "        out_specs=P(\n",
    "            (\"i\", \"j\"),\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "rng = jax.random.PRNGKey(0)\n",
    "out = fold_fn(rng)\n",
    "out = jax.device_get(out)\n",
    "for i in range(out.shape[0] // 2):\n",
    "    print(f\"Device {i}: RNG={out[2*i:2*i+2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6693143-23de-4704-b9f5-faae187b1922",
   "metadata": {},
   "source": [
    "# Data Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0997393b-d3fe-4de9-b6c7-aea212c1ea46",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b84ed8ce-90cb-4f44-b7c2-d71ebeb5b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "# Github URL where python scripts are stored.\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/uvadlc_notebooks/master/docs/tutorial_notebooks/scaling/JAX/\"\n",
    "# Files to download.\n",
    "python_files = [\"single_gpu.py\", \"utils.py\"]\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in python_files:\n",
    "    if not os.path.isfile(file_name):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_name)\n",
    "        except HTTPError as e:\n",
    "            print(\n",
    "                \"Something went wrong. Please try to download the file directly from the GitHub repository, or contact the author with the full output including the following error:\\n\",\n",
    "                e,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53341944-d0db-448e-8f86-bedb717e0b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import simulate_CPU_devices\n",
    "\n",
    "simulate_CPU_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "24343186-76ef-42da-ae49-cee09e2f5632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from pprint import pprint\n",
    "from typing import Any, Callable, Dict, Sequence, Tuple\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from absl import logging\n",
    "from jax import lax\n",
    "from jax.experimental.shard_map import shard_map\n",
    "from jax.sharding import Mesh\n",
    "from jax.sharding import PartitionSpec as P\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "PyTree = Any\n",
    "Metrics = Dict[str, Tuple[jax.Array, ...]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24886e79-a35f-4a56-b149-09a65ee46872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from single_gpu import Batch, TrainState, accumulate_gradients, print_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f95afd74-3038-466b-88e5-54395f457633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_rng_over_axis(rng: jax.random.PRNGKey, axis_name: str) -> jax.random.PRNGKey:\n",
    "    \"\"\"Folds the random number generator over the given axis.\n",
    "\n",
    "    This is useful for generating a different random number for each device\n",
    "    across a certain axis (e.g. the model axis).\n",
    "\n",
    "    Args:\n",
    "        rng: The random number generator.\n",
    "        axis_name: The axis name to fold the random number generator over.\n",
    "\n",
    "    Returns:\n",
    "        A new random number generator, different for each device index along the axis.\n",
    "    \"\"\"\n",
    "    axis_index = jax.lax.axis_index(axis_name)\n",
    "    return jax.random.fold_in(rng, axis_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47978e8-d8be-4446-93c3-a63a8f2efeff",
   "metadata": {},
   "source": [
    "## DP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d12f5f-f99a-4da6-9a51-2e8a328fb1e0",
   "metadata": {},
   "source": [
    "Non-JAX: We essentially just call `nn.DataParallel` on the net. This chunks the data and sums up gradients for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e87edb-9a3e-4eb0-a695-9e35633d5617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def train(net, num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n",
    "    def init_weights(module):\n",
    "        if type(module) in [nn.Linear, nn.Conv2d]:\n",
    "            nn.init.normal_(module.weight, std=0.01)\n",
    "    net.apply(init_weights)\n",
    "    # Set the model on multiple GPUs\n",
    "    net = nn.DataParallel(net, device_ids=devices)\n",
    "    trainer = torch.optim.SGD(net.parameters(), lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    timer, num_epochs = d2l.Timer(), 10\n",
    "    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        timer.start()\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(devices[0]), y.to(devices[0])\n",
    "            l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "        timer.stop()\n",
    "        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(net, test_iter),))\n",
    "    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '\n",
    "          f'on {str(devices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f780c0-7dcf-491c-b2bc-f2922a3429e8",
   "metadata": {},
   "source": [
    "JAX Tldr:\n",
    "- `shard_map` allows us to write the model code as though it operates on a single device, with the following exceptions:\n",
    "    - Wrap the initialization and training step function with `shard_map`\n",
    "    - Split the RNG key across devices (for Dropout)\n",
    "- We use `jax.lax.pmean` and `jax.lax.psum` to communicate gradients and loss across devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c4a3a3d4-149a-4c72-b06f-1b0a01f3ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPClassifier(nn.Module):\n",
    "    config: ConfigDict\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jax.Array, train: bool) -> jax.Array:\n",
    "        x = nn.Dense(\n",
    "            features=self.config.hidden_size,\n",
    "            dtype=self.config.dtype,\n",
    "            name=\"input_dense\",\n",
    "        )(x)\n",
    "        x = nn.silu(x)\n",
    "        x = nn.Dropout(rate=self.config.dropout_rate, deterministic=not train)(x)\n",
    "        x = nn.Dense(\n",
    "            features=self.config.num_classes,\n",
    "            dtype=self.config.dtype,\n",
    "            name=\"output_dense\",\n",
    "        )(x)\n",
    "        x = x.astype(jnp.float32)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "941fea13-e141-477c-973e-bcf7d295ed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = ConfigDict(\n",
    "    dict(\n",
    "        batch_size=128,\n",
    "        num_classes=10,\n",
    "        input_size=784,\n",
    "    )\n",
    ")\n",
    "model_config = ConfigDict(\n",
    "    dict(\n",
    "        hidden_size=512,\n",
    "        dropout_rate=0.1,\n",
    "        dtype=jnp.bfloat16,\n",
    "        num_classes=data_config.num_classes,\n",
    "        data_axis_name=\"data\",\n",
    "    )\n",
    ")\n",
    "optimizer_config = ConfigDict(\n",
    "    dict(\n",
    "        learning_rate=1e-3,\n",
    "        num_minibatches=4,\n",
    "    )\n",
    ")\n",
    "config = ConfigDict(\n",
    "    dict(\n",
    "        model=model_config,\n",
    "        optimizer=optimizer_config,\n",
    "        data=data_config,\n",
    "        data_axis_name=model_config.data_axis_name,\n",
    "        seed=42,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fee9232b-b304-4495-a60c-da8af3cf1033",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dp = DPClassifier(config=config.model)\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=config.optimizer.learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a2556c84-6717-4246-acc2-4b9bc3e0bdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(config.seed)\n",
    "model_init_rng, data_inputs_rng, data_labels_rng = jax.random.split(rng, 3)\n",
    "batch = Batch(\n",
    "    inputs=jax.random.normal(data_inputs_rng, (config.data.batch_size, config.data.input_size)),\n",
    "    labels=jax.random.randint(\n",
    "        data_labels_rng, (config.data.batch_size,), 0, config.data.num_classes\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c4e19b0f-d27d-4e47-8f6c-04b10bd1d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dp(rng: jax.random.PRNGKey, x: jax.Array, model: nn.Module) -> TrainState:\n",
    "    init_rng, rng = jax.random.split(rng)\n",
    "    variables = model.init({\"params\": init_rng}, x, train=False)\n",
    "    params = variables.pop(\"params\")\n",
    "    state = TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        tx=optimizer,\n",
    "        rng=rng,\n",
    "    )\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4db18051-8936-4385-a5e0-2a6d7de6a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_array = np.array(jax.devices())\n",
    "mesh = Mesh(device_array, (config.data_axis_name,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "70ee2977-2ade-4df9-a178-075375ca8004",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_dp_fn = jax.jit(\n",
    "    shard_map(\n",
    "        functools.partial(init_dp, model=model_dp),\n",
    "        mesh,\n",
    "        in_specs=(P(), P(config.data_axis_name)),\n",
    "        out_specs=P(),\n",
    "        check_rep=False,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ba20688f-9b89-4dab-95b6-e94e02ff6ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP Parameters\n",
      "{'input_dense': {'bias': ((512,),\n",
      "                          NamedSharding(mesh=Mesh('data': 8), spec=PartitionSpec())),\n",
      "                 'kernel': ((784, 512),\n",
      "                            NamedSharding(mesh=Mesh('data': 8), spec=PartitionSpec()))},\n",
      " 'output_dense': {'bias': ((10,),\n",
      "                           NamedSharding(mesh=Mesh('data': 8), spec=PartitionSpec())),\n",
      "                  'kernel': ((512, 10),\n",
      "                             NamedSharding(mesh=Mesh('data': 8), spec=PartitionSpec()))}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_64965/1564160775.py:3: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  pprint(jax.tree_map(lambda x: (x.shape, x.sharding), state_dp.params))\n"
     ]
    }
   ],
   "source": [
    "state_dp = init_dp_fn(model_init_rng, batch.inputs)\n",
    "print(\"DP Parameters\")\n",
    "pprint(jax.tree_map(lambda x: (x.shape, x.sharding), state_dp.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "72027574-17e7-4fd6-b775-a0aa5bce6b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(\n",
    "    params: PyTree, apply_fn: Any, batch: Batch, rng: jax.Array\n",
    ") -> Tuple[jax.Array, Dict[str, Any]]:\n",
    "    # Since dropout masks vary across the batch dimension, we want each device to generate a\n",
    "    # different mask. We can achieve this by folding the rng over the data axis, so that each\n",
    "    # device gets a different rng and thus mask.\n",
    "    dropout_rng = fold_rng_over_axis(rng, config.data_axis_name)\n",
    "    # Remaining computation is the same as before for single device.\n",
    "    logits = apply_fn({\"params\": params}, batch.inputs, train=True, rngs={\"dropout\": dropout_rng})\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch.labels)\n",
    "    correct_pred = jnp.equal(jnp.argmax(logits, axis=-1), batch.labels)\n",
    "    batch_size = batch.inputs.shape[0]\n",
    "    step_metrics = {\"loss\": (loss.sum(), batch_size), \"accuracy\": (correct_pred.sum(), batch_size)}\n",
    "    loss = loss.mean()\n",
    "    return loss, step_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "12944be5-8a03-47a8-932c-e05dcebea421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_dp(\n",
    "    state: TrainState,\n",
    "    metrics: Metrics | None,\n",
    "    batch: Batch,\n",
    ") -> Tuple[TrainState, Metrics]:\n",
    "    rng, step_rng = jax.random.split(state.rng)\n",
    "    grads, step_metrics = accumulate_gradients(\n",
    "        state,\n",
    "        batch,\n",
    "        step_rng,\n",
    "        config.optimizer.num_minibatches,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    # Update parameters. We need to sync the gradients across devices before updating.\n",
    "    with jax.named_scope(\"sync_gradients\"):\n",
    "        grads = jax.tree_map(lambda g: jax.lax.pmean(g, axis_name=config.data_axis_name), grads)\n",
    "    new_state = state.apply_gradients(grads=grads, rng=rng)\n",
    "    # Sum metrics across replicas. Alternatively, we could keep the metrics separate\n",
    "    # and only synchronize them before logging. For simplicity, we sum them here.\n",
    "    with jax.named_scope(\"sync_metrics\"):\n",
    "        step_metrics = jax.tree_map(\n",
    "            lambda x: jax.lax.psum(x, axis_name=config.data_axis_name), step_metrics\n",
    "        )\n",
    "    if metrics is None:\n",
    "        metrics = step_metrics\n",
    "    else:\n",
    "        metrics = jax.tree_map(jnp.add, metrics, step_metrics)\n",
    "    return new_state, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6728bb36-fcbd-4dea-bc7e-b6b63b8b1064",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_dp_fn = jax.jit(\n",
    "    shard_map(\n",
    "        train_step_dp,\n",
    "        mesh,\n",
    "        in_specs=(P(), P(), P(config.data_axis_name)),\n",
    "        out_specs=(P(), P()),\n",
    "        check_rep=False,\n",
    "    ),\n",
    "    donate_argnames=(\"state\", \"metrics\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6fd67608-d07f-4ad8-820a-66f64834f436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_64965/1423340975.py:16: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  grads = jax.tree_map(lambda g: jax.lax.pmean(g, axis_name=config.data_axis_name), grads)\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_64965/1423340975.py:21: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  step_metrics = jax.tree_map(\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_64965/2067979507.py:7: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  metrics_dp = jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)\n"
     ]
    }
   ],
   "source": [
    "_, metric_shapes = jax.eval_shape(\n",
    "    train_step_dp_fn,\n",
    "    state_dp,\n",
    "    None,\n",
    "    batch,\n",
    ")\n",
    "metrics_dp = jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2d8a6d92-fc60-4281-9b4e-790d67b2698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(15):\n",
    "    state_dp, metrics_dp = train_step_dp_fn(state_dp, metrics_dp, batch)\n",
    "final_metrics_dp = jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)\n",
    "state_dp, final_metrics_dp = train_step_dp_fn(state_dp, final_metrics_dp, batch)\n",
    "print_metrics(final_metrics_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f6653cca-5926-41df-b3d3-f90370f0eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DP Parameters\")\n",
    "pprint(jax.tree_map(lambda x: (x.shape, x.sharding), state_dp.params))\n",
    "print(\"Metrics\")\n",
    "pprint(jax.tree_map(lambda x: (x.shape, x.sharding), final_metrics_dp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd2dfd6-2086-48bf-b50a-4d8d0c2bd8e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## FSDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9b916f38-a05a-4fa1-a867-a887ed804a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Parameter = jax.Array | nn.Partitioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "04a10201-b9a3-4814-81c1-e7746b5af071",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.named_scope(\"shard_params\")\n",
    "def shard_params(params: PyTree, axis_name: str, min_weight_size: int = 2**18) -> PyTree:\n",
    "    \"\"\"Shard parameters across the given mesh axis.\n",
    "\n",
    "    Args:\n",
    "        params: The parameters to shard.\n",
    "        axis_name: The axis to shard parameters across.\n",
    "        min_weight_size: The minimum size of a parameter to shard. Parameters with fewer values will not be sharded.\n",
    "\n",
    "    Returns:\n",
    "        PyTree of same structure as params, but with leaves sharded over new axis if possible.\n",
    "    \"\"\"\n",
    "    axis_idx = jax.lax.axis_index(axis_name)\n",
    "    axis_size = jax.lax.psum(1, axis_name)\n",
    "\n",
    "    def _split(x: Parameter) -> Parameter:\n",
    "        if isinstance(x, nn.Partitioned):\n",
    "            value, names = x.value, x.names\n",
    "        else:\n",
    "            value = x\n",
    "            names = (None,) * value.ndim\n",
    "        if axis_name in names:\n",
    "            logging.warning(\n",
    "                f\"Parameter {value.shape} with names {names} already sharded on axis {axis_name}.\"\n",
    "            )\n",
    "            return x\n",
    "        elif value.size <= min_weight_size:\n",
    "            logging.info(\n",
    "                f\"Parameter {value.shape} with names {names} too small to shard, size {value.size} < {min_weight_size}.\"\n",
    "            )\n",
    "            return x\n",
    "        else:\n",
    "            shape = value.shape\n",
    "            idx = np.argsort(shape)[::-1]  # Shard along largest possible axis.\n",
    "            for i in idx:\n",
    "                if shape[i] % axis_size == 0 and names[i] is None:\n",
    "                    split_size = shape[i] // axis_size\n",
    "                    p_sharded = nn.Partitioned(\n",
    "                        value=lax.dynamic_slice_in_dim(  # Shard to keep on present device.\n",
    "                            value, axis_idx * split_size, split_size, axis=i\n",
    "                        ),\n",
    "                        names=names[:i] + (axis_name,) + names[i + 1 :],\n",
    "                    )\n",
    "                    return p_sharded\n",
    "            logging.warning(\n",
    "                f\"Could not shard {value.shape} with names {names} on axis {axis_name}, no suitable axis found.\"\n",
    "            )\n",
    "            return x\n",
    "\n",
    "    return jax.tree_util.tree_map(\n",
    "        _split,\n",
    "        params,\n",
    "        is_leaf=lambda x: isinstance(\n",
    "            x, nn.Partitioned\n",
    "        ),  # Consider a nn.Partitioned object as a leaf.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "703a2307-894d-4576-b36e-c905b630f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_array_with_mean_grads(x: jax.Array, axis: int, axis_name: str):\n",
    "    \"\"\"Gathering with averaging gradients across replicas.\"\"\"\n",
    "    axis_size = jax.lax.psum(1, axis_name)\n",
    "\n",
    "    # Define a custom gradient for the gather operation.\n",
    "    @jax.custom_gradient\n",
    "    def f(x):\n",
    "        def grad_fn(g):\n",
    "            # pmean_scatter\n",
    "            return (\n",
    "                jax.lax.psum_scatter(g, axis_name, scatter_dimension=axis, tiled=True) / axis_size\n",
    "            )\n",
    "\n",
    "        return jax.lax.all_gather(x, axis_name, axis=axis, tiled=True), grad_fn\n",
    "\n",
    "    return f(x)\n",
    "\n",
    "@jax.named_scope(\"gather_params\")\n",
    "def gather_params(params: PyTree, axis_name: str) -> PyTree:\n",
    "    \"\"\"Gather parameters from all replicas across the given axis.\n",
    "\n",
    "    Args:\n",
    "        params: The parameters to gather.\n",
    "        axis_name: The axis to gather parameters across.\n",
    "\n",
    "    Returns:\n",
    "        PyTree of same structure as params, but with leaves gathered if they were a nn.Partitioned object.\n",
    "    \"\"\"\n",
    "\n",
    "    def _gather(p: Parameter) -> Parameter:\n",
    "        if isinstance(p, nn.Partitioned) and axis_name in p.names:\n",
    "            param_shard = p.names\n",
    "            shard_axis = param_shard.index(axis_name)\n",
    "            value = gather_array_with_mean_grads(p.value, axis=shard_axis, axis_name=axis_name)\n",
    "            # If there are any other axes that are sharded, we need to keep the partitioned structure.\n",
    "            # Otherwise, we can return the value directly.\n",
    "            param_shard = param_shard[:shard_axis] + (None,) + param_shard[shard_axis + 1 :]\n",
    "            if any([name is not None for name in param_shard]):\n",
    "                return nn.Partitioned(value, param_shard)\n",
    "            else:\n",
    "                return value\n",
    "        else:\n",
    "            return p\n",
    "\n",
    "    return jax.tree_util.tree_map(_gather, params, is_leaf=lambda x: isinstance(x, nn.Partitioned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "622c7364-d5c2-4b25-8c85-fee4316895a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For computation, we want to gather params, compute, then shard params\n",
    "\n",
    "def shard_module_params(\n",
    "    target: nn.Module | Callable, axis_name: str, min_weight_size: int = 2**18\n",
    ") -> nn.Module | Callable:\n",
    "    \"\"\"Shard parameters of a module across replicas.\n",
    "\n",
    "    Args:\n",
    "        target: The module to shard.\n",
    "        axis_name: The axis name to shard parameters across.\n",
    "        min_weight_size: The minimum size of a parameter to shard. Parameters with fewer values will not be sharded.\n",
    "\n",
    "    Returns:\n",
    "        The module with sharded parameters.\n",
    "    \"\"\"\n",
    "    return nn.map_variables(\n",
    "        target,\n",
    "        trans_in_fn=functools.partial(gather_params, axis_name=axis_name),\n",
    "        trans_out_fn=functools.partial(\n",
    "            shard_params, axis_name=axis_name, min_weight_size=min_weight_size\n",
    "        ),\n",
    "        mapped_collections=\"params\",\n",
    "        mutable=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "27287110-fd6a-42f9-9170-384e6b8056a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSDPClassifier(nn.Module):\n",
    "    config: ConfigDict\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jax.Array, train: bool) -> jax.Array:\n",
    "        sharded_dense = shard_module_params(\n",
    "            nn.Dense,\n",
    "            axis_name=self.config.data_axis_name,\n",
    "            min_weight_size=self.config.min_weight_size,\n",
    "        )\n",
    "        x = sharded_dense(\n",
    "            features=self.config.hidden_size,\n",
    "            dtype=self.config.dtype,\n",
    "            name=\"input_dense\",\n",
    "        )(x)\n",
    "        x = nn.silu(x)\n",
    "        x = nn.Dropout(rate=self.config.dropout_rate, deterministic=not train)(x)\n",
    "        x = sharded_dense(\n",
    "            features=self.config.num_classes,\n",
    "            dtype=self.config.dtype,\n",
    "            name=\"output_dense\",\n",
    "        )(x)\n",
    "        x = x.astype(jnp.float32)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ac09633c-4f84-4e66-bcf5-f0351e7f7028",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model.min_weight_size = 2**4\n",
    "model_fsdp = FSDPClassifier(config=config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "91666ae9-bf08-445d-9ee2-837189e38edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNG PartitionSpec()\n",
      "\n",
      "Parameters\n",
      "{'input_dense': {'bias': PartitionSpec('data',),\n",
      "                 'kernel': PartitionSpec('data', None)},\n",
      " 'output_dense': {'bias': PartitionSpec(),\n",
      "                  'kernel': PartitionSpec('data', None)}}\n",
      "\n",
      "Optimizer state\n",
      "ScaleByAdamState(count=PartitionSpec(), mu={'input_dense': {'bias': PartitionSpec('data',), 'kernel': PartitionSpec('data', None)}, 'output_dense': {'bias': PartitionSpec(), 'kernel': PartitionSpec('data', None)}}, nu={'input_dense': {'bias': PartitionSpec('data',), 'kernel': PartitionSpec('data', None)}, 'output_dense': {'bias': PartitionSpec(), 'kernel': PartitionSpec('data', None)}})\n"
     ]
    }
   ],
   "source": [
    "init_fsdp_fn = shard_map(\n",
    "    functools.partial(init_dp, model=model_fsdp),\n",
    "    mesh,\n",
    "    in_specs=(P(), P(config.data_axis_name)),\n",
    "    out_specs=P(),\n",
    "    check_rep=False,\n",
    ")\n",
    "state_fsdp_shapes = jax.eval_shape(init_fsdp_fn, model_init_rng, batch.inputs)\n",
    "state_fsdp_specs = nn.get_partition_spec(state_fsdp_shapes)\n",
    "print(\"RNG\", state_fsdp_specs.rng)\n",
    "print(\"\\nParameters\")\n",
    "pprint(state_fsdp_specs.params)\n",
    "print(\"\\nOptimizer state\")\n",
    "pprint(state_fsdp_specs.opt_state[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "15fbe700-0675-4384-8cba-2b618f5a9537",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_fsdp_fn = jax.jit(\n",
    "    shard_map(\n",
    "        functools.partial(init_dp, model=model_fsdp),\n",
    "        mesh,\n",
    "        in_specs=(P(), P(config.data_axis_name)),\n",
    "        out_specs=state_fsdp_specs,\n",
    "        check_rep=False,\n",
    "    )\n",
    ")\n",
    "state_fsdp = init_fsdp_fn(model_init_rng, batch.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "029f8211-da3c-4dd1-93dd-af9aeb1b19ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSDP Parameters\n",
      "{'input_dense': {'bias': Partitioned(value=(512,), names=('data',), mesh=None),\n",
      "                 'kernel': Partitioned(value=(784, 512),\n",
      "                                       names=('data', None),\n",
      "                                       mesh=None)},\n",
      " 'output_dense': {'bias': (10,),\n",
      "                  'kernel': Partitioned(value=(512, 10),\n",
      "                                        names=('data', None),\n",
      "                                        mesh=None)}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_64965/1764988345.py:2: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  pprint(jax.tree_map(lambda x: x.shape, jax.device_get(state_fsdp.params)))\n"
     ]
    }
   ],
   "source": [
    "print(\"FSDP Parameters\")\n",
    "pprint(jax.tree_map(lambda x: x.shape, jax.device_get(state_fsdp.params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bae7a427-49a9-48dc-870a-586162f2ca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_gradients(\n",
    "    grads: PyTree,\n",
    "    axis_names: Sequence[str],\n",
    ") -> PyTree:\n",
    "    \"\"\"Synchronize gradients across devices.\n",
    "\n",
    "    Gradients for parameters that are replicated over a given axis are averaged across devices.\n",
    "    Parameters that are partitioned over a given axis are considered to already have a mean of\n",
    "    the gradients on each device, and hence do not need to be altered.\n",
    "\n",
    "    Args:\n",
    "        grads: The gradients to synchronize.\n",
    "        axis_names: The axis names to synchronize gradients across.\n",
    "\n",
    "    Returns:\n",
    "        The gradients averaged over the specified axes if they are replicated.\n",
    "    \"\"\"\n",
    "\n",
    "    def sync_grad(g: Parameter) -> Parameter:\n",
    "        if isinstance(g, nn.Partitioned):\n",
    "            # Tree leaves for flattening potentially nested axis (multiple names can exist for single array axis).\n",
    "            replication_axis_names = [\n",
    "                name for name in axis_names if name not in jax.tree_util.tree_leaves(g.names)\n",
    "            ]\n",
    "            if len(replication_axis_names) == 0:\n",
    "                # Parameters partitioned over all axes.\n",
    "                return g\n",
    "            else:\n",
    "                # Average over remaining replicated axes.\n",
    "                return g.replace(value=jax.lax.pmean(g.value, axis_name=replication_axis_names))\n",
    "        else:\n",
    "            # Parameters are replicated over all axes.\n",
    "            return jax.lax.pmean(g, axis_name=axis_names)\n",
    "\n",
    "    return jax.tree_map(sync_grad, grads, is_leaf=lambda x: isinstance(x, nn.Partitioned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5f3044d1-e047-46ce-8352-882ed7e26d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_fsdp(\n",
    "    state: TrainState,\n",
    "    metrics: Metrics,\n",
    "    batch: Batch,\n",
    ") -> Tuple[TrainState, Metrics]:\n",
    "    rng, step_rng = jax.random.split(state.rng)\n",
    "    grads, step_metrics = accumulate_gradients(\n",
    "        state,\n",
    "        batch,\n",
    "        step_rng,\n",
    "        config.optimizer.num_minibatches,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    # Update parameters. We need to sync the gradients across devices before updating.\n",
    "    with jax.named_scope(\"sync_gradients\"):\n",
    "        grads = sync_gradients(grads, (config.data_axis_name,))\n",
    "    new_state = state.apply_gradients(grads=grads, rng=rng)\n",
    "    # Sum metrics across replicas. Alternatively, we could keep the metrics separate\n",
    "    # and only synchronize them before logging. For simplicity, we sum them here.\n",
    "    with jax.named_scope(\"sync_metrics\"):\n",
    "        step_metrics = jax.tree_map(\n",
    "            lambda x: jax.lax.psum(x, axis_name=config.data_axis_name), step_metrics\n",
    "        )\n",
    "    if metrics is None:\n",
    "        metrics = step_metrics\n",
    "    else:\n",
    "        metrics = jax.tree_map(jnp.add, metrics, step_metrics)\n",
    "    return new_state, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "97c3a5da-7660-446a-9e52-4039ef5be8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_64965/283540172.py:35: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(sync_grad, grads, is_leaf=lambda x: isinstance(x, nn.Partitioned))\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_64965/434889388.py:21: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  step_metrics = jax.tree_map(\n",
      "/var/folders/2v/0s_wd0cd04g3xtcdpppg90980000gn/T/ipykernel_64965/434754111.py:17: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  metrics_fsdp = jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)\n"
     ]
    }
   ],
   "source": [
    "train_step_fsdp_fn = jax.jit(\n",
    "    shard_map(\n",
    "        train_step_fsdp,\n",
    "        mesh,\n",
    "        in_specs=(state_fsdp_specs, P(), P(config.data_axis_name)),\n",
    "        out_specs=(state_fsdp_specs, P()),\n",
    "        check_rep=False,\n",
    "    ),\n",
    "    donate_argnames=(\"state\", \"metrics\"),\n",
    ")\n",
    "_, metric_shapes = jax.eval_shape(\n",
    "    train_step_fsdp_fn,\n",
    "    state_fsdp,\n",
    "    None,\n",
    "    batch,\n",
    ")\n",
    "metrics_fsdp = jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cb13d989-e54d-4ba6-8f5e-84cbb2248e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(15):\n",
    "    state_fsdp, metrics_fsdp = train_step_fsdp_fn(state_fsdp, metrics_fsdp, batch)\n",
    "final_metrics_fsdp = jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)\n",
    "state_fsdp, final_metrics_fsdp = train_step_fsdp_fn(state_fsdp, final_metrics_fsdp, batch)\n",
    "print_metrics(final_metrics_fsdp, \"FSDP - Final metrics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
