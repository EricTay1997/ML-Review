{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "776b273c-4c3d-42b3-a844-9df49d9a57ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "863059eb-d61a-4260-9e42-9652502f44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import add_dummy_feature\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ccfbb2e-1f75-429c-af8d-992aced36625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "# Continuous\n",
    "np.random.seed(42)  \n",
    "m = 100  \n",
    "X = 2 * np.random.rand(m, 1)  \n",
    "y = 4 + 3 * X + np.random.randn(m, 1)  \n",
    "X_b = add_dummy_feature(X)  \n",
    "X_new = np.array([[0], [2]])\n",
    "\n",
    "# Categorical\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y_bin = (iris.target == 2) \n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3028abaa-138d-4aad-97b0-f3c46bfeab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern is\n",
    "# model()\n",
    "# model.fit(X_train, y_train)\n",
    "# model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44cc38cb-f6fc-49f0-90ad-5f0e6d04e4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "pred_lin = lin_reg.predict(X_new)\n",
    "\n",
    "ridge_reg = Ridge(alpha=100, solver=\"cholesky\")\n",
    "ridge_reg.fit(X, y)\n",
    "pred_ridge = ridge_reg.predict(X_new)\n",
    "\n",
    "lasso_reg = Lasso(alpha=100/(2*len(X)))\n",
    "lasso_reg.fit(X, y)\n",
    "pred_lasso = lasso_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "169c62f3-8d3f-436d-b500-302603893717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50,  0,  0],\n",
       "       [ 0, 47,  3],\n",
       "       [ 0,  2, 48]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X, y) # allows for multi-class\n",
    "y_pred = log_reg.predict(X)\n",
    "cm = confusion_matrix(y, y_pred) \n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "131aaf76-6270-4e7d-91aa-185182b8faa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "svm_clf1 = LinearSVC(C=1, max_iter=10_000, dual=True, random_state=42) # linear kernel, can use SVC for rbf kernel\n",
    "# model_loss = C*classification_loss + penalty, so lower C means higher regularization\n",
    "scaled_svm_clf1 = make_pipeline(scaler, svm_clf1)\n",
    "scaled_svm_clf1.fit(X, y) # allows for multi-class\n",
    "scaled_svm_clf1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6466ee8b-d74c-42b7-8d06-8a0c35fdfeea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2,\n",
       "       2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf.fit(X, y)\n",
    "tree_clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5523b33a-43ea-4a07-a827-633c4f23709b",
   "metadata": {},
   "source": [
    "# NN Skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "772bef86-c24c-428e-8428-7b18d8496bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088351f9-b45e-4ca5-ac95-43a11eaf1f58",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2ddebfeb-dd5e-4c0b-9ec0-c6b3b4383493",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XORDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, size, std=0.1):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.std = std\n",
    "        self.generate_continuous_xor()\n",
    "\n",
    "    def generate_continuous_xor(self):\n",
    "        data = torch.randint(low=0, high=2, size=(self.size, 2), dtype=torch.float32)\n",
    "        label = (data.sum(dim=1) == 1).to(torch.long)\n",
    "        data += self.std * torch.randn(data.shape)\n",
    "\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_point = self.data[idx]\n",
    "        data_label = self.label[idx]\n",
    "        return data_point, data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3641ac6-7fb9-4be2-8268-ad146197459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, act_fn = nn.Tanh()):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.act_fn = act_fn\n",
    "        self.linear2 = nn.Linear(num_hidden, num_outputs)\n",
    "\n",
    "    # One layer and no activation for glm\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef6dd6fb-a888-4ec3-b36c-64f894922f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, data_loader, loss_module, num_epochs=100):\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for data_inputs, data_labels in data_loader:\n",
    "            data_inputs = data_inputs.to(device)\n",
    "            data_labels = data_labels.to(device)\n",
    "            preds = model(data_inputs)\n",
    "            preds = preds.reshape(-1,)\n",
    "            loss = loss_module(preds, data_labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61763efd-c9d2-4c61-889b-e8596ca1d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader):\n",
    "    model.eval() \n",
    "    true_preds, num_preds = 0., 0.\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for data_inputs, data_labels in data_loader:\n",
    "            data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n",
    "            preds = model(data_inputs)\n",
    "            pred_labels = (torch.sigmoid(preds) > 0.5).int().reshape(-1)\n",
    "            true_preds += (pred_labels == data_labels).sum().float()\n",
    "            num_preds += data_labels.shape[0]\n",
    "\n",
    "    acc = true_preds / num_preds\n",
    "    print(f\"Accuracy of the model: {100.0*acc:4.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cd121db-7c90-47ed-a706-eb083554e3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\") if torch.mps.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device\", device)\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.mps.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2780019e-c1f4-44b0-8f5d-219d4aa1ccee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce31e0ea7a2f49e4a23caa74cff7eaa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'linear1.weight': tensor([[-0.6186, -1.2413],\n",
      "        [ 2.4791,  2.3084],\n",
      "        [-1.9578,  2.8000],\n",
      "        [ 3.2432, -2.6608]], device='mps:0'), 'linear1.bias': tensor([ 1.3746, -0.7270,  0.7655,  1.2364], device='mps:0'), 'linear2.weight': tensor([[ 2.0459,  3.4998, -3.6495, -4.2671]], device='mps:0'), 'linear2.bias': tensor([0.8146], device='mps:0')})\n",
      "Accuracy of the model: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor_str.py:145: UserWarning: MPS: nonzero op is supported natively starting from macOS 14.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1729646995093/work/aten/src/ATen/native/mps/operations/Indexing.mm:361.)\n",
      "  nonzero_finite_vals = torch.masked_select(\n"
     ]
    }
   ],
   "source": [
    "set_seed(1)\n",
    "# Change this to MSE for linear regression\n",
    "# Change this to CrossEntropyLoss for multi-category, but rmb to change num_outputs to number of classes\n",
    "loss_module = nn.BCEWithLogitsLoss()\n",
    "train_dataset = XORDataset(size=2500)\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_dataset = XORDataset(size=500)\n",
    "test_data_loader = data.DataLoader(test_dataset, batch_size=128, shuffle=False, drop_last=False)\n",
    "model = SimpleClassifier(num_inputs=2, num_hidden=4, num_outputs=1)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "train_model(model, optimizer, train_data_loader, loss_module)\n",
    "\n",
    "# Save model\n",
    "state_dict = model.state_dict()\n",
    "print(state_dict)\n",
    "# torch.save(state_dict, \"our_model.tar\")\n",
    "# state_dict = torch.load(\"our_model.tar\")\n",
    "# new_model = SimpleClassifier(num_inputs=2, num_hidden=4, num_outputs=1)\n",
    "# new_model.load_state_dict(state_dict)\n",
    "\n",
    "eval_model(model, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65cd243-9670-4841-807c-7b2816aadb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "# define dataloaders\n",
    "# define model\n",
    "# define device and set seed\n",
    "# define optimizer\n",
    "# define loss module\n",
    "# model.to(device)\n",
    "# model.train()\n",
    "# for each epoch\n",
    "# for data_input, data_label in data_loader:\n",
    "# predict\n",
    "# loss\n",
    "# zero\n",
    "# loss.backward\n",
    "# optimizer.step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f536113-1d68-4c73-ac0e-50184edb9f86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f72aa218-1f56-4fe2-8537-2c88531a0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XORDataset(data.Dataset):\n",
    "    def __init__(self, size, std = 0.1):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.data = torch.randint(2, size = (size, 2), dtype = torch.float32)\n",
    "        self.targets = (self.data.sum(axis = 1) == 1).to(torch.float32)\n",
    "        self.data += torch.randn(self.data.shape)*std\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.targets[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd698384-807e-4bef-8967-a071d630695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_inputs, num_hidden, num_classes, act_fn = nn.Tanh()):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.act_fn = act_fn\n",
    "        self.linear2 = nn.Linear(num_hidden, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c26bb5b-f893-4ed6-9598-964d2cd28ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.mps.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d128cdaf-5408-43cf-ab34-f76cedbedc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleClassifier(\n",
       "  (linear1): Linear(in_features=2, out_features=4, bias=True)\n",
       "  (act_fn): Tanh()\n",
       "  (linear2): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "set_seed(42)\n",
    "train_dataset = XORDataset(2500)\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size = 128, shuffle = True)\n",
    "model = SimpleClassifier(2, 4, 1)\n",
    "device = torch.device(\"mps\") if torch.mps.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "5113f71b-45d9-4086-b480-75aa8f3d9718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "50.32\n",
      "48.6\n",
      "47.96\n",
      "46.28\n",
      "48.96\n",
      "47.64\n",
      "49.64\n",
      "49.96\n",
      "50.08\n",
      "49.36\n",
      "51.8\n",
      "50.72\n",
      "51.64\n",
      "51.96\n",
      "52.12\n",
      "51.36\n",
      "56.36\n",
      "51.52\n",
      "50.8\n",
      "52.16\n",
      "50.92\n",
      "51.32\n",
      "51.2\n",
      "50.76\n",
      "51.44\n",
      "50.8\n",
      "52.32\n",
      "55.16\n",
      "62.76\n",
      "78.48\n",
      "89.4\n",
      "97.4\n",
      "99.4\n",
      "99.8\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "set_seed(42)\n",
    "train_dataset = XORDataset(2500)\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size = 128, shuffle = True)\n",
    "model = SimpleClassifier(2, 4, 1)\n",
    "device = torch.device(\"mps\") if torch.mps.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    sum_correct = 0\n",
    "    sum_data_points = 0\n",
    "    for data_input, data_label in train_data_loader:\n",
    "        data_input = data_input.to(device)\n",
    "        data_label = data_label.to(device)\n",
    "        pred = model(data_input).flatten()\n",
    "        loss = loss_fn(pred, data_label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sum_data_points += len(data_input)\n",
    "        sum_correct += ((pred > 0) == data_label).sum()\n",
    "    print(np.round((100*sum_correct/sum_data_points).cpu().item(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be6d1a8-df5f-41d7-85a2-960ca26d602f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "81efbabf-2fca-47e4-92a9-15c1c2698ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = self.__class__.__name__\n",
    "\n",
    "class Identity(ActivationFunction):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    def forward(self, x):\n",
    "        return x * (x > 0)\n",
    "\n",
    "class Tanh(ActivationFunction):\n",
    "    def forward(self, x):\n",
    "        return (torch.exp(x) - torch.exp(-x))/(torch.exp(x) + torch.exp(-x))\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    def forward(self, x):\n",
    "        return 1/(1 + torch.exp(-x))\n",
    "\n",
    "class SoftPlus(ActivationFunction):\n",
    "    def forward(self, x):\n",
    "        return torch.log(1 + np.exp(x))\n",
    "\n",
    "class ELU(ActivationFunction):\n",
    "    def __init__(self, alpha = 1):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.where(x >= 0, x, self.alpha*(torch.exp(x)-1))\n",
    "\n",
    "class GeLU(ActivationFunction):\n",
    "    def forward(self, x):\n",
    "        return 0.5*x*(1 + torch.tanh(torch.tensor(2/torch.pi)*(x + 0.044715*x**3)))\n",
    "\n",
    "class SiLU(ActivationFunction):\n",
    "    def forward(self, x):\n",
    "        return x/(1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ec2d8d-f51e-4daa-af15-bcf2f2d472f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23431610-2c62-401e-bf47-2eace6d232e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNetwork(nn.Module): \n",
    "    \n",
    "    def __init__(self, act_fn = nn.ReLU(), input_size=784, num_classes=10, hidden_sizes=[512, 256, 256, 128]):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        layer_sizes = [input_size] + hidden_sizes\n",
    "        for layer_index in range(1, len(layer_sizes)):\n",
    "            layers += [nn.Linear(layer_sizes[layer_index-1], layer_sizes[layer_index]),\n",
    "                       act_fn]\n",
    "        layers += [nn.Linear(layer_sizes[-1], num_classes)]\n",
    "        self.layers = nn.Sequential(*layers) \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if name.endswith(\"bias\"):\n",
    "                nn.init.zeros_(param)\n",
    "            else:\n",
    "                # nn.init.normal_(param, std = np.sqrt(2/(param.shape[0] + param.shape[1]))) # xavier\n",
    "                nn.init.normal_(param, std = np.sqrt(2/param.shape[1])) # kaiming\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e7b50-b574-4f27-be99-fa7550c395d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "06ec50eb-24f4-4bbf-914a-1839d65adaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerTemplate:\n",
    "    def __init__(self, params, lr):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.detach_()\n",
    "                p.grad.zero_()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                self.update_param(p)\n",
    "            \n",
    "    def update_param(self, p):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "13185628-f7a9-450f-a116-b0e62197ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(OptimizerTemplate):\n",
    "    def update_param(self, p):\n",
    "        p_update = -self.lr*p.grad\n",
    "        p.add_(p_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "333351ab-3e51-41cb-8d8f-bc04b56c96f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum(OptimizerTemplate):\n",
    "    def __init__(self, params, lr, momentum = 0.0):\n",
    "        super().__init__()\n",
    "        self.beta1 = momentum\n",
    "        self.param_momentum = {p : torch.zeros_like(p.data) for p in self.params}\n",
    "    \n",
    "    def update_param(self, p):\n",
    "        self.param_momentum[p] = p.grad + self.beta1*self.param_momentum[p]\n",
    "        p_update = -self.lr * self.param_momentum[p]\n",
    "        p.add_(p_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ab29fc5e-7a46-43f7-ba06-01198340398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad(OptimizerTemplate):\n",
    "    def __init__(self, params, lr, epsilon = 1e-8):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.param_sq_grad_sum = {p : torch.zeros_like(p.data) for p in self.params}\n",
    "\n",
    "    def update_param(self, p):\n",
    "        self.param_sq_grad_sum[p].add_(p.grad**2)\n",
    "        p_update = -self.lr * p.grad / torch.sqrt(self.param_sq_grad_sum[p] + self.epsilon)\n",
    "        p.add_(p_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4079e675-fcc5-4c75-bb80-ca6bdc33f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp(OptimizerTemplate):\n",
    "    def __init__(self, params, lr, epsilon = 1e-8, beta2 = 0.999):\n",
    "        super().__init__()\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.param_sq_grad_sum = {p : torch.zeros_like(p.data) for p in self.params}\n",
    "\n",
    "    def update_param(self, p):\n",
    "        self.param_sq_grad_sum[p] = self.beta2*self.param_sq_grad_sum[p] + (1-self.beta2)*(p.grad**2)\n",
    "        p_update = -self.lr * p.grad / torch.sqrt(self.param_sq_grad_sum[p] + self.epsilon)\n",
    "        p.add_(p_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d08ddac0-8b9d-48ef-81ff-d8b560ba71b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaDelta(OptimizerTemplate):\n",
    "    def __init__(self, params, lr = 1.0, epsilon = 1e-8, beta2 = 0.999): \n",
    "        super().__init__()\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.param_delta = {p : torch.zeros_like(p.data) for p in self.params}\n",
    "        self.param_sq_grad_sum = {p : torch.zeros_like(p.data) for p in self.params}\n",
    "\n",
    "    def update_param(self, p):\n",
    "        self.param_sq_grad_sum[p] = self.beta2*self.param_sq_grad_sum[p] + (1-self.beta2)*(p.grad**2)\n",
    "        ada_lr = torch.sqrt(self.param_delta[p] + self.epsilon)\n",
    "        p_update = -self.lr * ada_lr * p.grad / torch.sqrt(self.param_sq_grad_sum[p] + self.epsilon)\n",
    "        p.add_(p_update)\n",
    "        self.param_delta[p] = self.beta2*self.param_delta[p] + (1-self.beta2)*((p_update/self.lr)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7f342c1e-b603-4734-a932-cfa6e5611c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(OptimizerTemplate):\n",
    "    def __init__(self, params, lr, epsilon = 1e-8, beta1 = 0.99, beta2 = 0.999):\n",
    "        super().__init__()\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.param_step = {p : 0 for p in self.params}\n",
    "        self.param_momentum = {p : torch.zeros_like(p.data) for p in self.params}\n",
    "        self.param_sq_grad_sum = {p : torch.zeros_like(p.data) for p in self.params}\n",
    "\n",
    "    def update_param(self, p):\n",
    "        self.param_step[p] += 1\n",
    "        self.param_momentum[p] = self.beta1*self.param_momentum[p] + (1-self.beta1)*p.grad\n",
    "        self.param_sq_grad_sum[p] = self.beta2*self.param_sq_grad_sum[p] + (1-self.beta2)*(p.grad**2)\n",
    "        beta1_norm = 1 - self.beta1**self.param_step[p]\n",
    "        beta2_norm = 1 - self.beta2**self.param_step[p]\n",
    "        p_update = -self.lr * (self.param_momentum[p]/beta1_norm) / (torch.sqrt(self.param_sq_grad_sum[p]/beta2_norm) + self.epsilon)\n",
    "        p.add_(p_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddceea3-3a33-4e03-97c2-2fdd37ed7195",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dot Product Attention / Multi-Head Self Attention / Cross-Attention / Grouped Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "23ddfe0e-20f4-40ac-b1e7-9b28e5d040cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = k.size()[-1]\n",
    "    attn_logits = torch.einsum('bhij,bhkj->bhik', q, k)\n",
    "    attn_logits = attn_logits / d_k\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim = -1)\n",
    "    values_added = torch.einsum('bhij,bhjk->bhik', attention, v)\n",
    "    return values_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a9738afd-eca6-4634-bccc-23d93e83df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, max_context_length, dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim is indivisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim*3, bias = qkv_bias)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.register_buffer(\n",
    "            \"causal_mask\", torch.triu(torch.ones(max_context_length, max_context_length), diagonal = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        # b, n, 3*embed_dim\n",
    "        qkv = self.qkv(x) \n",
    "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "        # q is of shape b,h,n,d\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
    "        attention = torch.einsum(\"bhij,bhkj->bhik\", q, k)\n",
    "        attention /= torch.sqrt(self.head_dim)\n",
    "        attention = attention.masked_fill(self.causal_mask[:seq_len, :seq_len] == 1, -torch.inf)\n",
    "        attention = torch.softmax(attention, dim = -1)\n",
    "        attention = self.dropout(attention)\n",
    "        delta_x = torch.einsum(\"bhij,bhjk->bhik\", attention, v)\n",
    "        # previously, delta_x is of shape b,h,n,d\n",
    "        delta_x = delta_x.transpose(1,2).reshape(batch_size, seq_len, embed_dim)\n",
    "        delta_x = self.o_proj(delta_x)\n",
    "        return delta_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5119cef6-5361-4062-b6c5-706cf366b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very similar, except that y cross_dim and seq_len may be different\n",
    "# Also remove causal mask\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, cross_dim, num_heads, dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim is indivisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias = qkv_bias)\n",
    "        self.k_proj = nn.Linear(cross_dim, embed_dim, bias = qkv_bias)\n",
    "        self.v_proj = nn.Linear(cross_dim, embed_dim, bias = qkv_bias)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        # b, n, embed_dim\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(y)\n",
    "        v = self.v_proj(y)\n",
    "\n",
    "        # decoder seq_len can be different from encoder seq_len\n",
    "        q = q.reshape(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        k = k.reshape(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        v = v.reshape(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        attention = torch.einsum(\"bhij,bhkj->bhik\", q, k)\n",
    "        attention /= torch.sqrt(self.head_dim)\n",
    "        attention = torch.softmax(attention, dim = -1)\n",
    "        attention = self.dropout(attention)\n",
    "        delta_x = torch.einsum(\"bhij,bhjk->bhik\", attention, v)\n",
    "        # previously, delta_x is of shape b,h,n,d\n",
    "        delta_x = delta_x.transpose(1,2).reshape(batch_size, seq_len, embed_dim)\n",
    "        delta_x = self.o_proj(delta_x)\n",
    "        return delta_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca735f38-7ba0-49a9-8c19-a49eb7914665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we remove bias and dropout\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, max_context_length, num_heads, num_kv_heads, dtype = None):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim is indivisible by num_heads\"\n",
    "        assert num_heads % num_kv_heads == 0, \"num_heads is indivisible by num_kv_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.q_to_k_ratio = num_heads // num_kv_heads\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim + 2 * num_kv_heads * self.head_dim, bias = False, dtype = dtype)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim, bias = False)\n",
    "        self.register_buffer(\n",
    "            \"causal_mask\", torch.triu(torch.ones(max_context_length, max_context_length), diagonal = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        q = qkv[:, :, :embed_dim].reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        k, v = qkv[:, :, embed_dim:].reshape(batch_size, seq_len, 2, self.num_kv_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        k = k.repeat_interleave(self.qkv, dim = 1)\n",
    "        v = v.repeat_interleave(self.qkv, dim = 1)\n",
    "\n",
    "        attention = torch.einsum(\"bhij,bhkj->bhik\", q, k)\n",
    "        attention /= k.shape[-1]**0.5\n",
    "        attention = attention.masked_fill(self.causal_mask[:seq_len, :seq_len] == 1, -torch.inf)\n",
    "        attention = torch.softmax(attention, dim = -1)\n",
    "        delta_x = torch.einsum(\"bhij,bhjk->bhik\", q, v)\n",
    "        delta_x = delta_x.transpose(1,2).reshape(batch_size, seq_len, embed_dim)\n",
    "        delta_x = self.o_proj(delta_x)\n",
    "        return delta_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf553476-42d2-4166-b2d7-d14e28059301",
   "metadata": {},
   "source": [
    "# GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "00552396-b468-47bf-92b8-4bfba7fd676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False,      # Query-Key-Value bias\n",
    "    \"std\": 0.02\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "22eb245a-5011-44c8-b562-faa3f123cd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_embedding = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.final_linear = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False)\n",
    "        self.tok_embedding.weight = self.final_linear.weight # weight_tying\n",
    "\n",
    "        # init\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('proj.weight'):\n",
    "                nn.init.normal_(p, std = cfg[\"std\"]/((2*cfg[\"n_layers\"])**0.5))\n",
    "            elif pn.endswith('weight'):\n",
    "                nn.init.normal_(p, std = cfg[\"std\"])\n",
    "            elif pn.endswith('bias'):\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        tok_embeds = self.tok_embedding(x)\n",
    "        pos_embeds = self.pos_embedding(torch.arange(seq_len, device = x.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.dropout(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.final_linear(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "6e47d754-29d4-4b0b-a7fc-df93642e5519",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True, unbiased = False)\n",
    "        norm_x = (x-mean) / torch.sqrt(var + self.eps)\n",
    "        return norm_x*self.scale + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "afac0be2-dbbf-4f66-813d-64e6d32e1817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(cfg)\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        orig_x = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.dropout(x)\n",
    "        x += orig_x\n",
    "\n",
    "        orig_x = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout(x)\n",
    "        x += orig_x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "b85d63da-f4a4-45b7-a4ac-08737fc7a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        assert cfg[\"emb_dim\"] % cfg[\"n_heads\"] == 0, \"emb_dim is indivisible by n_heads\"\n",
    "        self.num_heads = cfg[\"n_heads\"]\n",
    "        self.head_dim = cfg[\"emb_dim\"] // cfg[\"n_heads\"]\n",
    "        self.qkv = nn.Linear(cfg[\"emb_dim\"], cfg[\"emb_dim\"]*3, bias = cfg[\"qkv_bias\"])\n",
    "        self.o_proj = nn.Linear(cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        self.att_dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.register_buffer(\n",
    "            \"causal_mask\", torch.triu(torch.ones(cfg[\"context_length\"], cfg[\"context_length\"]), diagonal = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, emb_dim = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
    "        attention = torch.einsum('bhij,bhkj->bhik', q, k)\n",
    "        attention /= k.shape[-1]**0.5\n",
    "        attention = attention.masked_fill(self.causal_mask[:seq_len, :seq_len] == 1, -torch.inf)\n",
    "        attention = torch.softmax(attention, dim = -1)\n",
    "        attention = self.att_dropout(attention)\n",
    "        delta_x = torch.einsum('bhij,bhjk->bhik', attention, v)\n",
    "        delta_x = delta_x.transpose(1,2)\n",
    "        delta_x = delta_x.reshape(batch_size, seq_len, emb_dim)\n",
    "        delta_x = self.o_proj(delta_x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "296e1706-e759-420d-9408-853ce3292b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"emb_dim\"]*4)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.proj = nn.Linear(cfg[\"emb_dim\"]*4, cfg[\"emb_dim\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a843c78e-f700-4745-b30a-94cee107e74e",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "- Fixed PE, RoPE\n",
    "- LayerNorm/RMSNorm\n",
    "- Pre/Post Layer norm\n",
    "- GeLU/SwiGLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a522241b-66b0-40e1-8298-dadd9c02b1b6",
   "metadata": {},
   "source": [
    "### GELU/SwiGLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "8ac6c0bf-f4dd-4a3c-a67c-afa3c1a1af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "be19905c-3501-451a-bcc5-27110041ca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype = cfg[\"dtype\"], bias = False)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype = cfg[\"dtype\"], bias = False)\n",
    "        self.proj = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype = cfg[\"dtype\"], bias = False)\n",
    "        self.silu = SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = self.silu(x_fc1) * x_fc2\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2502cd3b-975b-46d6-b17c-6a83af6205dd",
   "metadata": {},
   "source": [
    "### RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "8a5d2064-f3a9-4383-810f-f491c020355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps = 1e-05):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True, unbiased = False)\n",
    "        x = (x-mean)/torch.sqrt(var + self.eps)\n",
    "        return x*self.scale + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "717033bf-cc4d-47f6-bb0f-a7a93b3c1724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps = 1e-05):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(x.pow(2).mean(dim = -1, keepdim = True) + self.eps)\n",
    "        x = self.scale*(x/rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ccbaf4-ad36-4535-baa2-541456125838",
   "metadata": {},
   "source": [
    "### Pre/Post Layer Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "d83a6c3c-0045-4e58-bd6a-67aa3b882b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttn(cfg)\n",
    "        self.ffn = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward_pre(self, x): \n",
    "        orig_x = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.dropout(x)\n",
    "        x += orig_x\n",
    "\n",
    "        orig_x = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x += orig_x\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_post(self, x): # Also one more thing - in the GPTModel, we no longer need the final norm\n",
    "        orig_x = x\n",
    "        x = self.att(x)\n",
    "        x = self.dropout(x)\n",
    "        x += orig_x\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        orig_x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x += orig_x\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8806f8b1-fc8d-4d8d-89c6-0fd8e0bcab67",
   "metadata": {},
   "source": [
    "### PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "ca32862a-7fc7-4342-beef-2002e7245db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb:\n",
    "    @staticmethod\n",
    "    def generate_sinusoidal_pos_emb(ctx_len, emb_dim):\n",
    "        pe = torch.zeros((ctx_len, emb_dim))\n",
    "        position = torch.arange(0, ctx_len, dtype=torch.float).unsqueeze(1) # (ctx_len, 1)\n",
    "        div_term = torch.tensor(10000.0).pow(torch.arange(0, emb_dim, 2).float()/emb_dim) # (1, emb_dim)\n",
    "        pe[:, 0::2] = torch.sin(position / div_term)\n",
    "        pe[:, 1::2] = torch.cos(position / div_term)\n",
    "        return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "cfcaea7a-2c57-4b4f-ab35-7b45985921ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.learned_pos_embedding = nn.Embedding(cfg[\"ctx_length\"], cfg[\"emb_dim\"])\n",
    "        sin_pos_emb = SinusoidalPosEmb.generate_sinusoidal_pos_emb(cfg[\"ctx_length\"], cfg[\"emb_dim\"])\n",
    "        self.register_buffer('sin_pos_emb', sin_pos_emb, persistent = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        learned_pos_emb = self.learned_pos_embedding(torch.arange(seq_len, device = x.device))\n",
    "        sin_pos_emb = self.sin_pos_emb[:seq_len]\n",
    "        tok_embeds = self.tok_embedding(x)\n",
    "        x = tok_embeds + sin_pos_emb # add with broadcasting\n",
    "        return sin_pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "088e7473-4237-48fd-89d6-028d972b297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "bb9de396-5c6a-4935-ac6a-313f1b755e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"rope_base\": 500_000.0, \n",
    "    \"rope_freq\": {                        #YaRN\n",
    "        \"factor\": 8.0,\n",
    "        \"low_freq_factor\": 1.0,\n",
    "        \"high_freq_factor\": 4.0,\n",
    "        \"original_context_length\": 8192,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "8fd647d4-870f-4017-9bf6-469658e64ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        mask, cos, sin = SharedBuffers.get_buffers(cfg[\"ctx_length\"], self.head_dim, cfg[\"dtype\"], cfg['rope_base'], cfg['rope_freq'])\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "        self.register_buffer(\"cos\", cos)\n",
    "        self.register_buffer(\"sin\", sin)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # k and q are of shape (b, h, n, head_dim)\n",
    "        k = compute_rope(k, self.cos, self.sin)\n",
    "        q = compute_rope(q, self.cos, self.sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "5a643ee1-3061-459e-8118-65fb73db5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedBuffers:\n",
    "    _buffers = {} # In case we run with different configs\n",
    "\n",
    "    @staticmethod\n",
    "    def get_buffers(context_length, head_dim, dtype=torch.float32, rope_base = 10_000, freq_config = None):\n",
    "        key = (context_length, head_dim, dtype, rope_base, tuple(freq_config.values()) if freq_config else freq_config)\n",
    "\n",
    "        if key not in SharedBuffers._buffers:\n",
    "            mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "            cos, sin = precompute_rope_params(head_dim, rope_base, context_length, freq_config)\n",
    "            if dtype is not None:\n",
    "                cos = cos.to(dtype)\n",
    "                sin = sin.to(dtype)\n",
    "            SharedBuffers._buffers[key] = (mask, cos, sin)\n",
    "\n",
    "        return SharedBuffers._buffers[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "79699db8-bc96-498e-9998-ead18facb3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_rope_params(head_dim, theta_base=10_000, context_length=4096, freq_config=None):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "    #thetas\n",
    "    freqs = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "    # Rescale\n",
    "    if freq_config is not None:\n",
    "        # YaRN works by preserving (not-scaling) high frequencies, and scaling low frequencies down (increasing long wavelengths)\n",
    "        low_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"low_freq_factor\"]\n",
    "        high_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"high_freq_factor\"]\n",
    "        wavelens = 2 * torch.pi / freqs\n",
    "        # When wavelengths > context_length, decrease frequency\n",
    "        freqs_llama = torch.where(\n",
    "            wavelens > low_freq_wavelen, freqs / freq_config[\"factor\"], freqs\n",
    "        )\n",
    "        \n",
    "        smooth_factor = (freq_config[\"original_context_length\"] / wavelens - freq_config[\"low_freq_factor\"]) / (\n",
    "            freq_config[\"high_freq_factor\"] - freq_config[\"low_freq_factor\"]\n",
    "        )\n",
    "        smoothed_freqs = (\n",
    "            (1 - smooth_factor) * (freqs / freq_config[\"factor\"]) + smooth_factor * freqs\n",
    "        )\n",
    "        is_medium_freq = (wavelens <= low_freq_wavelen) & (wavelens >= high_freq_wavelen)\n",
    "        # When 1/4*context_length < wavelengths < context_length, decrease frequency by a smoothed amount\n",
    "        freqs_llama = torch.where(is_medium_freq, smoothed_freqs, freqs_llama)\n",
    "        freqs = freqs_llama\n",
    "    ms = torch.arange(context_length)\n",
    "    # Shape: (context_length, head_dim // 2)\n",
    "    angles = ms[:, None] * freqs[None, :]  \n",
    "    # Expand angles to match the head_dim, Shape: (context_length, head_dim)\n",
    "    angles = torch.cat([angles, angles], dim=1)\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "    return cos, sin # matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "fd58ddf6-db3a-4681-a3ff-17c14858b892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rope(x, cos, sin):\n",
    "    # See notes to understand why this works\n",
    "    # x: (batch_size, num_heads, num_tokens, head_dim)\n",
    "    batch_size, num_heads, num_tokens, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    # Adjust sin and cos shapes\n",
    "    cos = cos[:num_tokens, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, num_tokens, head_dim)\n",
    "    sin = sin[:num_tokens, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Reorder matrix columns\n",
    "    x1 = x[..., : head_dim // 2]  # First half\n",
    "    x2 = x[..., head_dim // 2 :]  # Second half\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
