{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b2bb67f-f37b-4716-b62d-6638d18af200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import Iterator, Optional, Sequence, List, TypeVar, Generic, Sized, Callable, Self\n",
    "from collections.abc import Mapping, Sequence as ABCSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b7c9a-b527-43ff-8ff4-5e886cbf9ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(arr):\n",
    "    for i in range(len(arr) - 1, 0, -1):\n",
    "        j = random.randint(0, i)  \n",
    "        arr[i], arr[j] = arr[j], arr[i]  \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ad65b49-b1fe-42e7-a645-fa5dfefdad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [1, 2, 3]\n",
    "my_iterator = iter(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85926ec2-fa68-414c-b96c-107c263557de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in my_iterator:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9abc4f51-957a-4ad7-a072-2409f526785c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([16, 5])\n",
      "Batch shape: torch.Size([16, 5])\n",
      "Batch shape: torch.Size([16, 5])\n",
      "Batch shape: torch.Size([16, 5])\n",
      "Batch shape: torch.Size([16, 5])\n",
      "Batch shape: torch.Size([16, 5])\n",
      "Batch shape: torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "class CustomDataLoader: # (Generic[T_co])\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: torch.utils.data.Dataset[torch.Tensor], # torch.utils.data.Dataset[T_co]\n",
    "        batch_size: int = 1,\n",
    "        shuffle: bool = False,\n",
    "        collate_fn: Optional[Callable] = None\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        # self.collate_fn = collate_fn if collate_fn is not None else default_collate\n",
    "        self.dataset_len = len(dataset)\n",
    "        self.num_batches = math.ceil(self.dataset_len / self.batch_size)\n",
    "        self.batches = None\n",
    "        \n",
    "    def __iter__(self) -> Iterator[torch.Tensor]: # Iterator[List[T_co]]\n",
    "        if self.shuffle:\n",
    "            indices = torch.randperm(self.dataset_len).tolist()\n",
    "        else:\n",
    "            indices = list(range(self.dataset_len))\n",
    "        # indices = list(self.sampler)\n",
    "        batches = []\n",
    "        for i in range(0, self.dataset_len, self.batch_size):\n",
    "            batch_indices = indices[i:i + self.batch_size]\n",
    "            batches.append(batch_indices)\n",
    "        self.batches = batches\n",
    "        return _DataLoaderIter(self)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.num_batches\n",
    "\n",
    "class _DataLoaderIter:    \n",
    "    def __init__(self, loader: CustomDataLoader):\n",
    "        self.loader = loader\n",
    "        self.current_batch = 0\n",
    "    \n",
    "    def __iter__(self) -> Self: # '_DataLoaderIter'\n",
    "        return self\n",
    "    \n",
    "    def __next__(self) -> torch.Tensor: # List[T_co]\n",
    "        if self.current_batch >= len(self.loader.batches):\n",
    "            raise StopIteration\n",
    "        batch_indices = self.loader.batches[self.current_batch]\n",
    "        self.current_batch += 1\n",
    "        batch_data = [self.loader.dataset[idx] for idx in batch_indices]\n",
    "        batch = torch.stack(batch_data, 0) # Assume tensor\n",
    "        # batch = self.loader.collate_fn(batch_data)\n",
    "        return batch\n",
    "\n",
    "class TensorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "data = torch.randn(100, 5)\n",
    "dataset = TensorDataset(data)\n",
    "dataloader = CustomDataLoader(dataset, batch_size=16, shuffle=True)\n",
    "for batch in dataloader:\n",
    "    print(f\"Batch shape: {batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1231d528-9d2d-42a1-9d4d-17a57b3baf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_collate(batch: List[T_co]):\n",
    "    elem = batch[0]\n",
    "    if isinstance(elem, torch.Tensor):\n",
    "        return torch.stack(batch, 0)\n",
    "    elif isinstance(elem, (str, bytes)):\n",
    "        return batch\n",
    "    elif isinstance(elem, Mapping):\n",
    "        return {key: default_collate([d[key] for d in batch]) for key in elem}\n",
    "    elif isinstance(elem, Sequence) and not isinstance(elem, (str, bytes)):\n",
    "        transposed = zip(*batch)\n",
    "        return [default_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0deb74-e02e-476d-a2c5-6c289fe4ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler: # Generic[T_co]\n",
    "    def __init__(self, data_source):\n",
    "        self.data_source = data_source\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        raise NotImplementedError\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_source)\n",
    "\n",
    "class SequentialSampler: # (Sampler[T_co])\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        return iter(range(len(self.data_source)))\n",
    "\n",
    "class RandomSampler: (Sampler[T_co])\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        return iter(torch.randperm(len(self.data_source)).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e3f87c8-fafe-4ed7-8921-5163de23f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator()\n",
    "generator.manual_seed(self.seed)\n",
    "indices = torch.randperm(self.dataset_len, generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8cc2ed18-5b79-4170-b128-a0fc7d7895fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import math\n",
    "# from typing import Iterator, Optional, List, TypeVar, Generic, Callable, Iterable\n",
    "# from collections.abc import Mapping, Sequence\n",
    "\n",
    "# T_co = TypeVar('T_co', covariant=True)\n",
    "\n",
    "# class Sampler(Generic[T_co]):\n",
    "#     \"\"\"\n",
    "#     Base class for all Samplers.\n",
    "    \n",
    "#     Every Sampler subclass has to provide an __iter__ method, providing a\n",
    "#     way to iterate over indices of dataset elements, and a __len__ method\n",
    "#     that returns the length of the returned iterators.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, data_source):\n",
    "#         \"\"\"\n",
    "#         Initialize Sampler.\n",
    "        \n",
    "#         Args:\n",
    "#             data_source: Dataset to sample from\n",
    "#         \"\"\"\n",
    "#         self.data_source = data_source\n",
    "    \n",
    "#     def __iter__(self) -> Iterator[int]:\n",
    "#         \"\"\"\n",
    "#         Return an iterator over the indices of the dataset.\n",
    "        \n",
    "#         Returns:\n",
    "#             Iterator over indices\n",
    "#         \"\"\"\n",
    "#         raise NotImplementedError\n",
    "    \n",
    "#     def __len__(self) -> int:\n",
    "#         \"\"\"\n",
    "#         Return the length of the dataset.\n",
    "        \n",
    "#         Returns:\n",
    "#             Number of samples in the dataset\n",
    "#         \"\"\"\n",
    "#         return len(self.data_source)\n",
    "\n",
    "# class SequentialSampler(Sampler[T_co]):\n",
    "#     \"\"\"\n",
    "#     Samples elements sequentially, always in the same order.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __iter__(self) -> Iterator[int]:\n",
    "#         return iter(range(len(self.data_source)))\n",
    "\n",
    "# class RandomSampler(Sampler[T_co]):\n",
    "#     \"\"\"\n",
    "#     Samples elements randomly, without replacement.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __iter__(self) -> Iterator[int]:\n",
    "#         return iter(torch.randperm(len(self.data_source)).tolist())\n",
    "\n",
    "# class CustomDataLoader(Generic[T_co]):\n",
    "#     \"\"\"\n",
    "#     Simplified Custom DataLoader implementation for PyTorch with sampler support.\n",
    "    \n",
    "#     This DataLoader works with map-style datasets that implement __getitem__ and __len__ methods.\n",
    "#     It supports sampling strategies via a sampler argument.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         dataset: torch.utils.data.Dataset[T_co],\n",
    "#         batch_size: int = 1,\n",
    "#         shuffle: bool = False,\n",
    "#         sampler: Optional[Sampler] = None,\n",
    "#         collate_fn: Optional[Callable] = None\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Initialize CustomDataLoader.\n",
    "        \n",
    "#         Args:\n",
    "#             dataset: Dataset from which to load data\n",
    "#             batch_size: Number of samples per batch\n",
    "#             shuffle: Whether to shuffle the data (ignored if sampler is specified)\n",
    "#             sampler: Strategy to draw samples from the dataset\n",
    "#             collate_fn: Function to merge samples into batches\n",
    "#         \"\"\"\n",
    "#         self.dataset = dataset\n",
    "#         self.batch_size = batch_size\n",
    "#         self.collate_fn = collate_fn if collate_fn is not None else default_collate\n",
    "        \n",
    "#         # Make sure sampler and shuffle are mutually exclusive\n",
    "#         if sampler is not None and shuffle:\n",
    "#             raise ValueError(\"sampler and shuffle cannot be used together\")\n",
    "        \n",
    "#         if sampler is None:\n",
    "#             if shuffle:\n",
    "#                 self.sampler = RandomSampler(dataset)\n",
    "#             else:\n",
    "#                 self.sampler = SequentialSampler(dataset)\n",
    "#         else:\n",
    "#             self.sampler = sampler\n",
    "        \n",
    "#         # Calculate the number of samples and batches\n",
    "#         self.dataset_len = len(dataset)\n",
    "#         self.num_batches = math.ceil(self.dataset_len / self.batch_size)\n",
    "    \n",
    "#     def __iter__(self) -> Iterator[List[T_co]]:\n",
    "#         \"\"\"\n",
    "#         Create a new iterator over the dataset.\n",
    "        \n",
    "#         Returns:\n",
    "#             Iterator over batches of data\n",
    "#         \"\"\"\n",
    "#         # Get indices from sampler\n",
    "#         indices = list(self.sampler)\n",
    "        \n",
    "#         # Create batches\n",
    "#         batches = []\n",
    "#         for i in range(0, len(indices), self.batch_size):\n",
    "#             batch_indices = indices[i:i + self.batch_size]\n",
    "#             batches.append(batch_indices)\n",
    "        \n",
    "#         return _DataLoaderIter(self, batches)\n",
    "    \n",
    "#     def __len__(self) -> int:\n",
    "#         \"\"\"\n",
    "#         Return the number of batches in the dataset.\n",
    "        \n",
    "#         Returns:\n",
    "#             Number of batches\n",
    "#         \"\"\"\n",
    "#         return self.num_batches\n",
    "\n",
    "# class _DataLoaderIter:\n",
    "#     \"\"\"\n",
    "#     Iterator class for the DataLoader.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, loader: CustomDataLoader, batches: List[List[int]]):\n",
    "#         \"\"\"\n",
    "#         Initialize the iterator.\n",
    "        \n",
    "#         Args:\n",
    "#             loader: The DataLoader instance\n",
    "#             batches: List of batch indices\n",
    "#         \"\"\"\n",
    "#         self.loader = loader\n",
    "#         self.batches = batches\n",
    "#         self.current_batch = 0\n",
    "    \n",
    "#     def __iter__(self) -> '_DataLoaderIter':\n",
    "#         return self\n",
    "    \n",
    "#     def __next__(self) -> List[T_co]:\n",
    "#         if self.current_batch >= len(self.batches):\n",
    "#             raise StopIteration\n",
    "        \n",
    "#         # Get current batch indices\n",
    "#         batch_indices = self.batches[self.current_batch]\n",
    "#         self.current_batch += 1\n",
    "        \n",
    "#         # Load data for this batch\n",
    "#         batch_data = [self.loader.dataset[idx] for idx in batch_indices]\n",
    "        \n",
    "#         # Apply collation function\n",
    "#         batch = self.loader.collate_fn(batch_data)\n",
    "        \n",
    "#         return batch\n",
    "\n",
    "# def default_collate(batch):\n",
    "#     \"\"\"\n",
    "#     Default collation function that converts a list of samples to a batch.\n",
    "    \n",
    "#     Args:\n",
    "#         batch: List of samples\n",
    "\n",
    "#     Returns:\n",
    "#         Collated batch\n",
    "#     \"\"\"\n",
    "#     elem = batch[0]\n",
    "#     if isinstance(elem, torch.Tensor):\n",
    "#         return torch.stack(batch, 0)\n",
    "#     elif isinstance(elem, (str, bytes)):\n",
    "#         return batch\n",
    "#     elif isinstance(elem, Mapping):\n",
    "#         return {key: default_collate([d[key] for d in batch]) for key in elem}\n",
    "#     elif isinstance(elem, Sequence) and not isinstance(elem, (str, bytes)):\n",
    "#         transposed = zip(*batch)\n",
    "#         return [default_collate(samples) for samples in transposed]\n",
    "#     else:\n",
    "#         return batch\n",
    "\n",
    "# # Example of a custom sampler: weighted sampling\n",
    "# class WeightedRandomSampler(Sampler):\n",
    "#     \"\"\"\n",
    "#     Samples elements randomly according to given weights.\n",
    "    \n",
    "#     Args:\n",
    "#         weights: a sequence of weights, not necessary summing up to 1\n",
    "#         num_samples: number of samples to draw\n",
    "#         replacement: if True, sampling is done with replacement\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, weights, num_samples, replacement=True):\n",
    "#         super().__init__(None)  # We don't actually need the data source\n",
    "#         self.weights = torch.as_tensor(weights, dtype=torch.double)\n",
    "#         self.num_samples = num_samples\n",
    "#         self.replacement = replacement\n",
    "    \n",
    "#     def __iter__(self):\n",
    "#         return iter(torch.multinomial(self.weights, self.num_samples, self.replacement).tolist())\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return self.num_samples\n",
    "\n",
    "# # Example usage:\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Create a simple tensor dataset\n",
    "#     class TensorDataset(torch.utils.data.Dataset):\n",
    "#         def __init__(self, data):\n",
    "#             self.data = data\n",
    "        \n",
    "#         def __getitem__(self, index):\n",
    "#             return self.data[index]\n",
    "        \n",
    "#         def __len__(self):\n",
    "#             return len(self.data)\n",
    "    \n",
    "#     # Create a dataset with 100 samples\n",
    "#     data = torch.randn(100, 5)\n",
    "#     dataset = TensorDataset(data)\n",
    "    \n",
    "#     # Example 1: Using shuffle parameter\n",
    "#     dataloader1 = CustomDataLoader(dataset, batch_size=16, shuffle=True)\n",
    "    \n",
    "#     # Example 2: Using a RandomSampler explicitly\n",
    "#     sampler = RandomSampler(dataset)\n",
    "#     dataloader2 = CustomDataLoader(dataset, batch_size=16, sampler=sampler)\n",
    "    \n",
    "#     # Example 3: Using a WeightedRandomSampler to sample with weights\n",
    "#     # Create weights that favor sampling from the first half of the dataset\n",
    "#     weights = torch.ones(len(dataset))\n",
    "#     weights[:len(dataset)//2] *= 2  # First half has double the weight\n",
    "#     weighted_sampler = WeightedRandomSampler(weights, num_samples=len(dataset), replacement=True)\n",
    "#     dataloader3 = CustomDataLoader(dataset, batch_size=16, sampler=weighted_sampler)\n",
    "    \n",
    "#     # Iterate through the DataLoader with weighted sampling\n",
    "#     print(\"Using weighted sampling:\")\n",
    "#     for i, batch in enumerate(dataloader3):\n",
    "#         if i < 3:  # Just print the first 3 batches\n",
    "#             print(f\"Batch {i} shape: {batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd2a5ae-a4b5-4a1d-84cf-377da93dd5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T_co = TypeVar('T_co', covariant=True)\n",
    "# T = TypeVar('T')\n",
    "\n",
    "# class CustomDataLoader(Generic[T_co]):\n",
    "#     \"\"\"\n",
    "#     Custom DataLoader implementation for PyTorch.\n",
    "    \n",
    "#     This DataLoader works with map-style datasets that implement __getitem__ and __len__ methods.\n",
    "#     It supports shuffling, batching, collation, and worker processes for parallel data loading.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         dataset: torch.utils.data.Dataset[T_co],\n",
    "#         batch_size: Optional[int] = 1,\n",
    "#         shuffle: bool = False,\n",
    "#         num_workers: int = 0,\n",
    "#         collate_fn: Optional[Callable] = None,\n",
    "#         drop_last: bool = False,\n",
    "#         pin_memory: bool = False,\n",
    "#         timeout: float = 0,\n",
    "#         worker_init_fn: Optional[Callable] = None,\n",
    "#         prefetch_factor: int = 2,\n",
    "#         persistent_workers: bool = False,\n",
    "#         seed: Optional[int] = None\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Initialize CustomDataLoader.\n",
    "        \n",
    "#         Args:\n",
    "#             dataset: Dataset from which to load data\n",
    "#             batch_size: Number of samples per batch\n",
    "#             shuffle: Whether to shuffle the data\n",
    "#             num_workers: Number of subprocesses for data loading\n",
    "#             collate_fn: Function to merge samples into batches\n",
    "#             drop_last: Whether to drop the last incomplete batch\n",
    "#             pin_memory: Whether to copy tensors to CUDA pinned memory\n",
    "#             timeout: Timeout for collecting a batch\n",
    "#             worker_init_fn: Function to initialize worker processes\n",
    "#             prefetch_factor: Number of batches loaded in advance by each worker\n",
    "#             persistent_workers: Whether to keep worker processes alive after iteration\n",
    "#             seed: Random seed for shuffling\n",
    "#         \"\"\"\n",
    "#         self.dataset = dataset\n",
    "#         self.batch_size = batch_size if batch_size is not None else 1\n",
    "#         self.shuffle = shuffle\n",
    "#         self.num_workers = num_workers\n",
    "#         self.collate_fn = collate_fn if collate_fn is not None else default_collate\n",
    "#         self.drop_last = drop_last\n",
    "#         self.pin_memory = pin_memory\n",
    "#         self.timeout = timeout\n",
    "#         self.worker_init_fn = worker_init_fn\n",
    "#         self.prefetch_factor = prefetch_factor\n",
    "#         self.persistent_workers = persistent_workers\n",
    "        \n",
    "#         # Set random seed for reproducibility\n",
    "#         self.seed = seed if seed is not None else torch.initial_seed()\n",
    "        \n",
    "#         # Calculate the number of samples and batches\n",
    "#         self.dataset_len = len(dataset)\n",
    "#         if self.drop_last:\n",
    "#             self.num_batches = self.dataset_len // self.batch_size\n",
    "#         else:\n",
    "#             self.num_batches = math.ceil(self.dataset_len / self.batch_size)\n",
    "        \n",
    "#         # Set up multiprocessing if using workers (simplified - would need actual implementation)\n",
    "#         self._setup_multiprocessing()\n",
    "    \n",
    "#     def _setup_multiprocessing(self):\n",
    "#         \"\"\"\n",
    "#         Set up multiprocessing for data loading.\n",
    "        \n",
    "#         Note: This is a simplified placeholder. A real implementation would need to use \n",
    "#         Python's multiprocessing library to spawn worker processes.\n",
    "#         \"\"\"\n",
    "#         # In a full implementation, this would set up worker processes using Python's multiprocessing\n",
    "#         self.workers = None\n",
    "#         if self.num_workers > 0:\n",
    "#             print(f\"Note: Using {self.num_workers} workers would require implementing multiprocessing\")\n",
    "#             # Setup worker processes, queues, etc.\n",
    "    \n",
    "    # def __iter__(self) -> Iterator[List[T_co]]:\n",
    "    #     \"\"\"\n",
    "    #     Create a new iterator over the dataset.\n",
    "        \n",
    "    #     Returns:\n",
    "    #         Iterator over batches of data\n",
    "    #     \"\"\"\n",
    "    #     # Create index sampler\n",
    "    #     if self.shuffle:\n",
    "    #         # Deterministically shuffle based on current epoch and seed\n",
    "    #         generator = torch.Generator()\n",
    "    #         generator.manual_seed(self.seed)\n",
    "    #         indices = torch.randperm(self.dataset_len, generator=generator).tolist()\n",
    "#         else:\n",
    "#             indices = list(range(self.dataset_len))\n",
    "        \n",
    "#         # Create batches\n",
    "#         batches = []\n",
    "#         for i in range(0, self.dataset_len, self.batch_size):\n",
    "#             batch_indices = indices[i:i + self.batch_size]\n",
    "#             if len(batch_indices) < self.batch_size and self.drop_last:\n",
    "#                 continue\n",
    "#             batches.append(batch_indices)\n",
    "        \n",
    "#         # For simplicity, this implementation loads data in the main process\n",
    "#         # A full implementation would distribute this to worker processes\n",
    "#         return _DataLoaderIter(self, batches)\n",
    "    \n",
    "#     def __len__(self) -> int:\n",
    "#         \"\"\"\n",
    "#         Return the number of batches in the dataset.\n",
    "        \n",
    "#         Returns:\n",
    "#             Number of batches\n",
    "#         \"\"\"\n",
    "#         return self.num_batches\n",
    "\n",
    "# class _DataLoaderIter:\n",
    "#     \"\"\"\n",
    "#     Iterator class for the DataLoader.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, loader: CustomDataLoader, batches: List[List[int]]):\n",
    "#         \"\"\"\n",
    "#         Initialize the iterator.\n",
    "        \n",
    "#         Args:\n",
    "#             loader: The DataLoader instance\n",
    "#             batches: List of batch indices\n",
    "#         \"\"\"\n",
    "#         self.loader = loader\n",
    "#         self.batches = batches\n",
    "#         self.current_batch = 0\n",
    "    \n",
    "#     def __iter__(self) -> '_DataLoaderIter':\n",
    "#         return self\n",
    "    \n",
    "#     def __next__(self) -> List[T_co]:\n",
    "#         if self.current_batch >= len(self.batches):\n",
    "#             raise StopIteration\n",
    "        \n",
    "#         # Get current batch indices\n",
    "#         batch_indices = self.batches[self.current_batch]\n",
    "#         self.current_batch += 1\n",
    "        \n",
    "#         # Load data for this batch\n",
    "#         batch_data = [self.loader.dataset[idx] for idx in batch_indices]\n",
    "        \n",
    "#         # Apply collation function\n",
    "#         batch = self.loader.collate_fn(batch_data)\n",
    "        \n",
    "#         # Pin memory if requested\n",
    "#         if self.loader.pin_memory:\n",
    "#             batch = _pin_memory(batch)\n",
    "        \n",
    "#         return batch\n",
    "    \n",
    "#     def __len__(self) -> int:\n",
    "#         return len(self.batches)\n",
    "\n",
    "# def _pin_memory(data):\n",
    "#     \"\"\"\n",
    "#     Pin memory for faster data transfer to GPU.\n",
    "    \n",
    "#     Args:\n",
    "#         data: Data to pin\n",
    "\n",
    "#     Returns:\n",
    "#         Pinned data\n",
    "#     \"\"\"\n",
    "#     if isinstance(data, torch.Tensor):\n",
    "#         return data.pin_memory()\n",
    "#     elif isinstance(data, Mapping):\n",
    "#         return {k: _pin_memory(v) for k, v in data.items()}\n",
    "#     elif isinstance(data, (tuple, list)):\n",
    "#         return [_pin_memory(x) for x in data]\n",
    "#     else:\n",
    "#         return data\n",
    "\n",
    "# def default_collate(batch):\n",
    "#     \"\"\"\n",
    "#     Default collation function that converts a list of samples to a batch.\n",
    "    \n",
    "#     Args:\n",
    "#         batch: List of samples\n",
    "\n",
    "#     Returns:\n",
    "#         Collated batch\n",
    "#     \"\"\"\n",
    "#     elem = batch[0]\n",
    "#     if isinstance(elem, torch.Tensor):\n",
    "#         return torch.stack(batch, 0)\n",
    "#     elif isinstance(elem, (str, bytes)):\n",
    "#         return batch\n",
    "#     elif isinstance(elem, Mapping):\n",
    "#         return {key: default_collate([d[key] for d in batch]) for key in elem}\n",
    "#     elif isinstance(elem, ABCSequence) and not isinstance(elem, (str, bytes)):\n",
    "#         transposed = zip(*batch)\n",
    "#         return [default_collate(samples) for samples in transposed]\n",
    "#     else:\n",
    "#         return batch\n",
    "\n",
    "# # Example usage:\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Create a simple tensor dataset\n",
    "#     class TensorDataset(torch.utils.data.Dataset):\n",
    "#         def __init__(self, data):\n",
    "#             self.data = data\n",
    "        \n",
    "#         def __getitem__(self, index):\n",
    "#             return self.data[index]\n",
    "        \n",
    "#         def __len__(self):\n",
    "#             return len(self.data)\n",
    "    \n",
    "#     # Create a dataset with 100 samples\n",
    "#     data = torch.randn(100, 5)\n",
    "#     dataset = TensorDataset(data)\n",
    "    \n",
    "#     # Create a DataLoader with batch_size=16 and shuffle=True\n",
    "#     dataloader = CustomDataLoader(dataset, batch_size=16, shuffle=True)\n",
    "    \n",
    "#     # Iterate through the DataLoader\n",
    "#     for batch in dataloader:\n",
    "#         print(f\"Batch shape: {batch.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
