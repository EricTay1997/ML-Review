{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c993df-8e18-4cae-821c-1cf3367f11d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "import matplotlib\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c720d0c1-3088-45ba-81d2-f72e6117942a",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cf936a-2442-4341-addf-0da38e0928a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6290f51-a60e-4cce-a548-0965e0e0eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall structure\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "        self.tok_emb.weight = self.out_head.weight # weight_tying, Raschka suggests that the model is easier to train without weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/((2 * cfg[\"n_layers\"]))**0.5)\n",
    "\n",
    "    def _init_weights(self, module, std = 0.02):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.dropout(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02286fd0-cca2-495a-a4b9-62e014c3386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fa112d9-76b9-49b0-b398-06d4d51d7a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be0dca59-7c83-4271-b5cc-e9d6cd6e1e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length, dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_out % num_heads == 0, \"embed_dim is indivisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.context_length = context_length\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, embed_dim = x.shape\n",
    "\n",
    "        # (b, num_tokens, embed_dim) --> (b, num_tokens, 3 * embed_dim)\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.view(batch_size, num_tokens, 3, self.num_heads, self.head_dim)\n",
    "        # (b, num_heads, num_tokens, head_dim)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        # (b, num_heads, num_tokens, num_tokens)\n",
    "        attn_scores = torch.einsum('bhij,bhkj->bhik', q, k)\n",
    "        attn_scores = attn_scores.masked_fill(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
    "        )\n",
    "        attn_weights = torch.softmax(attn_scores / k.shape[-1]**-0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        # (b, num_heads, num_tokens, head_dim)\n",
    "        context_vec = torch.einsum('bhij,bhjk->bhik', attn_weights, v)\n",
    "\n",
    "        # (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = context_vec.transpose(1, 2)\n",
    "        context_vec = context_vec.contiguous().view(batch_size, num_tokens, embed_dim)\n",
    "        context_vec = self.proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de8f575c-3d76-45c8-926d-018b78fc3343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP in Transformer Block\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"])\n",
    "        self.gelu = nn.GELU()\n",
    "        self.proj = nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(self.gelu(self.fc(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac092ad0-19f9-4827-8555-a9c160fd4989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch.shape)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3bc536f-602a-45ab-be7f-8d7a1a471c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[ 0.8852,  0.0329, -0.4911,  ...,  0.2467, -0.5240, -0.0369],\n",
      "        [-0.0906,  0.3250, -0.2124,  ..., -0.0765, -0.1040, -0.0471],\n",
      "        [-0.0909,  0.3702,  0.3636,  ...,  0.6508,  0.0633,  0.0326]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out[0,:3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100938e9-0463-4510-9d3f-8d5d7d650aba",
   "metadata": {},
   "source": [
    "# Load Pre-Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bc32a30-2499-4b52-ab50-9d5c1b846eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83d88190-40e5-45e8-85ce-e1b0b67ea058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db9b713c-dbb2-4815-9d18-e2cfef718bfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m     gpt\u001b[38;5;241m.\u001b[39mfinal_norm\u001b[38;5;241m.\u001b[39mshift \u001b[38;5;241m=\u001b[39m assign(gpt\u001b[38;5;241m.\u001b[39mfinal_norm\u001b[38;5;241m.\u001b[39mshift, params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     49\u001b[0m     gpt\u001b[38;5;241m.\u001b[39mout_head\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m assign(gpt\u001b[38;5;241m.\u001b[39mout_head\u001b[38;5;241m.\u001b[39mweight, params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwte\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 51\u001b[0m load_weights_into_gpt(gpt, params)\n\u001b[1;32m     52\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m gpt\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'params' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        gpt.trf_blocks[b].att.qkv.weight = assign(\n",
    "            gpt.trf_blocks[b].att.qkv.weight, \n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.qkv.bias = assign(\n",
    "            gpt.trf_blocks[b].att.qkv.bias, \n",
    "            ((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"]))\n",
    "        gpt.trf_blocks[b].att.proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.fc.weight = assign(\n",
    "            gpt.trf_blocks[b].ff.fc.weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.fc.bias = assign(\n",
    "            gpt.trf_blocks[b].ff.fc.bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.proj.weight = assign(\n",
    "            gpt.trf_blocks[b].ff.proj.weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.proj.bias = assign(\n",
    "            gpt.trf_blocks[b].ff.proj.bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8bc31be9-d978-4ff7-a950-a73ac6381392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/Users/erict/Desktop/ML-Review/dl/17_nlp/raschka_files')\n",
    "from gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "44f85b7c-aed1-4484-8b54-2a68e703b727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|███████████████████████████████████████████████████████████| 77.0/77.0 [00:00<00:00, 45.1kiB/s]\n",
      "encoder.json: 100%|███████████████████████████████████████████████████████| 1.04M/1.04M [00:00<00:00, 3.17MiB/s]\n",
      "hparams.json: 100%|█████████████████████████████████████████████████████████| 90.0/90.0 [00:00<00:00, 52.1kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|███████████████████████████████████████| 498M/498M [00:23<00:00, 21.2MiB/s]\n",
      "model.ckpt.index: 100%|███████████████████████████████████████████████████| 5.21k/5.21k [00:00<00:00, 1.42MiB/s]\n",
      "model.ckpt.meta: 100%|██████████████████████████████████████████████████████| 471k/471k [00:00<00:00, 1.58MiB/s]\n",
      "vocab.bpe: 100%|████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 1.89MiB/s]\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d0f91a1a-da9e-4b1d-be0a-a434370d75c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "device = torch.device(\"mps\") if torch.mps.is_available() else torch.device(\"cpu\")\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4c1999-a871-4a81-a46d-92f9c41074c5",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb575e7-2bca-46e8-a15a-ebfaa6e71b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for top_p, taken from https://github.com/johnma2006/candle/blob/main/candle/nlp/generation.py\n",
    "def nucleus_sample(probs: np.array,\n",
    "                   top_p: int):\n",
    "    \"\"\"Nucleus sampling. Filter to top probs such that the sum prob is just less than top_p.\n",
    "    \n",
    "    References:\n",
    "        [1] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi.\n",
    "            The Curious Case of Neural Text Degeneration. arXiv:1904.09751, 2019\n",
    "    \n",
    "    Args:\n",
    "        probs (np.array): Array of probabilities with shape (vocab_size, batch).\n",
    "            Modifies probs in place.\n",
    "        top_p (int): Filter to the top `k` probs such that the sum probs is <= top_p and k is largest.\n",
    "    \n",
    "    \"\"\"\n",
    "    sorted_probs = np.sort(probs, axis=0)[::-1]\n",
    "    cum_probs = sorted_probs.cumsum(axis=0)\n",
    "    top_k = (cum_probs <= top_p).sum(axis=0)\n",
    "\n",
    "    ranking = probs.shape[0] - np.argsort(np.argsort(probs, axis=0), axis=0)\n",
    "    mask = (ranking <= top_k) | (ranking == 1)  # | (ranking == 1) accounts for when the edge case if highest prob > top_p\n",
    "\n",
    "    probs[~mask] = 0\n",
    "    probs /= probs.sum(axis=0)\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d60cb5f9-8d82-47f2-bce4-e79285a692dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, token_ids, max_new_tokens, context_size, temperature = 1, top_k = None, eos_id = None):\n",
    "    # idx has shape (batch, n_tokens) \n",
    "    # We will continually append to idx\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop current context to supported context size\n",
    "        idx_cond = token_ids[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        # (batch, n_tokens, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "        if top_k is not None: # relevant when temperature is high\n",
    "            # Returns the top k for last dim, in sorted order\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            # this is threshold\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "        logits /= temperature\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples = 1)  # (batch, 1)\n",
    "        if next_token == eos_id:\n",
    "            break\n",
    "        token_ids = torch.cat((token_ids, next_token), dim=1)  # (batch, n_tokens+1)\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c4eaca45-28d2-4f84-9b3c-bc9558f510ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.mps.manual_seed(seed)\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "357ac29a-f471-49c9-bc30-2cc544c9022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    token_ids=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
