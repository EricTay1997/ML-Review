{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2e2b571e-5185-482b-9c6f-0d2d827a7e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "from typing import Sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180b54f7-12d5-4adb-8029-99003091ebbd",
   "metadata": {},
   "source": [
    "# Basic Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "66ddc4df-b782-4839-9436-9365a4935203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XORDataset(data.Dataset):\n",
    "    def __init__(self, size, std = 0.1):\n",
    "        self.size = size\n",
    "        self.std = std\n",
    "        self.data = None\n",
    "        self.label = None\n",
    "        self.generate_data_points()\n",
    "\n",
    "    def generate_data_points(self):\n",
    "        xs = torch.randint(0, 2, (self.size, 2), dtype = torch.float32)\n",
    "        ys = (xs.sum(dim = 1) == 1).to(torch.float32)\n",
    "        xs += (self.std * torch.randn_like(xs))\n",
    "        self.data = xs\n",
    "        self.label = ys\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "94955fe6-0af8-48d0-8cd2-73020d561550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataloader:\n",
    "    def __init__(self, dataset, batch_size, shuffle = True, collate_fn = None):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.collate_fn = collate_fn if collate_fn else default_collate\n",
    "        self.dataset_len = len(dataset)\n",
    "        self.num_batches = math.ceil(self.dataset_len / batch_size)\n",
    "        self.batches = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Resample\n",
    "        if self.shuffle:\n",
    "            indices = torch.randperm(self.dataset_len)\n",
    "        else:\n",
    "            indices = torch.arange(self.dataset_len)\n",
    "        batches = []\n",
    "        for i in range(0, self.dataset_len, self.batch_size):\n",
    "            batches.append(indices[i : i + self.batch_size])\n",
    "        self.batches = batches\n",
    "        return _DataIterator(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "class _DataIterator:\n",
    "    def __init__(self, loader):\n",
    "        self.loader = loader\n",
    "        self.current_batch = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_batch == len(self.loader.batches):\n",
    "            raise StopIteration\n",
    "        batch_indices = self.loader.batches[self.current_batch]\n",
    "        batch_data = [self.loader.dataset[i] for i in batch_indices]\n",
    "        batch = self.loader.collate_fn(batch_data)\n",
    "        self.current_batch += 1\n",
    "        return batch\n",
    "\n",
    "def default_collate(batch_data): \n",
    "    elem = batch_data[0]\n",
    "    if isinstance(elem, torch.Tensor):\n",
    "        return torch.stack(batch_data, 0)\n",
    "    elif isinstance(elem, Sequence):\n",
    "        transposed = zip(*batch_data)\n",
    "        return [default_collate(x) for x in transposed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ce959c37-1615-47dd-af68-015ee7a2d6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden, num_classes, act_fn = nn.Tanh()):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.linear2 = nn.Linear(num_hidden, num_classes)\n",
    "        self.act_fn = act_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9219bb14-ae95-44bc-ad92-29b52e2e38a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.mps.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "de586a19-14b9-4c74-909e-a2a4c9586524",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "50.32\n",
      "48.44\n",
      "44.04\n",
      "48.4\n",
      "49.52\n",
      "47.92\n",
      "47.8\n",
      "49.04\n",
      "50.8\n",
      "50.32\n",
      "50.32\n",
      "50.96\n",
      "53.76\n",
      "51.08\n",
      "51.6\n",
      "50.84\n",
      "52.04\n",
      "55.28\n",
      "50.76\n",
      "52.08\n",
      "50.76\n",
      "50.76\n",
      "50.8\n",
      "50.88\n",
      "51.04\n",
      "51.0\n",
      "53.68\n",
      "56.08\n",
      "64.08\n",
      "75.52\n",
      "89.12\n",
      "97.04\n",
      "99.36\n",
      "99.84\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "set_seed(42)\n",
    "train_dataset = XORDataset(2500)\n",
    "train_data_loader = CustomDataloader(train_dataset, batch_size = 128, shuffle = True)\n",
    "model = LinearModel(2,4,1)\n",
    "device = torch.device('mps') if torch.mps.is_available else torch.device('cpu')\n",
    "print(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss() # one class\n",
    "model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    sum_correct = 0\n",
    "    sum_data_points = 0\n",
    "    for data_input, data_label in train_data_loader:\n",
    "        data_input = data_input.to(device)\n",
    "        data_label = data_label.to(device)\n",
    "        pred = model(data_input).flatten()\n",
    "        loss = loss_fn(pred, data_label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sum_data_points += len(data_input)\n",
    "        sum_correct += ((pred > 0) == data_label).sum()\n",
    "    print(np.round((100*sum_correct/sum_data_points).cpu().item(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c32307-2b62-4f06-96e9-17a3a889a611",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "05d9d53f-cf9a-42e4-91a8-5e1bfdf97c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = self.__class__.__name__\n",
    "\n",
    "class Identity(Activation):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class ReLU(Activation):\n",
    "    def forward(self, x):\n",
    "        return x * (x > 0)\n",
    "\n",
    "class Tanh(Activation):\n",
    "    def forward(self, x):\n",
    "        exp_x = torch.exp(x)\n",
    "        exp_neg_x = torch.exp(-x)\n",
    "        return (exp_x - exp_neg_x)/(exp_x + exp_neg_x)\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    def forward(self, x):\n",
    "        return 1/(1 + torch.exp(-x))\n",
    "\n",
    "class SiLU(Activation):\n",
    "    def forward(self, x):\n",
    "        return x/(1 + torch.exp(-x))\n",
    "\n",
    "class SoftPlus(Activation):\n",
    "    def forward(self, x):\n",
    "        return torch.log(1 + nptorchexp(x))\n",
    "\n",
    "class ELU(Activation):\n",
    "    def __init__(self, alpha = 1):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "    def forward(self, x):\n",
    "        return torch.where(x < 0, alpha*(torch.exp(x)-1), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3156cca7-7b3b-4a93-a74d-ca116086ab13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affc82ac-2bf4-41ad-bf08-a9597b27a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden, num_classes, act_fn = nn.Tanh()):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.linear2 = nn.Linear(num_hidden, num_classes)\n",
    "        self.act_fn = act_fn\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if name.endswith('bias'):\n",
    "                nn.init.zeros_(param)\n",
    "            else:\n",
    "                nn.init.normal_(param, std = np.sqrt(2 / param.shape[1])) # kaiming\n",
    "                nn.init.normal_(param, std = np.sqrt(2 / (param.shape[0] + param.shape[1]))) # kaiming\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4830eb8e-7780-4749-9f4c-f8a66b1035a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "086714fd-bfc3-4eac-8d24-395cc87efb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerTemplate:\n",
    "    def __init__(self, params, lr):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.detach_()\n",
    "                p.grad.zero_()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                self.update_param(p)\n",
    "\n",
    "    def update_param(self, p):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cc95601b-0eab-4b9a-a075-eecc425078d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(OptimizerTemplate):\n",
    "    def update_param(self, p):\n",
    "        p_update = -self.lr*p.grad\n",
    "        p._add(p_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7f23011a-9a4c-4e65-8a8a-f38e72c7e149",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDM(OptimizerTemplate):\n",
    "    def __init__(self, params, lr, momentum = 0.0):\n",
    "        super().__init__()\n",
    "        self.beta1 = momentum\n",
    "        self.p_to_momentum = {p : torch.zeros_like(p.data) for p in params}\n",
    "    \n",
    "    def update_param(self, p):\n",
    "        self.p_to_momentum[p] = p.grad + self.beta1*self.p_to_momentum[p]\n",
    "        p_update = -self.lr*self.p_to_momentum[p]\n",
    "        p._add(p_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b68f1f-191e-4919-b208-00fb919a830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad(OptimizerTemplate):\n",
    "    def __init__(self, params, lr, epsilon = 1e-8):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.p_to_sq_grad = {p : torch.zeros_like(p.data) for p in params}\n",
    "\n",
    "    def update_params(self, p):\n",
    "        self.p_to_sq_grad[p].add_(p.grad**2)\n",
    "        p_update = -self.lr*p.grad/torch.sqrt(self.p_to_sq_grad[p] + self.epsilon)\n",
    "        p._add(p_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6f61fc-0d8a-4758-9c8b-4ca6506a3343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp(OptimizerTemplate):\n",
    "    def __init__(self, params, lr, epsilon = 1e-8, beta2 = 0.999):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.beta2 = beta2\n",
    "        self.p_to_delta = {p : torch.zeros_like(p.data) for p in params}\n",
    "        self.p_to_sq_grad = {p : torch.zeros_like(p.data) for p in params}\n",
    "\n",
    "    def update_params(self, p):\n",
    "        self.p_to_sq_grad[p] = self.beta2*self.p_to_sq_grad[p] + (1-self.beta2)*(p.grad**2)\n",
    "        ada_lr = torch.sqrt(self.p_to_delta[p] + self.epsilon)\n",
    "        p_update = -ada_lr*p.grad/torch.sqrt(self.p_to_sq_grad[p] + self.epsilon)\n",
    "        p._add(p_update)\n",
    "        self.p_to_delta[p] = self.beta2*self.p_to_delta[p] + (1-self.beta2)*(p_update**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288ee2c0-8d0a-47c4-b34c-66ac17fab6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaDelta(OptimizerTemplate):\n",
    "    def __init__(self, params, lr, epsilon = 1e-8, beta2 = 0.999):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.beta2 = beta2\n",
    "        self.p_to_sq_grad = {p : torch.zeros_like(p.data) for p in params}\n",
    "\n",
    "    def update_params(self, p):\n",
    "        self.p_to_sq_grad[p] = self.beta2*self.p_to_sq_grad[p] + (1-self.beta2)*(p.grad**2)\n",
    "        p_update = -self.lr*p.grad/torch.sqrt(self.p_to_sq_grad[p] + self.epsilon)\n",
    "        p._add(p_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb77373-0a1f-43d6-8eee-f14c8b1b9e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(OptimizerTemplate):\n",
    "    def __init__(self, params, lr, epsilon = 1e-8, beta1 = 0.9, beta2 = 0.999):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.p_to_num_updates = {p : 0 for p in params}\n",
    "        self.p_to_mom = {p : torch.zeros_like(p.data) for p in params}\n",
    "        self.p_to_sq_grad = {p : torch.zeros_like(p.data) for p in params}\n",
    "\n",
    "    def update_params(self, p):\n",
    "        self.p_to_mom[p] = self.beta1*self.p_to_mom[p] + (1-self.beta1)*(p.grad)\n",
    "        self.p_to_sq_grad[p] = self.beta2*self.p_to_sq_grad[p] + (1-self.beta2)*(p.grad**2)\n",
    "        self.p_to_num_updates[p] += 1\n",
    "        mom = self.p_to_mom[p] / (1 - self.beta1**self.p_to_num_updates[p])\n",
    "        sq_grad = self.p_to_sq_grad[p] / (1 - self.beta2**self.p_to_num_updates[p])\n",
    "        p_update = -self.lr*mom/(torch.sqrt(sq_grad) + self.epsilon)\n",
    "        p._add(p_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b6dac-ab97-412b-b543-bad95c1ef27a",
   "metadata": {},
   "source": [
    "# U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2191786b-875c-4544-9213-151304bfe769",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels = 3,\n",
    "        output_channels = 3,\n",
    "        base_channels = 64,\n",
    "        channel_multipliers = [1,2,4]\n",
    "    ):\n",
    "        super().__init__()\n",
    "        levels = len(channel_multipliers)\n",
    "        channels_list = [m*base_channels for m in channel_multipliers]\n",
    "        input_layers = []\n",
    "        output_layers = []\n",
    "        for i in range(len(channels_list)):\n",
    "            input_channel_num = input_channels if i == 0 else channels_list[i-1]\n",
    "            input_layers.append(nn.Conv2d(input_channel_num, channels_list[i], kernel = 3, padding = 1))\n",
    "            # Downsample\n",
    "            input_layers.append(nn.Conv2d(channels_list[i], channels_list[i], kernel = 3, padding = 1, stride = 2))\n",
    "        for i in reverse(range(len(channels_list))):\n",
    "            output_channel_num = output_channels if i == 0 else channels_list[i-1]\n",
    "            output_layers.append(nn.Conv2d(channels_list[i]*2, output_channel_num, kernel = 3, padding = 1))\n",
    "        self.input_blocks = nn.Sequential(*input_layers)\n",
    "        self.middle_blocks = nn.Conv2d(channels_list[-1], channels_list[-1], kernel = 1, padding = 1)\n",
    "        self.output_blocks = nn.Sequential(*output_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_blocks = []\n",
    "        for block in self.input_blocks:\n",
    "            x = block(x)\n",
    "            input_blocks.append(x)\n",
    "        x = self.middle_blocks(x)\n",
    "        for block in self.output_blocks:\n",
    "            x = F.interpolate(x, scale_factor = 2)\n",
    "            x = torch.cat([x, input_blocks.pop()], dim = 1)\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d21e62-e378-42ba-b647-3bbd57036552",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "acb996ae-5cd9-459f-97ae-7ba75c13e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOE(nn.Module):\n",
    "    def __init__(self, embed_dim, num_experts, topk):\n",
    "        self.num_experts = num_experts\n",
    "        self.topk = topk\n",
    "        self.router = nn.Linear(embed_dim, num_experts)\n",
    "        self.noise_router = nn.Linear(embed_dim, num_experts)\n",
    "        self.experts = [nn.Linear(embed_dim, embed_dim) for _ in range(num_experts)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, seq_len, embed_dim = x.shape\n",
    "        num_tokens = b*seq_len\n",
    "        logits = self.router(x)\n",
    "        noise_logits = self.noise_router(x)\n",
    "        noise = torch.randn_like(x) * torch.softmax(noise_logits, dim = -1)\n",
    "        logits += noise\n",
    "        values, indices = torch.topk(logits, self.topk, dim = -1)\n",
    "        masked_logits = torch.full_like(logits, -torch.inf)\n",
    "        masked_logits = masked_logits.scatter(-1, indices, values)\n",
    "        probs = torch.softmax(masked_logits, dim = -1).reshape(num_tokens, embed_dim)\n",
    "        x = x.reshape(num_tokens, embed_dim)\n",
    "        output = torch.zeros_like(x)\n",
    "        for i in range(self.num_experts):\n",
    "            expert_mask = (indices == i).any(dim = 1).flatten()\n",
    "            expert = self.experts[i]\n",
    "            if expert_mask.any():\n",
    "                filter_x = x[expert_mask]\n",
    "                filter_x = expert(x)\n",
    "                filter_probs = probs[expert_mask, i][:, None]\n",
    "                filter_x *= filter_probs\n",
    "                output[expert_mask] += filter_x\n",
    "        return output.reshape(b, seq_len, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee61c6-59e6-457c-8875-21c95f0e70ee",
   "metadata": {},
   "source": [
    "# Dot Product Attention / Multi-Head Self Attention / Cross-Attention / Grouped Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a6f807d9-5f0d-4938-b28f-689808135d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask = None):\n",
    "    d_k = k.shape[-1]\n",
    "    attention = torch.einsum('bhij,bhkj->bhik', q, k)\n",
    "    attention /= (d_k**0.5)\n",
    "    if mask is not None:\n",
    "        attention = attention.masked_fill(mask == 0, -torch.inf)\n",
    "    attention = torch.softmax(attention, dim = -1)\n",
    "    values_added = torch.einsum('bhij,bhjk->bhik', attention, v)\n",
    "    return values_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d574364-2987-4f0e-b482-2bda32c3fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, max_context_length, dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim is indivisible by num_heads\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.qkv = nn.Linear(embed_dim, 3*embed_dim, bias = qkv_bias)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'causal_mask', torch.triu(torch.ones(max_context_length, max_context_length), diagonal = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        # q is of shape b, h, n, d\n",
    "        q, k, v = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        attention = torch.einsum('bhij,bhkj->bhik', q, k)\n",
    "        attention /= (self.head_dim ** 0.5)\n",
    "        attention = attention.masked_fill(self.causal_mask[:seq_len,:seq_len] == 1, -torch.inf)\n",
    "        attention = torch.softmax(attention, dim = -1)\n",
    "        attention = self.dropout(attention)\n",
    "        values_added = torch.einsum('bhij,bhjk->bhik', attention, v)\n",
    "        values_added = values_added.transpose(1,2).reshape(batch_size, seq_len, embed_dim)\n",
    "        values_added = self.o_proj(values_added)\n",
    "        return values_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceb06b0-d316-40dd-9070-3aebaeaec674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, cross_dim, num_heads, dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim is indivisible by num_heads\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias = qkv_bias)\n",
    "        self.k_proj = nn.Linear(cross_dim, embed_dim, bias = qkv_bias)\n",
    "        self.v_proj = nn.Linear(cross_dim, embed_dim, bias = qkv_bias)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        attention = torch.einsum('bhij,bhkj->bhik', q, k)\n",
    "        attention /= (self.head_dim ** 0.5)\n",
    "        attention = torch.softmax(attention, dim = -1)\n",
    "        attention = self.dropout(attention)\n",
    "        values_added = torch.einsum('bhij,bhjk->bhik', attention, v)\n",
    "        values_added = values_added.transpose(1,2).reshape(batch_size, seq_len, embed_dim)\n",
    "        values_added = self.o_proj(values_added)\n",
    "        return values_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f363c100-1f4c-4a89-a90c-67fe92e11928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_kv_heads, max_context_length, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim is indivisible by num_heads\"\n",
    "        assert num_heads % num_kv_heads == 0, \"num_heads is indivisible by num_kv_heads\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.q_to_k_ratio = num_heads // num_kv_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim + self.head_dim*num_kv_heads, bias = qkv_bias)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.register_mask(\n",
    "            \"causal_mask\", torch.triu(torch.ones(max_context_length, max_context_length), diagonal = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        q = qkv[:,:,:embed_dim].reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        k, v = qkv[:,:,embed_dim:].reshape(batch_size, seq_len, 2, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        k = k.repeat_interleave(self.q_to_k_ratio, dim = 1)\n",
    "        v = v.repeat_interleave(self.q_to_k_ratio, dim = 1)\n",
    "        attention = torch.einsum('bhij,bhkj->bhik', q, k)\n",
    "        attention /= (self.head_dim ** 0.5)\n",
    "        attention = attention.masked_fill(self.causal_mask[:seq_len,:seq_len] == 1, -torch.inf)\n",
    "        attention = torch.softmax(attention, dim = -1)\n",
    "        attention = self.dropout(attention)\n",
    "        delta_x = torch.einsum('bhij,bhjk->bhik', attention, v)\n",
    "        delta_x = delta_x.transpose(1,2).reshape(batch_size, seq_len, embed_dim)\n",
    "        delta_x = self.o_proj(delta_x)\n",
    "        return values_added"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae309a88-835f-4006-82c6-8b872707b437",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6e79e6d5-4023-4b21-ad21-c337039bc5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False,      # Query-Key-Value bias\n",
    "    \"std\": 0.02\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d640c-ce79-4249-b1fc-f03286b620ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super()._init__()\n",
    "        self.tok_embedding = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_embedding = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.final_layer = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"])\n",
    "        self.tok_embedding.weight = self.final_layer.weight\n",
    "\n",
    "        # init\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('proj.weight'):\n",
    "                nn.init.normal_(p, std = cfg[\"std\"]/((2*cfg[\"n_layers\"])**0.5))\n",
    "            elif pn.endswith('weight'):\n",
    "                nn.init.normal_(p, std = cfg[\"std\"])\n",
    "            elif pn.endswith('bias'):\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "    def forward(self, x): \n",
    "        batch_size, seq_len = x.shape\n",
    "        tok_embed = self.tok_embedding(x)\n",
    "        pos_embed = self.pos_embedding(x)\n",
    "        x = tok_embed + pos_embed\n",
    "        x = self.dropout(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.final_linear(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "81f1e544-3f83-4abd-8145-196a9c07ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embed_dim, eps = 1e-5):\n",
    "        super()._init__()\n",
    "        self.shift = nn.Parameter(torch.zeros(embed_dim))\n",
    "        self.scale = nn.Parameter(torch.ones(embed_dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True, unbiased = False)\n",
    "        norm_x = (x-mean) / torch.sqrt(var + self.eps)\n",
    "        return norm_x*self.scale + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f191f0bb-4aa2-46e9-a0e5-3f2856f9c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(cfg)\n",
    "        self.ffn = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"embed_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"embed_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, cfg):\n",
    "        orig = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.dropout(x)\n",
    "        x += orig\n",
    "\n",
    "        orig = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x += orig\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc7b23c-598e-49f4-afa6-7278f83bffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False,      # Query-Key-Value bias\n",
    "    \"std\": 0.02\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c80867-9865-4ac2-a189-a4a7429aa6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        assert cfg[\"emb_dim\"] % cfg[\"n_heads\"] == 0, \"emb_dim is indivisible by n_heads\"\n",
    "        self.n_heads = cfg[\"n_heads\"]\n",
    "        self.head_dim = cfg[\"emb_dim\"] // cfg[\"n_heads\"]\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.qkv = nn.Linear(cfg[\"emb_dim\"], 3*cfg[\"emb_dim\"], bias = cfg[\"qkv_bias\"])\n",
    "        self.o_proj = nn.Linear(cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        self.register_buffer(\n",
    "            \"causal_mask\", torch.triu(torch.ones(cfg[\"context_length\"], cfg[\"context_length\"]), diagonal = 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.reshape(batch_size, seq_len, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        attn = torch.einsum('bhij,bhkj->bhik', q, k)\n",
    "        attn /= (k.shape[-1]**0.5)\n",
    "        attn = attn.masked_fill(self.causal_mask == 1, -torch.inf)\n",
    "        attn = torch.softmax(attn, dim = -1)\n",
    "        attn = self.dropout(attn)\n",
    "        delta_x = torch.einsum('bhij,bhjk->bhik', attn, v)\n",
    "        delta_x = delta_x.transpose(1,2).reshape(batch_size, seq_len, embed_dim)\n",
    "        delta_x = self.o_proj(delta_x)\n",
    "        return delta_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0dd594-6606-4479-81a9-bc7209c11285",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"])\n",
    "        self.gelu = nn.GELU()\n",
    "        self.proj = nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329a0ed7-7ed5-410d-bc94-1cfa38c608b0",
   "metadata": {},
   "source": [
    "## GELU/SwiGLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b84c308e-fe5c-4643-9912-bb3f71e8cd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"])\n",
    "        self.linear2 = nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"])\n",
    "        self.silu = SiLU()\n",
    "        self.proj = nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.linear1(x)\n",
    "        x_fc2 = self.linear1(x)\n",
    "        x = self.silu(x_fc1) * x_fc2\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb7980-dbf1-4b69-843c-5aff357a86d6",
   "metadata": {},
   "source": [
    "## RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4752048-7d5b-4bd7-a52f-a308d9702f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, embed_dim, eps = 1e-05):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt((x**2).mean(dim = -1, keepdim = True) + self.eps)\n",
    "        return (x/rms)*self.scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d942f-621b-4a7e-8929-382f1e31e8aa",
   "metadata": {},
   "source": [
    "## Pre/Post Layer Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "33e99157-5a79-4423-bf8d-8ebda8e40bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttn(cfg)\n",
    "        self.ffn = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward_pre(self, x): \n",
    "        orig_x = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.dropout(x)\n",
    "        x += orig_x\n",
    "\n",
    "        orig_x = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x += orig_x\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_post(self, x):\n",
    "        orig_x = x\n",
    "        x = self.att(x)\n",
    "        x = self.dropout(x)\n",
    "        x += orig_x\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        orig_x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x += orig_x\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c48d594-da44-4d4a-bdd9-975c6737ba1d",
   "metadata": {},
   "source": [
    "## PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49755f0a-c117-430c-83c5-f081725b378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sinusoidal_pos_emb(ctx_len, emb_dim):\n",
    "    pe = torch.zeros(ctx_len, emb_dim)\n",
    "    position = torch.arange(ctx_len, dtype = torch.float)[:, None]\n",
    "    div_term = torch.tensor(10000.0).pow(torch.arange(0, emd_dim, 2).float()/embed_dim)[None, :]\n",
    "    pe[:, 0::2] = torch.sin(position / div_term)\n",
    "    pe[:, 0::2] = torch.cos(position / div_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "cf3296f8-18fe-4062-aecc-650401c81364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(10000.0).pow(torch.arange(0, 5, 2).float()/5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8748f9f-2e36-4713-802b-5528dd110913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
