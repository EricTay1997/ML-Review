# A Broad Review of ML Topics
This repository will be used to help me consolidate my knowledge regarding various ML topics.

This repository will include: 
* Notes on various topics (Edit: Note that markdown on Github isn't formatting the same as my local IDE, PyCharm, which may cause readability issues!)
  * Classical (Non-DL) ML/Statistics
    * Linear Algebra and Calculus
    * Probability and Information Theory
    * Statistical Learning Theory
    * Statistical Testing and Metrics
    * Bayesian Statistics
    * Linear Regression & Regularization
    * Naive Bayes & Logistic Regression & GLMs
    * SVMs
    * Decision Trees
    * Ensemble Learning, Random Forests and Boosting
    * Dimensionality Reduction
    * Unsupervised Clustering
    * Gaussian Process
    * Causal Inference
    * ARIMA
  * DL (Note that some topics may bleed into each category)
    * Basics
    * Activation Functions
    * Initialization
    * Optimization and Regularization
    * Coding Practices
    * CNNs
    * RNNs
    * Attention & Transformers
    * Autoencoders
    * Diffusion Models
    * Flow-based Models
    * Generative Adversarial Networks
    * Graph Neural Networks
    * Meta-Learning
    * Self-Supervised Contrastive Learning
    * Computer Vision
    * Natural Language Processing
    * Reinforcement Learning
    * Audio 
    * Video
    * Multimodal
    * Post Training
    * AI Safety
    * Hyperparameter Optimization
    * Computational Performance
    * Personal Projects
    * Misc
* Code implementations for various algorithms, which will mostly come from online resources/tutorials. 
  * The first priority would be to fulfill learning goals.
  * If time permits, a stretch goal would be to refactor the code with a greater emphasis on OOP, e.g. [John's repo](https://github.com/johnma2006/candle/tree/main)
  * Code currently includes:
    * From scratch implementations, including BERT, GPT-2, Llama 2-3.2, DDPM, Real-NVP.
    * Post-training experiments, including (LoRA) fine-tuning and DPO. 
    * Data and model parallelism, with and without JAX (+FLAX).
* Interview Preparation

I shall try to be diligent in citing my sources. Due to visa-related time constraints, I do apologize for any lapses in citation. At the current moment (1/10/25), I have pulled most heavily from the following sources:
* [UvA Deep Learning Tutorials by Lippe](https://uvadlc-notebooks.readthedocs.io/en/latest/)
* Notes / Code from classes I took at Duke.
* [Hands-On Machine Learning by Aurélien Géron](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1098125975)
* [Dive into Deep Learning by Zhang, Lipton, Li and Smola](http://d2l.ai)
* [Build a Large Language Model (From Scratch) by Raschka](https://github.com/rasbt/LLMs-from-scratch)
* [Hugging Face's Diffusion Course](https://huggingface.co/learn/diffusion-course/unit0/1)
* [Deep Learning by Goodfellow, Bengio and Courville](https://www.deeplearningbook.org)
* [Designing Machine Learning Systems by Huyen](https://www.amazon.com/Designing-Machine-Learning-Systems-Production-Ready/dp/1098107969)
* [Machine Learning System Design Interview by Aminian and Xu](https://bytebytego.com/intro/machine-learning-system-design-interview)
* [Machine Learning System Design Interview by Pham](https://www.amazon.com/Machine-Learning-Design-Interview-System/dp/B09YQWX59Z)
* [System Design Interview by Xu](https://www.amazon.com/System-Design-Interview-insiders-Second/dp/B08CMF2CQF)
* [Anthropic's Research](https://www.anthropic.com/research)
* [Lilian Weng's Blog](https://lilianweng.github.io)
