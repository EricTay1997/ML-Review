# Bayesian Statistics

## Frequentist vs Bayesian interpretation
- A frequentist perspective is that the true parameter value $\pmb{\theta}$ is fixed but unknown, while the point estimate $\hat{\pmb{\theta}}$ is a RV since it is a function of the dataset (which is seen as random).
- The Bayesian perspective is that the data is directly observed and so is not random. On the other hand, the true parameter $\pmb{\theta}$ is unknown/uncertain and thus is represented as a random variable. 
  - Ultimately, the Bayesian framework uses the **likelihood** of the data to update the **prior** distribution, obtaining the **posterior** distribution. 
  - Now, while a frequentist may address the uncertainty of $\hat{\pmb{\theta}}$ by calculating its variance, a Bayesian statistician can now reference this posterior distribution, which can of course be used to calculate variance.

## Common Probability Distributions

### Discrete

| Distribution      | Support                                     | $\pmb{\theta}$ | $P(X = x)$                                                     | Mean               | Variance             | Interpretation                                                                                                                                   | $\pmb{\hat{\theta}}_{MLE}$ | $\mathcal{I}(\pmb{\theta})$ | $M_X(t)$                                |
|-------------------|---------------------------------------------|----------------|----------------------------------------------------------------|--------------------|----------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------|-----------------------------|-----------------------------------------|
| Bernoulli         | {0,1}                                       | $p$            | $p^x(1-p)^{1-x}$                                               | $p$                | $p(1-p)$             | Outcome of 1 coin flip.                                                                                                                          | $\bar{X}$                  | $\frac{1}{p(1-p)}$          | $(1-p)+pe^t$                            |
| Binomial          | {$0,1,\ldots,n$}                            | $p$            | $\binom{n}{x} p^x(1-p)^{n-x}$                                  | $np$               | $np(1-p)$            | Number of heads after $n$ coin flips.                                                                                                            | $\frac{\bar{X}}{n}$        | $\frac{1}{p(1-p)}$          | $((1-p)+pe^t)^n$                        |
| Negative Binomial | {$0,1,\ldots,\infty$}                       | $p$            | $\binom{x+r-1}{x} p^r(1-p)^{x}$                                | $\frac{r(1-p)}{p}$ | $\frac{r(1-p)}{p^2}$ | Number of failures before $r$ successes.                                                                                                         |                            | $\frac{r}{p^2(1-p)}$        | $\left( \frac{p}{1-(1-p)e^t} \right)^r$ |
| Multinomial       | $x_i \in\{0,1,2, \ldots, n\}, \sum_i x_i=n$ | $\mathbf{p}$   | $\frac{n!}{x_{1}!x_{2}!\ldots x_{k}!} \prod_{i=1}^k p_i^{x_i}$ |                    |                      | $x_i$ represents the number of items in the the $i^{th}$ category after $n$ draws from $k$ categories.                                           |                            |                             |                                         |
| Poisson           | {$0,1,\ldots,\infty$}                       | $\lambda$      | $e^{-\lambda}\frac{\lambda^x}{x!} $                            | $\lambda$          | $\lambda$            | The number of events to occur in a fixed amount of time. If this number $\sim Pois(\lambda t)$, then the inter-arrival times $\sim Exp(\lambda)$ | $\bar{X}$                  | $\frac{1}{\lambda}$         | $\exp(\lambda(e^t-1))$                  |
| Geometric         | {$1,2,\ldots,\infty$}                       | $p$            | $(1-p)^{x-1}p$                                                 | $\frac{1}{p}$      | $\frac{1-p}{p^2}$    | The number of trials to get one success.                                                                                                         | $\frac{n}{\sum X_i}$       | $\frac{1}{p^2(1-p)}$        | $\frac{p e^{t}}{1-(1-p) e^{t}}$         |

### Continuous

| Distribution        | Range                                  | $\pmb{\theta}$      | $p(x) $                                                                                                                                                                | Mean                            | Variance                                               | Interpretation                                                                                                                                                                                                  | $\pmb{\hat{\theta}}_{MLE}$                                                                                                                    | $\mathcal{I}(\pmb{\theta})$                                                                  | $M_X(t)$                                                                                        |
|---------------------|----------------------------------------|---------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------|--------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|
| Uniform             | $[a,b]$                                | $a,b$               | $\frac{1}{b-a}$                                                                                                                                                        | $\frac{a+b}{2}$                 | $\frac{(b-a)^2}{12}$                                   |                                                                                                                                                                                                                 | $\min(X_i)$, $\max(X_i)$                                                                                                                      |                                                                                              | $\frac{e^{ta}-e^{ta}}{b-a}$                                                                     |
| Exponential         | $[0,\infty)$                           | $\lambda$           | $\lambda e^{-\lambda x}$                                                                                                                                               | $\frac{1}{\lambda}$             | $\frac{1}{\lambda^2}$                                  | See Poisson                                                                                                                                                                                                     | $\frac{n}{\sum X_i}$                                                                                                                          | $\frac{1}{\lambda^2}$                                                                        | $\frac{\lambda}{\lambda - t}$                                                                   |
| Beta                | $(0,1)$                                | $\alpha, \beta$     | $\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}$                                                                                                                  | $\frac{\alpha}{\alpha + \beta}$ | $\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$ | $\alpha$ successes, $\beta$ failures. When values are large, we concentrate around 0.5.                                                                                                                         |                                                                                                                                               |                                                                                              |                                                                                                 |
| Gamma               | $(0, \infty)$                          | $\alpha, \beta$     | $\frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x}$                                                                                                        | $\frac{\alpha}{\beta}$          | $\frac{\alpha}{\beta^2}$                               | $\beta$ is known as the rate. The exponential and chi-squared distributions are special cases of the Gamma distribution. We can think of $\alpha$ as controlling the mean and $\beta$ controlling the variance. |                                                                                                                                               |                                                                                              | $\left(\frac{\beta}{\beta-t}\right)^\alpha$                                                     |
| Inverse-Gamma       | $(0, \infty)$                          | $\alpha, \beta$     | $\frac{\beta^\alpha}{\Gamma(\alpha)} x^{-\alpha-1} e^{-\beta/x}$                                                                                                       | $\frac{\beta}{\alpha-1}$        | $\frac{\beta^2}{(\alpha-1)^2(\alpha-2)}$               | $\beta$ is known as the scale (we use the other parameterization here)                                                                                                                                          |                                                                                                                                               |                                                                                              |                                                                                                 |
| Normal              | $(-\infty,\infty)$                     | $\mu, \sigma^2$     | $\frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\|x-\mu\|^2 /\left(2 \sigma^2\right)}$                                                                                            | $\mu$                           | $\sigma^2$                                             |                                                                                                                                                                                                                 | $\hat{\mu} = \bar{X}$, $\hat{\sigma^2}=\frac{\sum_i\left(X_i-\mu\right)^2}{n}$, $\hat{\sigma^2}=\frac{\sum_i\left(X_i-\bar{X}\right)^2}{n-1}$ | $\mathcal{I}(\mu)=\frac{1}{\sigma^2}, \mathcal{I}\left(\sigma^2\right)=\frac{1}{2 \sigma^4}$ | $\exp \left(\mu t+\frac{\sigma^2 t^2}{2}\right)$                                                |
| Chi-Squared         | $(0,\infty)$                           | $n$                 | $\frac{1}{2^{n / 2} \Gamma(n / 2)} x^{n / 2-1} e^{-x / 2}$                                                                                                             | $n$                             | $2n$                                                   | The sum of $n$ squared standard Normals.                                                                                                                                                                        |                                                                                                                                               |                                                                                              | $(1-2 t)^{-n/2}$                                                                                |
| Multivariate-Normal | $\mathbb{R}^p$                         | $\pmb{\mu, \Sigma}$ | $(2 \pi)^{-p / 2} \operatorname{det}(\pmb{\Sigma})^{-1 / 2} \exp \left(-\frac{1}{2}(\mathbf{x}-\pmb{\mu})^{\mathrm{T}} \pmb{\Sigma}^{-1}(\mathbf{x}-\pmb{\mu})\right)$ | $\pmb{\mu}$                     | $\pmb{\Sigma}$                                         |                                                                                                                                                                                                                 |                                                                                                                                               |                                                                                              | $\exp\left(\pmb{\mu}^{\top}\mathbf{t} + \frac12 \mathbf{t}^{\top}\pmb{\Sigma}\mathbf{t}\right)$ |
| Dirichlet           | $x_i \in [0,1]$ and $\sum_i^K x_i = 1$ | $\pmb{\alpha}$      | $\frac{1}{\mathrm{~B}(\pmb{\alpha})} \prod_{i=1}^K x_i^{\alpha_i-1}$                                                                                                   |                                 |                                                        | $\mathbf{x}$ are the probabilities to draw from the $K$ classes. $\alpha_i$ occurrences of category $i$. When the overall $\alpha$ value is higher, we tend to get more even probabilities.                     |                                                                                                                                               |                                                                                              |                                                                                                 |
| Laplace             | $(-\infty,\infty)$                     | $b, \mu$            | $\frac{1}{2b}\exp\left(-\frac{\|x-\mu\|}{b}\right)$                                                                                                                    |                                 |                                                        | The double exponential - splice two exponentials around $\mu$.                                                                                                                                                  |                                                                                                                                               |                                                                                              |                                                                                                 |                                                                                                 |
| Dirac               |                                        |                     |                                                                                                                                                                        |                                 |                                                        |                                                                                                                                                                                                                 |                                                                                                                                               |                                                                                              |                                                                                                 |
| Pareto              | $(\beta,\infty)$                       | $\alpha, \beta$     | $\frac{\alpha\beta^\alpha}{x^{\alpha+1}}$                                                                                                                              |                                 |                                                        |                                                                                                                                                                                                                 |                                                                                                                                               |                                                                                              |                                                                                                 |

### Conjugate priors

| Name                  | Prior                                              | Likelihood                                                  | Posterior                                                                                  | Interpretation                                                                                                                                                                                   | Posterior Predictive                                   | 
|-----------------------|----------------------------------------------------|-------------------------------------------------------------|--------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------|
| Beta-Binomial         | $p \sim \operatorname{Beta}(\alpha, \beta)$        | $x_i \sim \operatorname{Binom}(n_i,p)$                      | $\operatorname{Beta}(\alpha + \sum_i^Nx_i, \beta + \sum_i^N n_i - \sum_i^N x_i)$           | $\alpha$ successes, $\beta$ failures.                                                                                                                                                            | Beta-Binomial                                          |
| Gamma-Poisson         | $\lambda \sim \operatorname{Ga}(\alpha, \beta)$    | $x \sim \operatorname{Pois}(\lambda)$                       | $\operatorname{Ga}(\alpha + \sum_i^Nx_i, \beta + N)$                                       | $\alpha$ total occurrences in $\beta$ intervals.                                                                                                                                                 | $\operatorname{NB}(\alpha', \frac{\beta'}{1 + \beta'}$ |
| Dirichlet-Multinomial | $\mathbf{p} \sim \operatorname{Dir}(\pmb{\alpha})$ | $\mathbf{x} \sim \operatorname{Multinomial}(n, \mathbf{p})$ | $\operatorname{Dir}(\pmb{\alpha} + \mathbf{x})$                                            | $\alpha_i$ occurrences of category $i$.                                                                                                                                                          | Dirichlet-Multinomial                                  |
| Normal-Normal         | $\mu \sim \mathcal{N}(\mu_0, \tau_0^{-1})$         | $x \sim \mathcal{N}(\mu, \tau^{-1})$                        | $\frac{\tau_0 \mu_0+\tau \sum_{i=1}^N x_i}{\tau_0+n \tau},\left(\tau_0+n \tau\right)^{-1}$ | Known variance $\sigma^2$. The higher the precision, the more information each sample conveys. $\mu'$ is the precision-weighted mean. $\tau'$ shrinks as our uncertainty around $\mu$ decreases. | $\mathcal{N}(\mu', \frac{1}{\tau'} + \frac{1}{\tau})$  |
| Inverse Gamma-Normal  | $\sigma^2 \sim \operatorname{IG}(\alpha, \beta)$   | $x \sim \mathcal{N}(\mu, \sigma^2)$                         | $\operatorname{IG}(\alpha + \frac{N}{2}, \beta + \frac{\sum_{i=1}^N (x_i - \mu)^2}{2})$    | Known mean $\mu$. Variance was estimated from $2\alpha$ observations with sample variance $\frac{\beta}{\alpha}$.                                                                                | $t_{2\alpha'}(\mu, \sigma^2 = \frac{\beta'}{\alpha'})$ |

### Sampling
- Markov Chain Monte Carlo (MCMC)
  - Gibbs sampling
    - Efficient when conditionals are easy to sample
    - In the section above, we mostly have closed forms for the joint probability that permit for easy sampling, but sometimes it is easier to sample from the conditional distribution instead. 
    - Gibbs sampling is an MCMC algorithm to sample from the joint distribution by iteratively sampling from conditional distributions. 
    - Samples are generally not independent and hence effective sample size is lower. 
    - In addition, samples from the beginning of the chain (the burn-in period) may not accurately represent the desired distribution and are usually discarded.
  - Metropolis-Hastings
    - Flexible and works in many cases, but convergence can be slow if the proposal distribution is not well-chosen
    - Propose a state from a proposal distribution $q(x'\mid x)$, accept/reject depending on $p(x)$ and $p(x')$.
  - Hamiltonian MC
    - Efficient exploration of high-dimensional spaces using physics-based dynamics, but requires gradient computation
    - Treat the parameters of the model as "particles" and simulate their motion using Hamiltonian dynamics
    - Propose new states by solving the Hamiltonian equations of motion using numerical integration
    - Accept or reject the new state based on the Metropolis-Hastings acceptance criterion
  - Langevian MC
    - Update both the position and momentum using a stochastic differential equation (SDE) that includes a noise term to mimic the random walk
    - A lighter version of HMC, with more efficient updates but still requiring gradient information
- Non-MCMC
  - Rejection Sampling
    - Inefficient if proposal distribution is not well-chosen
    - Similar to MH, except we choose the proposal distribution $q(x)$ and accept based on $p(x)$.
  - Importance Sampling
    - Can have high variance if proposal distribution is not well-chosen
    - Weight samples $w(x) = \frac{p(x)}{q(x)}$

### Uninformative priors
* One common uninformative prior is the Jeffrey's Prior, $p(\theta) \propto |I(\theta)|^{1/2}$
  * The key characteristic of this prior is that it is "invariant" under reparametrization. I found this statement confusing and [this video](https://www.youtube.com/watch?v=S42N_6pQ5TA) was very helpful for my understanding.
    * Note that this seems necessary for an uninformative prior, but I'm not sure about its sufficiency. On this topic, I like [Salegg Apfelton's response](https://stats.stackexchange.com/questions/7519/why-are-jeffreys-priors-considered-noninformative). $\mathcal{I}(\theta)$ is large when there are fewer values of $X$ that would be compatible with $\theta$. Therefore, it is "easier" to find evidence against these values, which implies lower posterior probabilities. Jeffrey's prior counteracts this effect by increasing the prior density for those values of $\theta$.


